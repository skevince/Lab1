{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda3\\lib\\site-packages\\sklearn\\cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iterations  1 \n",
      " Loss_train:       [[ 0.83361939]] \n",
      " Loss_Validation:  [[ 0.78920391]]\n",
      "iterations  2 \n",
      " Loss_train:       [[ 0.83337783]] \n",
      " Loss_Validation:  [[ 0.7889721]]\n",
      "iterations  3 \n",
      " Loss_train:       [[ 0.83313626]] \n",
      " Loss_Validation:  [[ 0.7887403]]\n",
      "iterations  4 \n",
      " Loss_train:       [[ 0.8328947]] \n",
      " Loss_Validation:  [[ 0.7885085]]\n",
      "iterations  5 \n",
      " Loss_train:       [[ 0.83265313]] \n",
      " Loss_Validation:  [[ 0.7882767]]\n",
      "iterations  6 \n",
      " Loss_train:       [[ 0.83241157]] \n",
      " Loss_Validation:  [[ 0.78804559]]\n",
      "iterations  7 \n",
      " Loss_train:       [[ 0.83217001]] \n",
      " Loss_Validation:  [[ 0.78781571]]\n",
      "iterations  8 \n",
      " Loss_train:       [[ 0.83192845]] \n",
      " Loss_Validation:  [[ 0.78758583]]\n",
      "iterations  9 \n",
      " Loss_train:       [[ 0.83168688]] \n",
      " Loss_Validation:  [[ 0.78735595]]\n",
      "iterations  10 \n",
      " Loss_train:       [[ 0.83144532]] \n",
      " Loss_Validation:  [[ 0.78712607]]\n",
      "iterations  11 \n",
      " Loss_train:       [[ 0.83120409]] \n",
      " Loss_Validation:  [[ 0.78689619]]\n",
      "iterations  12 \n",
      " Loss_train:       [[ 0.83096666]] \n",
      " Loss_Validation:  [[ 0.78666818]]\n",
      "iterations  13 \n",
      " Loss_train:       [[ 0.83072923]] \n",
      " Loss_Validation:  [[ 0.78644017]]\n",
      "iterations  14 \n",
      " Loss_train:       [[ 0.8304918]] \n",
      " Loss_Validation:  [[ 0.78621216]]\n",
      "iterations  15 \n",
      " Loss_train:       [[ 0.83025438]] \n",
      " Loss_Validation:  [[ 0.78598416]]\n",
      "iterations  16 \n",
      " Loss_train:       [[ 0.83001695]] \n",
      " Loss_Validation:  [[ 0.78575615]]\n",
      "iterations  17 \n",
      " Loss_train:       [[ 0.82977953]] \n",
      " Loss_Validation:  [[ 0.78552814]]\n",
      "iterations  18 \n",
      " Loss_train:       [[ 0.8295421]] \n",
      " Loss_Validation:  [[ 0.78530013]]\n",
      "iterations  19 \n",
      " Loss_train:       [[ 0.82930468]] \n",
      " Loss_Validation:  [[ 0.78507213]]\n",
      "iterations  20 \n",
      " Loss_train:       [[ 0.82906734]] \n",
      " Loss_Validation:  [[ 0.78484412]]\n",
      "iterations  21 \n",
      " Loss_train:       [[ 0.8288323]] \n",
      " Loss_Validation:  [[ 0.78461719]]\n",
      "iterations  22 \n",
      " Loss_train:       [[ 0.82859725]] \n",
      " Loss_Validation:  [[ 0.78439026]]\n",
      "iterations  23 \n",
      " Loss_train:       [[ 0.82836221]] \n",
      " Loss_Validation:  [[ 0.78416333]]\n",
      "iterations  24 \n",
      " Loss_train:       [[ 0.82812717]] \n",
      " Loss_Validation:  [[ 0.78393639]]\n",
      "iterations  25 \n",
      " Loss_train:       [[ 0.82789213]] \n",
      " Loss_Validation:  [[ 0.78370946]]\n",
      "iterations  26 \n",
      " Loss_train:       [[ 0.82765708]] \n",
      " Loss_Validation:  [[ 0.78348253]]\n",
      "iterations  27 \n",
      " Loss_train:       [[ 0.82742204]] \n",
      " Loss_Validation:  [[ 0.7832556]]\n",
      "iterations  28 \n",
      " Loss_train:       [[ 0.827187]] \n",
      " Loss_Validation:  [[ 0.78302868]]\n",
      "iterations  29 \n",
      " Loss_train:       [[ 0.82695196]] \n",
      " Loss_Validation:  [[ 0.78280175]]\n",
      "iterations  30 \n",
      " Loss_train:       [[ 0.82671692]] \n",
      " Loss_Validation:  [[ 0.78257482]]\n",
      "iterations  31 \n",
      " Loss_train:       [[ 0.82648188]] \n",
      " Loss_Validation:  [[ 0.78234789]]\n",
      "iterations  32 \n",
      " Loss_train:       [[ 0.82624684]] \n",
      " Loss_Validation:  [[ 0.78212097]]\n",
      "iterations  33 \n",
      " Loss_train:       [[ 0.8260118]] \n",
      " Loss_Validation:  [[ 0.78189404]]\n",
      "iterations  34 \n",
      " Loss_train:       [[ 0.82577677]] \n",
      " Loss_Validation:  [[ 0.78166712]]\n",
      "iterations  35 \n",
      " Loss_train:       [[ 0.82554173]] \n",
      " Loss_Validation:  [[ 0.78144019]]\n",
      "iterations  36 \n",
      " Loss_train:       [[ 0.82530767]] \n",
      " Loss_Validation:  [[ 0.78121327]]\n",
      "iterations  37 \n",
      " Loss_train:       [[ 0.82507668]] \n",
      " Loss_Validation:  [[ 0.78098823]]\n",
      "iterations  38 \n",
      " Loss_train:       [[ 0.8248457]] \n",
      " Loss_Validation:  [[ 0.7807632]]\n",
      "iterations  39 \n",
      " Loss_train:       [[ 0.82461472]] \n",
      " Loss_Validation:  [[ 0.78053816]]\n",
      "iterations  40 \n",
      " Loss_train:       [[ 0.82438373]] \n",
      " Loss_Validation:  [[ 0.78031313]]\n",
      "iterations  41 \n",
      " Loss_train:       [[ 0.82415275]] \n",
      " Loss_Validation:  [[ 0.7800881]]\n",
      "iterations  42 \n",
      " Loss_train:       [[ 0.82392177]] \n",
      " Loss_Validation:  [[ 0.77986306]]\n",
      "iterations  43 \n",
      " Loss_train:       [[ 0.82369079]] \n",
      " Loss_Validation:  [[ 0.77963803]]\n",
      "iterations  44 \n",
      " Loss_train:       [[ 0.82345981]] \n",
      " Loss_Validation:  [[ 0.779413]]\n",
      "iterations  45 \n",
      " Loss_train:       [[ 0.82322883]] \n",
      " Loss_Validation:  [[ 0.77918797]]\n",
      "iterations  46 \n",
      " Loss_train:       [[ 0.82299785]] \n",
      " Loss_Validation:  [[ 0.77896294]]\n",
      "iterations  47 \n",
      " Loss_train:       [[ 0.82276687]] \n",
      " Loss_Validation:  [[ 0.77873791]]\n",
      "iterations  48 \n",
      " Loss_train:       [[ 0.82253589]] \n",
      " Loss_Validation:  [[ 0.77851288]]\n",
      "iterations  49 \n",
      " Loss_train:       [[ 0.82230491]] \n",
      " Loss_Validation:  [[ 0.77828785]]\n",
      "iterations  50 \n",
      " Loss_train:       [[ 0.82207394]] \n",
      " Loss_Validation:  [[ 0.77806283]]\n",
      "iterations  51 \n",
      " Loss_train:       [[ 0.82184296]] \n",
      " Loss_Validation:  [[ 0.7778378]]\n",
      "iterations  52 \n",
      " Loss_train:       [[ 0.82161198]] \n",
      " Loss_Validation:  [[ 0.77761277]]\n",
      "iterations  53 \n",
      " Loss_train:       [[ 0.82138101]] \n",
      " Loss_Validation:  [[ 0.77738775]]\n",
      "iterations  54 \n",
      " Loss_train:       [[ 0.82115003]] \n",
      " Loss_Validation:  [[ 0.77716272]]\n",
      "iterations  55 \n",
      " Loss_train:       [[ 0.82091905]] \n",
      " Loss_Validation:  [[ 0.77693769]]\n",
      "iterations  56 \n",
      " Loss_train:       [[ 0.82068808]] \n",
      " Loss_Validation:  [[ 0.77671267]]\n",
      "iterations  57 \n",
      " Loss_train:       [[ 0.8204571]] \n",
      " Loss_Validation:  [[ 0.77648765]]\n",
      "iterations  58 \n",
      " Loss_train:       [[ 0.82022613]] \n",
      " Loss_Validation:  [[ 0.77626287]]\n",
      "iterations  59 \n",
      " Loss_train:       [[ 0.81999516]] \n",
      " Loss_Validation:  [[ 0.77604072]]\n",
      "iterations  60 \n",
      " Loss_train:       [[ 0.81976418]] \n",
      " Loss_Validation:  [[ 0.77581857]]\n",
      "iterations  61 \n",
      " Loss_train:       [[ 0.81953321]] \n",
      " Loss_Validation:  [[ 0.77559643]]\n",
      "iterations  62 \n",
      " Loss_train:       [[ 0.81930224]] \n",
      " Loss_Validation:  [[ 0.77537428]]\n",
      "iterations  63 \n",
      " Loss_train:       [[ 0.81907127]] \n",
      " Loss_Validation:  [[ 0.77515213]]\n",
      "iterations  64 \n",
      " Loss_train:       [[ 0.8188403]] \n",
      " Loss_Validation:  [[ 0.77492998]]\n",
      "iterations  65 \n",
      " Loss_train:       [[ 0.81860933]] \n",
      " Loss_Validation:  [[ 0.77470784]]\n",
      "iterations  66 \n",
      " Loss_train:       [[ 0.81837836]] \n",
      " Loss_Validation:  [[ 0.77448569]]\n",
      "iterations  67 \n",
      " Loss_train:       [[ 0.81814739]] \n",
      " Loss_Validation:  [[ 0.77426355]]\n",
      "iterations  68 \n",
      " Loss_train:       [[ 0.81791642]] \n",
      " Loss_Validation:  [[ 0.7740414]]\n",
      "iterations  69 \n",
      " Loss_train:       [[ 0.81768545]] \n",
      " Loss_Validation:  [[ 0.77381926]]\n",
      "iterations  70 \n",
      " Loss_train:       [[ 0.81745448]] \n",
      " Loss_Validation:  [[ 0.77359712]]\n",
      "iterations  71 \n",
      " Loss_train:       [[ 0.81722351]] \n",
      " Loss_Validation:  [[ 0.77337497]]\n",
      "iterations  72 \n",
      " Loss_train:       [[ 0.81699254]] \n",
      " Loss_Validation:  [[ 0.77315283]]\n",
      "iterations  73 \n",
      " Loss_train:       [[ 0.81676158]] \n",
      " Loss_Validation:  [[ 0.77293069]]\n",
      "iterations  74 \n",
      " Loss_train:       [[ 0.81653061]] \n",
      " Loss_Validation:  [[ 0.77270855]]\n",
      "iterations  75 \n",
      " Loss_train:       [[ 0.81629964]] \n",
      " Loss_Validation:  [[ 0.77248641]]\n",
      "iterations  76 \n",
      " Loss_train:       [[ 0.81606987]] \n",
      " Loss_Validation:  [[ 0.77226427]]\n",
      "iterations  77 \n",
      " Loss_train:       [[ 0.81584278]] \n",
      " Loss_Validation:  [[ 0.77204395]]\n",
      "iterations  78 \n",
      " Loss_train:       [[ 0.81561569]] \n",
      " Loss_Validation:  [[ 0.77182362]]\n",
      "iterations  79 \n",
      " Loss_train:       [[ 0.8153886]] \n",
      " Loss_Validation:  [[ 0.7716033]]\n",
      "iterations  80 \n",
      " Loss_train:       [[ 0.81516151]] \n",
      " Loss_Validation:  [[ 0.77138298]]\n",
      "iterations  81 \n",
      " Loss_train:       [[ 0.81493442]] \n",
      " Loss_Validation:  [[ 0.77116266]]\n",
      "iterations  82 \n",
      " Loss_train:       [[ 0.81470813]] \n",
      " Loss_Validation:  [[ 0.77094234]]\n",
      "iterations  83 \n",
      " Loss_train:       [[ 0.8144834]] \n",
      " Loss_Validation:  [[ 0.77072317]]\n",
      "iterations  84 \n",
      " Loss_train:       [[ 0.81425866]] \n",
      " Loss_Validation:  [[ 0.77050399]]\n",
      "iterations  85 \n",
      " Loss_train:       [[ 0.81403393]] \n",
      " Loss_Validation:  [[ 0.77028482]]\n",
      "iterations  86 \n",
      " Loss_train:       [[ 0.81380919]] \n",
      " Loss_Validation:  [[ 0.77006564]]\n",
      "iterations  87 \n",
      " Loss_train:       [[ 0.81358446]] \n",
      " Loss_Validation:  [[ 0.76984647]]\n",
      "iterations  88 \n",
      " Loss_train:       [[ 0.81335973]] \n",
      " Loss_Validation:  [[ 0.7696273]]\n",
      "iterations  89 \n",
      " Loss_train:       [[ 0.81313499]] \n",
      " Loss_Validation:  [[ 0.76940812]]\n",
      "iterations  90 \n",
      " Loss_train:       [[ 0.81291026]] \n",
      " Loss_Validation:  [[ 0.76918895]]\n",
      "iterations  91 \n",
      " Loss_train:       [[ 0.81268553]] \n",
      " Loss_Validation:  [[ 0.76896978]]\n",
      "iterations  92 \n",
      " Loss_train:       [[ 0.8124608]] \n",
      " Loss_Validation:  [[ 0.76875061]]\n",
      "iterations  93 \n",
      " Loss_train:       [[ 0.81223607]] \n",
      " Loss_Validation:  [[ 0.76853144]]\n",
      "iterations  94 \n",
      " Loss_train:       [[ 0.81201134]] \n",
      " Loss_Validation:  [[ 0.76831227]]\n",
      "iterations  95 \n",
      " Loss_train:       [[ 0.81178661]] \n",
      " Loss_Validation:  [[ 0.7680931]]\n",
      "iterations  96 \n",
      " Loss_train:       [[ 0.81156188]] \n",
      " Loss_Validation:  [[ 0.76787393]]\n",
      "iterations  97 \n",
      " Loss_train:       [[ 0.81133715]] \n",
      " Loss_Validation:  [[ 0.76765476]]\n",
      "iterations  98 \n",
      " Loss_train:       [[ 0.81111242]] \n",
      " Loss_Validation:  [[ 0.7674356]]\n",
      "iterations  99 \n",
      " Loss_train:       [[ 0.8108877]] \n",
      " Loss_Validation:  [[ 0.76721643]]\n",
      "iterations  100 \n",
      " Loss_train:       [[ 0.81066297]] \n",
      " Loss_Validation:  [[ 0.76699726]]\n",
      "iterations  101 \n",
      " Loss_train:       [[ 0.81043824]] \n",
      " Loss_Validation:  [[ 0.7667781]]\n",
      "iterations  102 \n",
      " Loss_train:       [[ 0.81021351]] \n",
      " Loss_Validation:  [[ 0.76655893]]\n",
      "iterations  103 \n",
      " Loss_train:       [[ 0.80998879]] \n",
      " Loss_Validation:  [[ 0.76633977]]\n",
      "iterations  104 \n",
      " Loss_train:       [[ 0.80976406]] \n",
      " Loss_Validation:  [[ 0.7661206]]\n",
      "iterations  105 \n",
      " Loss_train:       [[ 0.80953934]] \n",
      " Loss_Validation:  [[ 0.76590144]]\n",
      "iterations  106 \n",
      " Loss_train:       [[ 0.80931461]] \n",
      " Loss_Validation:  [[ 0.76568228]]\n",
      "iterations  107 \n",
      " Loss_train:       [[ 0.80908989]] \n",
      " Loss_Validation:  [[ 0.76546312]]\n",
      "iterations  108 \n",
      " Loss_train:       [[ 0.80886516]] \n",
      " Loss_Validation:  [[ 0.76524395]]\n",
      "iterations  109 \n",
      " Loss_train:       [[ 0.80864044]] \n",
      " Loss_Validation:  [[ 0.76502479]]\n",
      "iterations  110 \n",
      " Loss_train:       [[ 0.80841572]] \n",
      " Loss_Validation:  [[ 0.76480563]]\n",
      "iterations  111 \n",
      " Loss_train:       [[ 0.808191]] \n",
      " Loss_Validation:  [[ 0.76458647]]\n",
      "iterations  112 \n",
      " Loss_train:       [[ 0.80796627]] \n",
      " Loss_Validation:  [[ 0.76436731]]\n",
      "iterations  113 \n",
      " Loss_train:       [[ 0.80774155]] \n",
      " Loss_Validation:  [[ 0.76414815]]\n",
      "iterations  114 \n",
      " Loss_train:       [[ 0.80751683]] \n",
      " Loss_Validation:  [[ 0.763929]]\n",
      "iterations  115 \n",
      " Loss_train:       [[ 0.80729225]] \n",
      " Loss_Validation:  [[ 0.76370984]]\n",
      "iterations  116 \n",
      " Loss_train:       [[ 0.80706786]] \n",
      " Loss_Validation:  [[ 0.76349061]]\n",
      "iterations  117 \n",
      " Loss_train:       [[ 0.80684347]] \n",
      " Loss_Validation:  [[ 0.76327138]]\n",
      "iterations  118 \n",
      " Loss_train:       [[ 0.80661908]] \n",
      " Loss_Validation:  [[ 0.76305215]]\n",
      "iterations  119 \n",
      " Loss_train:       [[ 0.80639469]] \n",
      " Loss_Validation:  [[ 0.76283292]]\n",
      "iterations  120 \n",
      " Loss_train:       [[ 0.80617031]] \n",
      " Loss_Validation:  [[ 0.76261369]]\n",
      "iterations  121 \n",
      " Loss_train:       [[ 0.80594592]] \n",
      " Loss_Validation:  [[ 0.76239447]]\n",
      "iterations  122 \n",
      " Loss_train:       [[ 0.80572153]] \n",
      " Loss_Validation:  [[ 0.76217524]]\n",
      "iterations  123 \n",
      " Loss_train:       [[ 0.80549715]] \n",
      " Loss_Validation:  [[ 0.76195601]]\n",
      "iterations  124 \n",
      " Loss_train:       [[ 0.80527276]] \n",
      " Loss_Validation:  [[ 0.76173679]]\n",
      "iterations  125 \n",
      " Loss_train:       [[ 0.80504838]] \n",
      " Loss_Validation:  [[ 0.76151756]]\n",
      "iterations  126 \n",
      " Loss_train:       [[ 0.80482399]] \n",
      " Loss_Validation:  [[ 0.76129834]]\n",
      "iterations  127 \n",
      " Loss_train:       [[ 0.80459961]] \n",
      " Loss_Validation:  [[ 0.76107911]]\n",
      "iterations  128 \n",
      " Loss_train:       [[ 0.80437522]] \n",
      " Loss_Validation:  [[ 0.76085989]]\n",
      "iterations  129 \n",
      " Loss_train:       [[ 0.80415084]] \n",
      " Loss_Validation:  [[ 0.76064067]]\n",
      "iterations  130 \n",
      " Loss_train:       [[ 0.80392646]] \n",
      " Loss_Validation:  [[ 0.76042145]]\n",
      "iterations  131 \n",
      " Loss_train:       [[ 0.80370208]] \n",
      " Loss_Validation:  [[ 0.76020222]]\n",
      "iterations  132 \n",
      " Loss_train:       [[ 0.8034777]] \n",
      " Loss_Validation:  [[ 0.759983]]\n",
      "iterations  133 \n",
      " Loss_train:       [[ 0.80325331]] \n",
      " Loss_Validation:  [[ 0.75976378]]\n",
      "iterations  134 \n",
      " Loss_train:       [[ 0.80302893]] \n",
      " Loss_Validation:  [[ 0.75954456]]\n",
      "iterations  135 \n",
      " Loss_train:       [[ 0.80280455]] \n",
      " Loss_Validation:  [[ 0.75932534]]\n",
      "iterations  136 \n",
      " Loss_train:       [[ 0.80258017]] \n",
      " Loss_Validation:  [[ 0.75910612]]\n",
      "iterations  137 \n",
      " Loss_train:       [[ 0.80235579]] \n",
      " Loss_Validation:  [[ 0.75888725]]\n",
      "iterations  138 \n",
      " Loss_train:       [[ 0.80213141]] \n",
      " Loss_Validation:  [[ 0.75866986]]\n",
      "iterations  139 \n",
      " Loss_train:       [[ 0.80190704]] \n",
      " Loss_Validation:  [[ 0.75845246]]\n",
      "iterations  140 \n",
      " Loss_train:       [[ 0.80168266]] \n",
      " Loss_Validation:  [[ 0.75823507]]\n",
      "iterations  141 \n",
      " Loss_train:       [[ 0.80145828]] \n",
      " Loss_Validation:  [[ 0.75801768]]\n",
      "iterations  142 \n",
      " Loss_train:       [[ 0.80123398]] \n",
      " Loss_Validation:  [[ 0.75780029]]\n",
      "iterations  143 \n",
      " Loss_train:       [[ 0.80101195]] \n",
      " Loss_Validation:  [[ 0.7575842]]\n",
      "iterations  144 \n",
      " Loss_train:       [[ 0.80078992]] \n",
      " Loss_Validation:  [[ 0.75736811]]\n",
      "iterations  145 \n",
      " Loss_train:       [[ 0.8005679]] \n",
      " Loss_Validation:  [[ 0.75715202]]\n",
      "iterations  146 \n",
      " Loss_train:       [[ 0.80034587]] \n",
      " Loss_Validation:  [[ 0.75693593]]\n",
      "iterations  147 \n",
      " Loss_train:       [[ 0.80012384]] \n",
      " Loss_Validation:  [[ 0.75671984]]\n",
      "iterations  148 \n",
      " Loss_train:       [[ 0.79990182]] \n",
      " Loss_Validation:  [[ 0.75650375]]\n",
      "iterations  149 \n",
      " Loss_train:       [[ 0.79967979]] \n",
      " Loss_Validation:  [[ 0.75628766]]\n",
      "iterations  150 \n",
      " Loss_train:       [[ 0.79945777]] \n",
      " Loss_Validation:  [[ 0.75607157]]\n",
      "iterations  151 \n",
      " Loss_train:       [[ 0.79923574]] \n",
      " Loss_Validation:  [[ 0.75585548]]\n",
      "iterations  152 \n",
      " Loss_train:       [[ 0.79901372]] \n",
      " Loss_Validation:  [[ 0.7556394]]\n",
      "iterations  153 \n",
      " Loss_train:       [[ 0.7987917]] \n",
      " Loss_Validation:  [[ 0.75542331]]\n",
      "iterations  154 \n",
      " Loss_train:       [[ 0.79856967]] \n",
      " Loss_Validation:  [[ 0.75520722]]\n",
      "iterations  155 \n",
      " Loss_train:       [[ 0.79834765]] \n",
      " Loss_Validation:  [[ 0.75499114]]\n",
      "iterations  156 \n",
      " Loss_train:       [[ 0.79812563]] \n",
      " Loss_Validation:  [[ 0.75477505]]\n",
      "iterations  157 \n",
      " Loss_train:       [[ 0.79790361]] \n",
      " Loss_Validation:  [[ 0.75455897]]\n",
      "iterations  158 \n",
      " Loss_train:       [[ 0.79768158]] \n",
      " Loss_Validation:  [[ 0.75434288]]\n",
      "iterations  159 \n",
      " Loss_train:       [[ 0.79745956]] \n",
      " Loss_Validation:  [[ 0.7541268]]\n",
      "iterations  160 \n",
      " Loss_train:       [[ 0.79723754]] \n",
      " Loss_Validation:  [[ 0.75391072]]\n",
      "iterations  161 \n",
      " Loss_train:       [[ 0.79701552]] \n",
      " Loss_Validation:  [[ 0.75369464]]\n",
      "iterations  162 \n",
      " Loss_train:       [[ 0.7967935]] \n",
      " Loss_Validation:  [[ 0.75347856]]\n",
      "iterations  163 \n",
      " Loss_train:       [[ 0.79657148]] \n",
      " Loss_Validation:  [[ 0.75326247]]\n",
      "iterations  164 \n",
      " Loss_train:       [[ 0.79634947]] \n",
      " Loss_Validation:  [[ 0.75304639]]\n",
      "iterations  165 \n",
      " Loss_train:       [[ 0.79612745]] \n",
      " Loss_Validation:  [[ 0.75283031]]\n",
      "iterations  166 \n",
      " Loss_train:       [[ 0.79590561]] \n",
      " Loss_Validation:  [[ 0.75261423]]\n",
      "iterations  167 \n",
      " Loss_train:       [[ 0.795686]] \n",
      " Loss_Validation:  [[ 0.75239951]]\n",
      "iterations  168 \n",
      " Loss_train:       [[ 0.79546639]] \n",
      " Loss_Validation:  [[ 0.75218478]]\n",
      "iterations  169 \n",
      " Loss_train:       [[ 0.79524678]] \n",
      " Loss_Validation:  [[ 0.75197005]]\n",
      "iterations  170 \n",
      " Loss_train:       [[ 0.79502717]] \n",
      " Loss_Validation:  [[ 0.75175532]]\n",
      "iterations  171 \n",
      " Loss_train:       [[ 0.79480756]] \n",
      " Loss_Validation:  [[ 0.75154059]]\n",
      "iterations  172 \n",
      " Loss_train:       [[ 0.79458796]] \n",
      " Loss_Validation:  [[ 0.75132587]]\n",
      "iterations  173 \n",
      " Loss_train:       [[ 0.79436835]] \n",
      " Loss_Validation:  [[ 0.75111114]]\n",
      "iterations  174 \n",
      " Loss_train:       [[ 0.79414874]] \n",
      " Loss_Validation:  [[ 0.75089641]]\n",
      "iterations  175 \n",
      " Loss_train:       [[ 0.79392913]] \n",
      " Loss_Validation:  [[ 0.75068169]]\n",
      "iterations  176 \n",
      " Loss_train:       [[ 0.79370953]] \n",
      " Loss_Validation:  [[ 0.75046696]]\n",
      "iterations  177 \n",
      " Loss_train:       [[ 0.79348992]] \n",
      " Loss_Validation:  [[ 0.75025318]]\n",
      "iterations  178 \n",
      " Loss_train:       [[ 0.79327032]] \n",
      " Loss_Validation:  [[ 0.75004042]]\n",
      "iterations  179 \n",
      " Loss_train:       [[ 0.79305076]] \n",
      " Loss_Validation:  [[ 0.74982767]]\n",
      "iterations  180 \n",
      " Loss_train:       [[ 0.79283326]] \n",
      " Loss_Validation:  [[ 0.74961588]]\n",
      "iterations  181 \n",
      " Loss_train:       [[ 0.79261575]] \n",
      " Loss_Validation:  [[ 0.7494041]]\n",
      "iterations  182 \n",
      " Loss_train:       [[ 0.79239825]] \n",
      " Loss_Validation:  [[ 0.74919231]]\n",
      "iterations  183 \n",
      " Loss_train:       [[ 0.79218074]] \n",
      " Loss_Validation:  [[ 0.74898053]]\n",
      "iterations  184 \n",
      " Loss_train:       [[ 0.79196324]] \n",
      " Loss_Validation:  [[ 0.74876874]]\n",
      "iterations  185 \n",
      " Loss_train:       [[ 0.79174574]] \n",
      " Loss_Validation:  [[ 0.74855696]]\n",
      "iterations  186 \n",
      " Loss_train:       [[ 0.79152823]] \n",
      " Loss_Validation:  [[ 0.74834518]]\n",
      "iterations  187 \n",
      " Loss_train:       [[ 0.79131073]] \n",
      " Loss_Validation:  [[ 0.74813339]]\n",
      "iterations  188 \n",
      " Loss_train:       [[ 0.79109323]] \n",
      " Loss_Validation:  [[ 0.74792161]]\n",
      "iterations  189 \n",
      " Loss_train:       [[ 0.79087573]] \n",
      " Loss_Validation:  [[ 0.74770983]]\n",
      "iterations  190 \n",
      " Loss_train:       [[ 0.79065919]] \n",
      " Loss_Validation:  [[ 0.74749805]]\n",
      "iterations  191 \n",
      " Loss_train:       [[ 0.79044398]] \n",
      " Loss_Validation:  [[ 0.74728757]]\n",
      "iterations  192 \n",
      " Loss_train:       [[ 0.79022876]] \n",
      " Loss_Validation:  [[ 0.74707708]]\n",
      "iterations  193 \n",
      " Loss_train:       [[ 0.79001355]] \n",
      " Loss_Validation:  [[ 0.7468666]]\n",
      "iterations  194 \n",
      " Loss_train:       [[ 0.78979834]] \n",
      " Loss_Validation:  [[ 0.74665612]]\n",
      "iterations  195 \n",
      " Loss_train:       [[ 0.78958312]] \n",
      " Loss_Validation:  [[ 0.74644564]]\n",
      "iterations  196 \n",
      " Loss_train:       [[ 0.78936791]] \n",
      " Loss_Validation:  [[ 0.74623516]]\n",
      "iterations  197 \n",
      " Loss_train:       [[ 0.7891527]] \n",
      " Loss_Validation:  [[ 0.74602469]]\n",
      "iterations  198 \n",
      " Loss_train:       [[ 0.78893749]] \n",
      " Loss_Validation:  [[ 0.74581421]]\n",
      "iterations  199 \n",
      " Loss_train:       [[ 0.78872227]] \n",
      " Loss_Validation:  [[ 0.74560373]]\n",
      "iterations  200 \n",
      " Loss_train:       [[ 0.78850706]] \n",
      " Loss_Validation:  [[ 0.74539325]]\n",
      "iterations  201 \n",
      " Loss_train:       [[ 0.78829185]] \n",
      " Loss_Validation:  [[ 0.74518278]]\n",
      "iterations  202 \n",
      " Loss_train:       [[ 0.78807664]] \n",
      " Loss_Validation:  [[ 0.7449723]]\n",
      "iterations  203 \n",
      " Loss_train:       [[ 0.78786143]] \n",
      " Loss_Validation:  [[ 0.74476183]]\n",
      "iterations  204 \n",
      " Loss_train:       [[ 0.78764622]] \n",
      " Loss_Validation:  [[ 0.74455135]]\n",
      "iterations  205 \n",
      " Loss_train:       [[ 0.78743101]] \n",
      " Loss_Validation:  [[ 0.74434088]]\n",
      "iterations  206 \n",
      " Loss_train:       [[ 0.7872158]] \n",
      " Loss_Validation:  [[ 0.7441304]]\n",
      "iterations  207 \n",
      " Loss_train:       [[ 0.7870006]] \n",
      " Loss_Validation:  [[ 0.74392117]]\n",
      "iterations  208 \n",
      " Loss_train:       [[ 0.78678539]] \n",
      " Loss_Validation:  [[ 0.74371236]]\n",
      "iterations  209 \n",
      " Loss_train:       [[ 0.78657018]] \n",
      " Loss_Validation:  [[ 0.74350354]]\n",
      "iterations  210 \n",
      " Loss_train:       [[ 0.78635497]] \n",
      " Loss_Validation:  [[ 0.74329472]]\n",
      "iterations  211 \n",
      " Loss_train:       [[ 0.78613977]] \n",
      " Loss_Validation:  [[ 0.74308591]]\n",
      "iterations  212 \n",
      " Loss_train:       [[ 0.78592456]] \n",
      " Loss_Validation:  [[ 0.74287709]]\n",
      "iterations  213 \n",
      " Loss_train:       [[ 0.78570936]] \n",
      " Loss_Validation:  [[ 0.74266828]]\n",
      "iterations  214 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Loss_train:       [[ 0.78549415]] \n",
      " Loss_Validation:  [[ 0.74245947]]\n",
      "iterations  215 \n",
      " Loss_train:       [[ 0.78527895]] \n",
      " Loss_Validation:  [[ 0.74225065]]\n",
      "iterations  216 \n",
      " Loss_train:       [[ 0.78506374]] \n",
      " Loss_Validation:  [[ 0.74204184]]\n",
      "iterations  217 \n",
      " Loss_train:       [[ 0.78484854]] \n",
      " Loss_Validation:  [[ 0.74183303]]\n",
      "iterations  218 \n",
      " Loss_train:       [[ 0.78463333]] \n",
      " Loss_Validation:  [[ 0.74162422]]\n",
      "iterations  219 \n",
      " Loss_train:       [[ 0.78441813]] \n",
      " Loss_Validation:  [[ 0.74141541]]\n",
      "iterations  220 \n",
      " Loss_train:       [[ 0.78420293]] \n",
      " Loss_Validation:  [[ 0.7412066]]\n",
      "iterations  221 \n",
      " Loss_train:       [[ 0.78398773]] \n",
      " Loss_Validation:  [[ 0.74099779]]\n",
      "iterations  222 \n",
      " Loss_train:       [[ 0.78377252]] \n",
      " Loss_Validation:  [[ 0.74078898]]\n",
      "iterations  223 \n",
      " Loss_train:       [[ 0.78355732]] \n",
      " Loss_Validation:  [[ 0.74058017]]\n",
      "iterations  224 \n",
      " Loss_train:       [[ 0.78334212]] \n",
      " Loss_Validation:  [[ 0.74037363]]\n",
      "iterations  225 \n",
      " Loss_train:       [[ 0.78312692]] \n",
      " Loss_Validation:  [[ 0.7401678]]\n",
      "iterations  226 \n",
      " Loss_train:       [[ 0.78291172]] \n",
      " Loss_Validation:  [[ 0.73996197]]\n",
      "iterations  227 \n",
      " Loss_train:       [[ 0.78269652]] \n",
      " Loss_Validation:  [[ 0.73975614]]\n",
      "iterations  228 \n",
      " Loss_train:       [[ 0.78248132]] \n",
      " Loss_Validation:  [[ 0.73955031]]\n",
      "iterations  229 \n",
      " Loss_train:       [[ 0.78226612]] \n",
      " Loss_Validation:  [[ 0.73934448]]\n",
      "iterations  230 \n",
      " Loss_train:       [[ 0.78205093]] \n",
      " Loss_Validation:  [[ 0.73913865]]\n",
      "iterations  231 \n",
      " Loss_train:       [[ 0.78183573]] \n",
      " Loss_Validation:  [[ 0.73893282]]\n",
      "iterations  232 \n",
      " Loss_train:       [[ 0.78162053]] \n",
      " Loss_Validation:  [[ 0.738727]]\n",
      "iterations  233 \n",
      " Loss_train:       [[ 0.78140533]] \n",
      " Loss_Validation:  [[ 0.73852117]]\n",
      "iterations  234 \n",
      " Loss_train:       [[ 0.78119014]] \n",
      " Loss_Validation:  [[ 0.73831534]]\n",
      "iterations  235 \n",
      " Loss_train:       [[ 0.78097494]] \n",
      " Loss_Validation:  [[ 0.73810951]]\n",
      "iterations  236 \n",
      " Loss_train:       [[ 0.78075975]] \n",
      " Loss_Validation:  [[ 0.73790369]]\n",
      "iterations  237 \n",
      " Loss_train:       [[ 0.78054455]] \n",
      " Loss_Validation:  [[ 0.73769786]]\n",
      "iterations  238 \n",
      " Loss_train:       [[ 0.78032935]] \n",
      " Loss_Validation:  [[ 0.73749204]]\n",
      "iterations  239 \n",
      " Loss_train:       [[ 0.78011416]] \n",
      " Loss_Validation:  [[ 0.73728621]]\n",
      "iterations  240 \n",
      " Loss_train:       [[ 0.77989897]] \n",
      " Loss_Validation:  [[ 0.73708039]]\n",
      "iterations  241 \n",
      " Loss_train:       [[ 0.77968377]] \n",
      " Loss_Validation:  [[ 0.73687457]]\n",
      "iterations  242 \n",
      " Loss_train:       [[ 0.77946858]] \n",
      " Loss_Validation:  [[ 0.73666874]]\n",
      "iterations  243 \n",
      " Loss_train:       [[ 0.77925339]] \n",
      " Loss_Validation:  [[ 0.73646292]]\n",
      "iterations  244 \n",
      " Loss_train:       [[ 0.77903819]] \n",
      " Loss_Validation:  [[ 0.7362571]]\n",
      "iterations  245 \n",
      " Loss_train:       [[ 0.778823]] \n",
      " Loss_Validation:  [[ 0.73605128]]\n",
      "iterations  246 \n",
      " Loss_train:       [[ 0.77860781]] \n",
      " Loss_Validation:  [[ 0.73584546]]\n",
      "iterations  247 \n",
      " Loss_train:       [[ 0.77839262]] \n",
      " Loss_Validation:  [[ 0.73563964]]\n",
      "iterations  248 \n",
      " Loss_train:       [[ 0.77817743]] \n",
      " Loss_Validation:  [[ 0.73543382]]\n",
      "iterations  249 \n",
      " Loss_train:       [[ 0.77796224]] \n",
      " Loss_Validation:  [[ 0.735228]]\n",
      "iterations  250 \n",
      " Loss_train:       [[ 0.77774782]] \n",
      " Loss_Validation:  [[ 0.73502218]]\n",
      "iterations  251 \n",
      " Loss_train:       [[ 0.77753603]] \n",
      " Loss_Validation:  [[ 0.73481796]]\n",
      "iterations  252 \n",
      " Loss_train:       [[ 0.77732425]] \n",
      " Loss_Validation:  [[ 0.73461374]]\n",
      "iterations  253 \n",
      " Loss_train:       [[ 0.77711247]] \n",
      " Loss_Validation:  [[ 0.73440952]]\n",
      "iterations  254 \n",
      " Loss_train:       [[ 0.77690069]] \n",
      " Loss_Validation:  [[ 0.7342053]]\n",
      "iterations  255 \n",
      " Loss_train:       [[ 0.77668892]] \n",
      " Loss_Validation:  [[ 0.73400108]]\n",
      "iterations  256 \n",
      " Loss_train:       [[ 0.77647714]] \n",
      " Loss_Validation:  [[ 0.73379686]]\n",
      "iterations  257 \n",
      " Loss_train:       [[ 0.77626536]] \n",
      " Loss_Validation:  [[ 0.73359265]]\n",
      "iterations  258 \n",
      " Loss_train:       [[ 0.77605358]] \n",
      " Loss_Validation:  [[ 0.73338843]]\n",
      "iterations  259 \n",
      " Loss_train:       [[ 0.7758418]] \n",
      " Loss_Validation:  [[ 0.73318421]]\n",
      "iterations  260 \n",
      " Loss_train:       [[ 0.77563003]] \n",
      " Loss_Validation:  [[ 0.73297999]]\n",
      "iterations  261 \n",
      " Loss_train:       [[ 0.77541889]] \n",
      " Loss_Validation:  [[ 0.73277578]]\n",
      "iterations  262 \n",
      " Loss_train:       [[ 0.77520946]] \n",
      " Loss_Validation:  [[ 0.73257282]]\n",
      "iterations  263 \n",
      " Loss_train:       [[ 0.77500003]] \n",
      " Loss_Validation:  [[ 0.73236986]]\n",
      "iterations  264 \n",
      " Loss_train:       [[ 0.7747906]] \n",
      " Loss_Validation:  [[ 0.73216689]]\n",
      "iterations  265 \n",
      " Loss_train:       [[ 0.77458117]] \n",
      " Loss_Validation:  [[ 0.73196393]]\n",
      "iterations  266 \n",
      " Loss_train:       [[ 0.77437174]] \n",
      " Loss_Validation:  [[ 0.73176097]]\n",
      "iterations  267 \n",
      " Loss_train:       [[ 0.77416231]] \n",
      " Loss_Validation:  [[ 0.73155802]]\n",
      "iterations  268 \n",
      " Loss_train:       [[ 0.77395288]] \n",
      " Loss_Validation:  [[ 0.73135506]]\n",
      "iterations  269 \n",
      " Loss_train:       [[ 0.77374345]] \n",
      " Loss_Validation:  [[ 0.7311521]]\n",
      "iterations  270 \n",
      " Loss_train:       [[ 0.77353509]] \n",
      " Loss_Validation:  [[ 0.73094914]]\n",
      "iterations  271 \n",
      " Loss_train:       [[ 0.77332816]] \n",
      " Loss_Validation:  [[ 0.73074754]]\n",
      "iterations  272 \n",
      " Loss_train:       [[ 0.77312123]] \n",
      " Loss_Validation:  [[ 0.73054593]]\n",
      "iterations  273 \n",
      " Loss_train:       [[ 0.7729143]] \n",
      " Loss_Validation:  [[ 0.73034433]]\n",
      "iterations  274 \n",
      " Loss_train:       [[ 0.77270738]] \n",
      " Loss_Validation:  [[ 0.73014273]]\n",
      "iterations  275 \n",
      " Loss_train:       [[ 0.77250216]] \n",
      " Loss_Validation:  [[ 0.72994112]]\n",
      "iterations  276 \n",
      " Loss_train:       [[ 0.77229881]] \n",
      " Loss_Validation:  [[ 0.72974124]]\n",
      "iterations  277 \n",
      " Loss_train:       [[ 0.77209546]] \n",
      " Loss_Validation:  [[ 0.72954135]]\n",
      "iterations  278 \n",
      " Loss_train:       [[ 0.7718921]] \n",
      " Loss_Validation:  [[ 0.72934146]]\n",
      "iterations  279 \n",
      " Loss_train:       [[ 0.77168923]] \n",
      " Loss_Validation:  [[ 0.72914158]]\n",
      "iterations  280 \n",
      " Loss_train:       [[ 0.77148804]] \n",
      " Loss_Validation:  [[ 0.72894296]]\n",
      "iterations  281 \n",
      " Loss_train:       [[ 0.77128685]] \n",
      " Loss_Validation:  [[ 0.72874434]]\n",
      "iterations  282 \n",
      " Loss_train:       [[ 0.77108566]] \n",
      " Loss_Validation:  [[ 0.72854572]]\n",
      "iterations  283 \n",
      " Loss_train:       [[ 0.77088447]] \n",
      " Loss_Validation:  [[ 0.7283471]]\n",
      "iterations  284 \n",
      " Loss_train:       [[ 0.77068328]] \n",
      " Loss_Validation:  [[ 0.72814849]]\n",
      "iterations  285 \n",
      " Loss_train:       [[ 0.77048209]] \n",
      " Loss_Validation:  [[ 0.72794987]]\n",
      "iterations  286 \n",
      " Loss_train:       [[ 0.77028091]] \n",
      " Loss_Validation:  [[ 0.72775125]]\n",
      "iterations  287 \n",
      " Loss_train:       [[ 0.77007972]] \n",
      " Loss_Validation:  [[ 0.72755264]]\n",
      "iterations  288 \n",
      " Loss_train:       [[ 0.76987853]] \n",
      " Loss_Validation:  [[ 0.72735402]]\n",
      "iterations  289 \n",
      " Loss_train:       [[ 0.76967734]] \n",
      " Loss_Validation:  [[ 0.72715541]]\n",
      "iterations  290 \n",
      " Loss_train:       [[ 0.76947616]] \n",
      " Loss_Validation:  [[ 0.72695679]]\n",
      "iterations  291 \n",
      " Loss_train:       [[ 0.76927497]] \n",
      " Loss_Validation:  [[ 0.72675818]]\n",
      "iterations  292 \n",
      " Loss_train:       [[ 0.76907378]] \n",
      " Loss_Validation:  [[ 0.72655956]]\n",
      "iterations  293 \n",
      " Loss_train:       [[ 0.7688726]] \n",
      " Loss_Validation:  [[ 0.72636095]]\n",
      "iterations  294 \n",
      " Loss_train:       [[ 0.76867141]] \n",
      " Loss_Validation:  [[ 0.72616234]]\n",
      "iterations  295 \n",
      " Loss_train:       [[ 0.76847023]] \n",
      " Loss_Validation:  [[ 0.72596373]]\n",
      "iterations  296 \n",
      " Loss_train:       [[ 0.76826904]] \n",
      " Loss_Validation:  [[ 0.72576512]]\n",
      "iterations  297 \n",
      " Loss_train:       [[ 0.76806786]] \n",
      " Loss_Validation:  [[ 0.7255665]]\n",
      "iterations  298 \n",
      " Loss_train:       [[ 0.76786742]] \n",
      " Loss_Validation:  [[ 0.72536789]]\n",
      "iterations  299 \n",
      " Loss_train:       [[ 0.76766996]] \n",
      " Loss_Validation:  [[ 0.72517106]]\n",
      "iterations  300 \n",
      " Loss_train:       [[ 0.7674725]] \n",
      " Loss_Validation:  [[ 0.72497423]]\n",
      "iterations  301 \n",
      " Loss_train:       [[ 0.76727505]] \n",
      " Loss_Validation:  [[ 0.72477739]]\n",
      "iterations  302 \n",
      " Loss_train:       [[ 0.76707759]] \n",
      " Loss_Validation:  [[ 0.72458056]]\n",
      "iterations  303 \n",
      " Loss_train:       [[ 0.76688013]] \n",
      " Loss_Validation:  [[ 0.72438373]]\n",
      "iterations  304 \n",
      " Loss_train:       [[ 0.76668267]] \n",
      " Loss_Validation:  [[ 0.72418689]]\n",
      "iterations  305 \n",
      " Loss_train:       [[ 0.76648522]] \n",
      " Loss_Validation:  [[ 0.72399006]]\n",
      "iterations  306 \n",
      " Loss_train:       [[ 0.76628776]] \n",
      " Loss_Validation:  [[ 0.72379323]]\n",
      "iterations  307 \n",
      " Loss_train:       [[ 0.76609031]] \n",
      " Loss_Validation:  [[ 0.7235964]]\n",
      "iterations  308 \n",
      " Loss_train:       [[ 0.76589285]] \n",
      " Loss_Validation:  [[ 0.72339957]]\n",
      "iterations  309 \n",
      " Loss_train:       [[ 0.76569539]] \n",
      " Loss_Validation:  [[ 0.72320274]]\n",
      "iterations  310 \n",
      " Loss_train:       [[ 0.76549794]] \n",
      " Loss_Validation:  [[ 0.72300591]]\n",
      "iterations  311 \n",
      " Loss_train:       [[ 0.76530049]] \n",
      " Loss_Validation:  [[ 0.72280908]]\n",
      "iterations  312 \n",
      " Loss_train:       [[ 0.76510303]] \n",
      " Loss_Validation:  [[ 0.72261226]]\n",
      "iterations  313 \n",
      " Loss_train:       [[ 0.76490558]] \n",
      " Loss_Validation:  [[ 0.72241543]]\n",
      "iterations  314 \n",
      " Loss_train:       [[ 0.76470812]] \n",
      " Loss_Validation:  [[ 0.7222186]]\n",
      "iterations  315 \n",
      " Loss_train:       [[ 0.76451067]] \n",
      " Loss_Validation:  [[ 0.72202178]]\n",
      "iterations  316 \n",
      " Loss_train:       [[ 0.76431322]] \n",
      " Loss_Validation:  [[ 0.72182495]]\n",
      "iterations  317 \n",
      " Loss_train:       [[ 0.76411577]] \n",
      " Loss_Validation:  [[ 0.72162812]]\n",
      "iterations  318 \n",
      " Loss_train:       [[ 0.76391832]] \n",
      " Loss_Validation:  [[ 0.7214313]]\n",
      "iterations  319 \n",
      " Loss_train:       [[ 0.76372086]] \n",
      " Loss_Validation:  [[ 0.72123447]]\n",
      "iterations  320 \n",
      " Loss_train:       [[ 0.76352341]] \n",
      " Loss_Validation:  [[ 0.72103765]]\n",
      "iterations  321 \n",
      " Loss_train:       [[ 0.76332596]] \n",
      " Loss_Validation:  [[ 0.72084083]]\n",
      "iterations  322 \n",
      " Loss_train:       [[ 0.76312851]] \n",
      " Loss_Validation:  [[ 0.720644]]\n",
      "iterations  323 \n",
      " Loss_train:       [[ 0.76293106]] \n",
      " Loss_Validation:  [[ 0.72044718]]\n",
      "iterations  324 \n",
      " Loss_train:       [[ 0.76273361]] \n",
      " Loss_Validation:  [[ 0.72025036]]\n",
      "iterations  325 \n",
      " Loss_train:       [[ 0.76253617]] \n",
      " Loss_Validation:  [[ 0.72005354]]\n",
      "iterations  326 \n",
      " Loss_train:       [[ 0.76233872]] \n",
      " Loss_Validation:  [[ 0.71985672]]\n",
      "iterations  327 \n",
      " Loss_train:       [[ 0.76214127]] \n",
      " Loss_Validation:  [[ 0.71965989]]\n",
      "iterations  328 \n",
      " Loss_train:       [[ 0.76194382]] \n",
      " Loss_Validation:  [[ 0.71946307]]\n",
      "iterations  329 \n",
      " Loss_train:       [[ 0.76174637]] \n",
      " Loss_Validation:  [[ 0.71926625]]\n",
      "iterations  330 \n",
      " Loss_train:       [[ 0.76154893]] \n",
      " Loss_Validation:  [[ 0.71906944]]\n",
      "iterations  331 \n",
      " Loss_train:       [[ 0.76135148]] \n",
      " Loss_Validation:  [[ 0.71887262]]\n",
      "iterations  332 \n",
      " Loss_train:       [[ 0.76115403]] \n",
      " Loss_Validation:  [[ 0.7186758]]\n",
      "iterations  333 \n",
      " Loss_train:       [[ 0.76095659]] \n",
      " Loss_Validation:  [[ 0.71847898]]\n",
      "iterations  334 \n",
      " Loss_train:       [[ 0.76075914]] \n",
      " Loss_Validation:  [[ 0.71828216]]\n",
      "iterations  335 \n",
      " Loss_train:       [[ 0.7605617]] \n",
      " Loss_Validation:  [[ 0.71808535]]\n",
      "iterations  336 \n",
      " Loss_train:       [[ 0.76036425]] \n",
      " Loss_Validation:  [[ 0.71788853]]\n",
      "iterations  337 \n",
      " Loss_train:       [[ 0.76016681]] \n",
      " Loss_Validation:  [[ 0.71769171]]\n",
      "iterations  338 \n",
      " Loss_train:       [[ 0.75996937]] \n",
      " Loss_Validation:  [[ 0.7174949]]\n",
      "iterations  339 \n",
      " Loss_train:       [[ 0.75977192]] \n",
      " Loss_Validation:  [[ 0.71729808]]\n",
      "iterations  340 \n",
      " Loss_train:       [[ 0.75957448]] \n",
      " Loss_Validation:  [[ 0.71710127]]\n",
      "iterations  341 \n",
      " Loss_train:       [[ 0.75937704]] \n",
      " Loss_Validation:  [[ 0.71690446]]\n",
      "iterations  342 \n",
      " Loss_train:       [[ 0.7591796]] \n",
      " Loss_Validation:  [[ 0.71670764]]\n",
      "iterations  343 \n",
      " Loss_train:       [[ 0.75898215]] \n",
      " Loss_Validation:  [[ 0.71651083]]\n",
      "iterations  344 \n",
      " Loss_train:       [[ 0.75878471]] \n",
      " Loss_Validation:  [[ 0.71631402]]\n",
      "iterations  345 \n",
      " Loss_train:       [[ 0.75858727]] \n",
      " Loss_Validation:  [[ 0.71611721]]\n",
      "iterations  346 \n",
      " Loss_train:       [[ 0.75838983]] \n",
      " Loss_Validation:  [[ 0.7159204]]\n",
      "iterations  347 \n",
      " Loss_train:       [[ 0.75819239]] \n",
      " Loss_Validation:  [[ 0.71572358]]\n",
      "iterations  348 \n",
      " Loss_train:       [[ 0.75799495]] \n",
      " Loss_Validation:  [[ 0.71552677]]\n",
      "iterations  349 \n",
      " Loss_train:       [[ 0.75779751]] \n",
      " Loss_Validation:  [[ 0.71532996]]\n",
      "iterations  350 \n",
      " Loss_train:       [[ 0.75760007]] \n",
      " Loss_Validation:  [[ 0.71513315]]\n",
      "iterations  351 \n",
      " Loss_train:       [[ 0.75740263]] \n",
      " Loss_Validation:  [[ 0.71493635]]\n",
      "iterations  352 \n",
      " Loss_train:       [[ 0.7572052]] \n",
      " Loss_Validation:  [[ 0.71473954]]\n",
      "iterations  353 \n",
      " Loss_train:       [[ 0.75700776]] \n",
      " Loss_Validation:  [[ 0.71454273]]\n",
      "iterations  354 \n",
      " Loss_train:       [[ 0.75681032]] \n",
      " Loss_Validation:  [[ 0.71434592]]\n",
      "iterations  355 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Loss_train:       [[ 0.75661288]] \n",
      " Loss_Validation:  [[ 0.71414912]]\n",
      "iterations  356 \n",
      " Loss_train:       [[ 0.75641545]] \n",
      " Loss_Validation:  [[ 0.71395231]]\n",
      "iterations  357 \n",
      " Loss_train:       [[ 0.75621801]] \n",
      " Loss_Validation:  [[ 0.7137555]]\n",
      "iterations  358 \n",
      " Loss_train:       [[ 0.75602057]] \n",
      " Loss_Validation:  [[ 0.7135587]]\n",
      "iterations  359 \n",
      " Loss_train:       [[ 0.75582314]] \n",
      " Loss_Validation:  [[ 0.71336189]]\n",
      "iterations  360 \n",
      " Loss_train:       [[ 0.7556257]] \n",
      " Loss_Validation:  [[ 0.71316509]]\n",
      "iterations  361 \n",
      " Loss_train:       [[ 0.75542827]] \n",
      " Loss_Validation:  [[ 0.71296828]]\n",
      "iterations  362 \n",
      " Loss_train:       [[ 0.75523084]] \n",
      " Loss_Validation:  [[ 0.71277148]]\n",
      "iterations  363 \n",
      " Loss_train:       [[ 0.7550334]] \n",
      " Loss_Validation:  [[ 0.71257468]]\n",
      "iterations  364 \n",
      " Loss_train:       [[ 0.75483597]] \n",
      " Loss_Validation:  [[ 0.71237788]]\n",
      "iterations  365 \n",
      " Loss_train:       [[ 0.75463854]] \n",
      " Loss_Validation:  [[ 0.71218107]]\n",
      "iterations  366 \n",
      " Loss_train:       [[ 0.7544411]] \n",
      " Loss_Validation:  [[ 0.71198427]]\n",
      "iterations  367 \n",
      " Loss_train:       [[ 0.75424367]] \n",
      " Loss_Validation:  [[ 0.71178747]]\n",
      "iterations  368 \n",
      " Loss_train:       [[ 0.75404624]] \n",
      " Loss_Validation:  [[ 0.71159067]]\n",
      "iterations  369 \n",
      " Loss_train:       [[ 0.75384881]] \n",
      " Loss_Validation:  [[ 0.71139387]]\n",
      "iterations  370 \n",
      " Loss_train:       [[ 0.75365138]] \n",
      " Loss_Validation:  [[ 0.71119707]]\n",
      "iterations  371 \n",
      " Loss_train:       [[ 0.75345395]] \n",
      " Loss_Validation:  [[ 0.71100027]]\n",
      "iterations  372 \n",
      " Loss_train:       [[ 0.75325651]] \n",
      " Loss_Validation:  [[ 0.71080347]]\n",
      "iterations  373 \n",
      " Loss_train:       [[ 0.75305909]] \n",
      " Loss_Validation:  [[ 0.71060668]]\n",
      "iterations  374 \n",
      " Loss_train:       [[ 0.75286166]] \n",
      " Loss_Validation:  [[ 0.71040988]]\n",
      "iterations  375 \n",
      " Loss_train:       [[ 0.75266423]] \n",
      " Loss_Validation:  [[ 0.71021379]]\n",
      "iterations  376 \n",
      " Loss_train:       [[ 0.7524668]] \n",
      " Loss_Validation:  [[ 0.71001987]]\n",
      "iterations  377 \n",
      " Loss_train:       [[ 0.75226937]] \n",
      " Loss_Validation:  [[ 0.70982595]]\n",
      "iterations  378 \n",
      " Loss_train:       [[ 0.75207194]] \n",
      " Loss_Validation:  [[ 0.70963204]]\n",
      "iterations  379 \n",
      " Loss_train:       [[ 0.75187451]] \n",
      " Loss_Validation:  [[ 0.70943812]]\n",
      "iterations  380 \n",
      " Loss_train:       [[ 0.75167709]] \n",
      " Loss_Validation:  [[ 0.7092442]]\n",
      "iterations  381 \n",
      " Loss_train:       [[ 0.75147966]] \n",
      " Loss_Validation:  [[ 0.70905028]]\n",
      "iterations  382 \n",
      " Loss_train:       [[ 0.75128223]] \n",
      " Loss_Validation:  [[ 0.70885637]]\n",
      "iterations  383 \n",
      " Loss_train:       [[ 0.75108481]] \n",
      " Loss_Validation:  [[ 0.70866245]]\n",
      "iterations  384 \n",
      " Loss_train:       [[ 0.75088738]] \n",
      " Loss_Validation:  [[ 0.70846854]]\n",
      "iterations  385 \n",
      " Loss_train:       [[ 0.75068996]] \n",
      " Loss_Validation:  [[ 0.70827462]]\n",
      "iterations  386 \n",
      " Loss_train:       [[ 0.75049253]] \n",
      " Loss_Validation:  [[ 0.70808071]]\n",
      "iterations  387 \n",
      " Loss_train:       [[ 0.75029511]] \n",
      " Loss_Validation:  [[ 0.7078868]]\n",
      "iterations  388 \n",
      " Loss_train:       [[ 0.75009768]] \n",
      " Loss_Validation:  [[ 0.70769288]]\n",
      "iterations  389 \n",
      " Loss_train:       [[ 0.74990026]] \n",
      " Loss_Validation:  [[ 0.70749897]]\n",
      "iterations  390 \n",
      " Loss_train:       [[ 0.74970284]] \n",
      " Loss_Validation:  [[ 0.70730506]]\n",
      "iterations  391 \n",
      " Loss_train:       [[ 0.74950541]] \n",
      " Loss_Validation:  [[ 0.70711115]]\n",
      "iterations  392 \n",
      " Loss_train:       [[ 0.74930799]] \n",
      " Loss_Validation:  [[ 0.70691723]]\n",
      "iterations  393 \n",
      " Loss_train:       [[ 0.74911057]] \n",
      " Loss_Validation:  [[ 0.70672332]]\n",
      "iterations  394 \n",
      " Loss_train:       [[ 0.74891315]] \n",
      " Loss_Validation:  [[ 0.70652941]]\n",
      "iterations  395 \n",
      " Loss_train:       [[ 0.74871573]] \n",
      " Loss_Validation:  [[ 0.7063355]]\n",
      "iterations  396 \n",
      " Loss_train:       [[ 0.74851831]] \n",
      " Loss_Validation:  [[ 0.70614159]]\n",
      "iterations  397 \n",
      " Loss_train:       [[ 0.74832089]] \n",
      " Loss_Validation:  [[ 0.70594769]]\n",
      "iterations  398 \n",
      " Loss_train:       [[ 0.74812347]] \n",
      " Loss_Validation:  [[ 0.70575378]]\n",
      "iterations  399 \n",
      " Loss_train:       [[ 0.74792605]] \n",
      " Loss_Validation:  [[ 0.70555987]]\n",
      "iterations  400 \n",
      " Loss_train:       [[ 0.74772863]] \n",
      " Loss_Validation:  [[ 0.70536596]]\n",
      "iterations  401 \n",
      " Loss_train:       [[ 0.74753121]] \n",
      " Loss_Validation:  [[ 0.70517206]]\n",
      "iterations  402 \n",
      " Loss_train:       [[ 0.74733379]] \n",
      " Loss_Validation:  [[ 0.70497815]]\n",
      "iterations  403 \n",
      " Loss_train:       [[ 0.74713637]] \n",
      " Loss_Validation:  [[ 0.70478424]]\n",
      "iterations  404 \n",
      " Loss_train:       [[ 0.74693895]] \n",
      " Loss_Validation:  [[ 0.70459034]]\n",
      "iterations  405 \n",
      " Loss_train:       [[ 0.74674233]] \n",
      " Loss_Validation:  [[ 0.70439643]]\n",
      "iterations  406 \n",
      " Loss_train:       [[ 0.74654892]] \n",
      " Loss_Validation:  [[ 0.70420462]]\n",
      "iterations  407 \n",
      " Loss_train:       [[ 0.74635551]] \n",
      " Loss_Validation:  [[ 0.70401448]]\n",
      "iterations  408 \n",
      " Loss_train:       [[ 0.7461621]] \n",
      " Loss_Validation:  [[ 0.70382515]]\n",
      "iterations  409 \n",
      " Loss_train:       [[ 0.74596869]] \n",
      " Loss_Validation:  [[ 0.70363581]]\n",
      "iterations  410 \n",
      " Loss_train:       [[ 0.74577529]] \n",
      " Loss_Validation:  [[ 0.70344647]]\n",
      "iterations  411 \n",
      " Loss_train:       [[ 0.74558188]] \n",
      " Loss_Validation:  [[ 0.70325714]]\n",
      "iterations  412 \n",
      " Loss_train:       [[ 0.74538847]] \n",
      " Loss_Validation:  [[ 0.7030678]]\n",
      "iterations  413 \n",
      " Loss_train:       [[ 0.74519507]] \n",
      " Loss_Validation:  [[ 0.70287847]]\n",
      "iterations  414 \n",
      " Loss_train:       [[ 0.74500166]] \n",
      " Loss_Validation:  [[ 0.70268914]]\n",
      "iterations  415 \n",
      " Loss_train:       [[ 0.74480826]] \n",
      " Loss_Validation:  [[ 0.7024998]]\n",
      "iterations  416 \n",
      " Loss_train:       [[ 0.74461485]] \n",
      " Loss_Validation:  [[ 0.70231047]]\n",
      "iterations  417 \n",
      " Loss_train:       [[ 0.74442145]] \n",
      " Loss_Validation:  [[ 0.70212114]]\n",
      "iterations  418 \n",
      " Loss_train:       [[ 0.74422843]] \n",
      " Loss_Validation:  [[ 0.7019318]]\n",
      "iterations  419 \n",
      " Loss_train:       [[ 0.74403715]] \n",
      " Loss_Validation:  [[ 0.70174369]]\n",
      "iterations  420 \n",
      " Loss_train:       [[ 0.74384587]] \n",
      " Loss_Validation:  [[ 0.70155557]]\n",
      "iterations  421 \n",
      " Loss_train:       [[ 0.74365459]] \n",
      " Loss_Validation:  [[ 0.70136745]]\n",
      "iterations  422 \n",
      " Loss_train:       [[ 0.74346331]] \n",
      " Loss_Validation:  [[ 0.70117933]]\n",
      "iterations  423 \n",
      " Loss_train:       [[ 0.74327203]] \n",
      " Loss_Validation:  [[ 0.70099121]]\n",
      "iterations  424 \n",
      " Loss_train:       [[ 0.74308075]] \n",
      " Loss_Validation:  [[ 0.7008031]]\n",
      "iterations  425 \n",
      " Loss_train:       [[ 0.74288947]] \n",
      " Loss_Validation:  [[ 0.70061498]]\n",
      "iterations  426 \n",
      " Loss_train:       [[ 0.74269819]] \n",
      " Loss_Validation:  [[ 0.70042686]]\n",
      "iterations  427 \n",
      " Loss_train:       [[ 0.74250692]] \n",
      " Loss_Validation:  [[ 0.70023875]]\n",
      "iterations  428 \n",
      " Loss_train:       [[ 0.74231564]] \n",
      " Loss_Validation:  [[ 0.70005063]]\n",
      "iterations  429 \n",
      " Loss_train:       [[ 0.74212436]] \n",
      " Loss_Validation:  [[ 0.69986252]]\n",
      "iterations  430 \n",
      " Loss_train:       [[ 0.74193308]] \n",
      " Loss_Validation:  [[ 0.6996744]]\n",
      "iterations  431 \n",
      " Loss_train:       [[ 0.74174181]] \n",
      " Loss_Validation:  [[ 0.69948629]]\n",
      "iterations  432 \n",
      " Loss_train:       [[ 0.74155053]] \n",
      " Loss_Validation:  [[ 0.69929818]]\n",
      "iterations  433 \n",
      " Loss_train:       [[ 0.74135925]] \n",
      " Loss_Validation:  [[ 0.69911006]]\n",
      "iterations  434 \n",
      " Loss_train:       [[ 0.74116798]] \n",
      " Loss_Validation:  [[ 0.69892195]]\n",
      "iterations  435 \n",
      " Loss_train:       [[ 0.7409767]] \n",
      " Loss_Validation:  [[ 0.69873384]]\n",
      "iterations  436 \n",
      " Loss_train:       [[ 0.74078543]] \n",
      " Loss_Validation:  [[ 0.69854573]]\n",
      "iterations  437 \n",
      " Loss_train:       [[ 0.74059415]] \n",
      " Loss_Validation:  [[ 0.69835762]]\n",
      "iterations  438 \n",
      " Loss_train:       [[ 0.74040288]] \n",
      " Loss_Validation:  [[ 0.69816951]]\n",
      "iterations  439 \n",
      " Loss_train:       [[ 0.74021243]] \n",
      " Loss_Validation:  [[ 0.6979814]]\n",
      "iterations  440 \n",
      " Loss_train:       [[ 0.74002477]] \n",
      " Loss_Validation:  [[ 0.69779501]]\n",
      "iterations  441 \n",
      " Loss_train:       [[ 0.7398371]] \n",
      " Loss_Validation:  [[ 0.69760862]]\n",
      "iterations  442 \n",
      " Loss_train:       [[ 0.73964944]] \n",
      " Loss_Validation:  [[ 0.69742223]]\n",
      "iterations  443 \n",
      " Loss_train:       [[ 0.73946178]] \n",
      " Loss_Validation:  [[ 0.69723584]]\n",
      "iterations  444 \n",
      " Loss_train:       [[ 0.73927412]] \n",
      " Loss_Validation:  [[ 0.69704945]]\n",
      "iterations  445 \n",
      " Loss_train:       [[ 0.73908646]] \n",
      " Loss_Validation:  [[ 0.69686307]]\n",
      "iterations  446 \n",
      " Loss_train:       [[ 0.7388988]] \n",
      " Loss_Validation:  [[ 0.69667668]]\n",
      "iterations  447 \n",
      " Loss_train:       [[ 0.73871114]] \n",
      " Loss_Validation:  [[ 0.69649029]]\n",
      "iterations  448 \n",
      " Loss_train:       [[ 0.73852348]] \n",
      " Loss_Validation:  [[ 0.69630391]]\n",
      "iterations  449 \n",
      " Loss_train:       [[ 0.73833582]] \n",
      " Loss_Validation:  [[ 0.69611752]]\n",
      "iterations  450 \n",
      " Loss_train:       [[ 0.73814816]] \n",
      " Loss_Validation:  [[ 0.69593114]]\n",
      "iterations  451 \n",
      " Loss_train:       [[ 0.73796175]] \n",
      " Loss_Validation:  [[ 0.69574475]]\n",
      "iterations  452 \n",
      " Loss_train:       [[ 0.73777777]] \n",
      " Loss_Validation:  [[ 0.69556014]]\n",
      "iterations  453 \n",
      " Loss_train:       [[ 0.73759379]] \n",
      " Loss_Validation:  [[ 0.69537553]]\n",
      "iterations  454 \n",
      " Loss_train:       [[ 0.7374098]] \n",
      " Loss_Validation:  [[ 0.69519092]]\n",
      "iterations  455 \n",
      " Loss_train:       [[ 0.73722582]] \n",
      " Loss_Validation:  [[ 0.69500632]]\n",
      "iterations  456 \n",
      " Loss_train:       [[ 0.73704297]] \n",
      " Loss_Validation:  [[ 0.69482171]]\n",
      "iterations  457 \n",
      " Loss_train:       [[ 0.73686239]] \n",
      " Loss_Validation:  [[ 0.69463876]]\n",
      "iterations  458 \n",
      " Loss_train:       [[ 0.73668182]] \n",
      " Loss_Validation:  [[ 0.69445581]]\n",
      "iterations  459 \n",
      " Loss_train:       [[ 0.73650124]] \n",
      " Loss_Validation:  [[ 0.69427286]]\n",
      "iterations  460 \n",
      " Loss_train:       [[ 0.73632067]] \n",
      " Loss_Validation:  [[ 0.69408992]]\n",
      "iterations  461 \n",
      " Loss_train:       [[ 0.73614009]] \n",
      " Loss_Validation:  [[ 0.69390697]]\n",
      "iterations  462 \n",
      " Loss_train:       [[ 0.73595952]] \n",
      " Loss_Validation:  [[ 0.69372402]]\n",
      "iterations  463 \n",
      " Loss_train:       [[ 0.73577894]] \n",
      " Loss_Validation:  [[ 0.69354108]]\n",
      "iterations  464 \n",
      " Loss_train:       [[ 0.73559837]] \n",
      " Loss_Validation:  [[ 0.69335813]]\n",
      "iterations  465 \n",
      " Loss_train:       [[ 0.7354178]] \n",
      " Loss_Validation:  [[ 0.69317518]]\n",
      "iterations  466 \n",
      " Loss_train:       [[ 0.73523723]] \n",
      " Loss_Validation:  [[ 0.69299224]]\n",
      "iterations  467 \n",
      " Loss_train:       [[ 0.73505665]] \n",
      " Loss_Validation:  [[ 0.6928093]]\n",
      "iterations  468 \n",
      " Loss_train:       [[ 0.73487608]] \n",
      " Loss_Validation:  [[ 0.69262635]]\n",
      "iterations  469 \n",
      " Loss_train:       [[ 0.73469551]] \n",
      " Loss_Validation:  [[ 0.69244341]]\n",
      "iterations  470 \n",
      " Loss_train:       [[ 0.73451494]] \n",
      " Loss_Validation:  [[ 0.69226046]]\n",
      "iterations  471 \n",
      " Loss_train:       [[ 0.73433437]] \n",
      " Loss_Validation:  [[ 0.69207752]]\n",
      "iterations  472 \n",
      " Loss_train:       [[ 0.7341538]] \n",
      " Loss_Validation:  [[ 0.69189458]]\n",
      "iterations  473 \n",
      " Loss_train:       [[ 0.73397323]] \n",
      " Loss_Validation:  [[ 0.69171164]]\n",
      "iterations  474 \n",
      " Loss_train:       [[ 0.73379266]] \n",
      " Loss_Validation:  [[ 0.6915287]]\n",
      "iterations  475 \n",
      " Loss_train:       [[ 0.73361209]] \n",
      " Loss_Validation:  [[ 0.69134604]]\n",
      "iterations  476 \n",
      " Loss_train:       [[ 0.73343152]] \n",
      " Loss_Validation:  [[ 0.69116389]]\n",
      "iterations  477 \n",
      " Loss_train:       [[ 0.73325095]] \n",
      " Loss_Validation:  [[ 0.69098175]]\n",
      "iterations  478 \n",
      " Loss_train:       [[ 0.73307038]] \n",
      " Loss_Validation:  [[ 0.6907996]]\n",
      "iterations  479 \n",
      " Loss_train:       [[ 0.73288981]] \n",
      " Loss_Validation:  [[ 0.69061757]]\n",
      "iterations  480 \n",
      " Loss_train:       [[ 0.73270925]] \n",
      " Loss_Validation:  [[ 0.69043686]]\n",
      "iterations  481 \n",
      " Loss_train:       [[ 0.73252868]] \n",
      " Loss_Validation:  [[ 0.69025615]]\n",
      "iterations  482 \n",
      " Loss_train:       [[ 0.73234811]] \n",
      " Loss_Validation:  [[ 0.69007545]]\n",
      "iterations  483 \n",
      " Loss_train:       [[ 0.73216755]] \n",
      " Loss_Validation:  [[ 0.68989475]]\n",
      "iterations  484 \n",
      " Loss_train:       [[ 0.73198698]] \n",
      " Loss_Validation:  [[ 0.68971404]]\n",
      "iterations  485 \n",
      " Loss_train:       [[ 0.73180642]] \n",
      " Loss_Validation:  [[ 0.68953334]]\n",
      "iterations  486 \n",
      " Loss_train:       [[ 0.73162585]] \n",
      " Loss_Validation:  [[ 0.68935263]]\n",
      "iterations  487 \n",
      " Loss_train:       [[ 0.73144528]] \n",
      " Loss_Validation:  [[ 0.68917206]]\n",
      "iterations  488 \n",
      " Loss_train:       [[ 0.73126472]] \n",
      " Loss_Validation:  [[ 0.68899282]]\n",
      "iterations  489 \n",
      " Loss_train:       [[ 0.73108416]] \n",
      " Loss_Validation:  [[ 0.68881358]]\n",
      "iterations  490 \n",
      " Loss_train:       [[ 0.73090359]] \n",
      " Loss_Validation:  [[ 0.68863434]]\n",
      "iterations  491 \n",
      " Loss_train:       [[ 0.73072303]] \n",
      " Loss_Validation:  [[ 0.6884551]]\n",
      "iterations  492 \n",
      " Loss_train:       [[ 0.73054247]] \n",
      " Loss_Validation:  [[ 0.68827586]]\n",
      "iterations  493 \n",
      " Loss_train:       [[ 0.7303619]] \n",
      " Loss_Validation:  [[ 0.68809662]]\n",
      "iterations  494 \n",
      " Loss_train:       [[ 0.73018134]] \n",
      " Loss_Validation:  [[ 0.68791738]]\n",
      "iterations  495 \n",
      " Loss_train:       [[ 0.73000136]] \n",
      " Loss_Validation:  [[ 0.68773814]]\n",
      "iterations  496 \n",
      " Loss_train:       [[ 0.72982409]] \n",
      " Loss_Validation:  [[ 0.68756049]]\n",
      "iterations  497 \n",
      " Loss_train:       [[ 0.72964682]] \n",
      " Loss_Validation:  [[ 0.68738284]]\n",
      "iterations  498 \n",
      " Loss_train:       [[ 0.72947066]] \n",
      " Loss_Validation:  [[ 0.68720519]]\n",
      "iterations  499 \n",
      " Loss_train:       [[ 0.72929691]] \n",
      " Loss_Validation:  [[ 0.68702926]]\n",
      "iterations  500 \n",
      " Loss_train:       [[ 0.72912317]] \n",
      " Loss_Validation:  [[ 0.68685333]]\n",
      "iterations  501 \n",
      " Loss_train:       [[ 0.72894943]] \n",
      " Loss_Validation:  [[ 0.6866774]]\n",
      "iterations  502 \n",
      " Loss_train:       [[ 0.72877569]] \n",
      " Loss_Validation:  [[ 0.68650147]]\n",
      "iterations  503 \n",
      " Loss_train:       [[ 0.72860194]] \n",
      " Loss_Validation:  [[ 0.68632554]]\n",
      "iterations  504 \n",
      " Loss_train:       [[ 0.7284282]] \n",
      " Loss_Validation:  [[ 0.68614961]]\n",
      "iterations  505 \n",
      " Loss_train:       [[ 0.72825446]] \n",
      " Loss_Validation:  [[ 0.68597369]]\n",
      "iterations  506 \n",
      " Loss_train:       [[ 0.72808072]] \n",
      " Loss_Validation:  [[ 0.68579776]]\n",
      "iterations  507 \n",
      " Loss_train:       [[ 0.72790698]] \n",
      " Loss_Validation:  [[ 0.68562183]]\n",
      "iterations  508 \n",
      " Loss_train:       [[ 0.72773381]] \n",
      " Loss_Validation:  [[ 0.68544591]]\n",
      "iterations  509 \n",
      " Loss_train:       [[ 0.72756133]] \n",
      " Loss_Validation:  [[ 0.68527062]]\n",
      "iterations  510 \n",
      " Loss_train:       [[ 0.72738885]] \n",
      " Loss_Validation:  [[ 0.68509533]]\n",
      "iterations  511 \n",
      " Loss_train:       [[ 0.72721638]] \n",
      " Loss_Validation:  [[ 0.68492004]]\n",
      "iterations  512 \n",
      " Loss_train:       [[ 0.7270439]] \n",
      " Loss_Validation:  [[ 0.68474475]]\n",
      "iterations  513 \n",
      " Loss_train:       [[ 0.72687143]] \n",
      " Loss_Validation:  [[ 0.68456946]]\n",
      "iterations  514 \n",
      " Loss_train:       [[ 0.72669922]] \n",
      " Loss_Validation:  [[ 0.68439417]]\n",
      "iterations  515 \n",
      " Loss_train:       [[ 0.72652776]] \n",
      " Loss_Validation:  [[ 0.68421939]]\n",
      "iterations  516 \n",
      " Loss_train:       [[ 0.72635629]] \n",
      " Loss_Validation:  [[ 0.68404461]]\n",
      "iterations  517 \n",
      " Loss_train:       [[ 0.72618483]] \n",
      " Loss_Validation:  [[ 0.68386983]]\n",
      "iterations  518 \n",
      " Loss_train:       [[ 0.72601337]] \n",
      " Loss_Validation:  [[ 0.68369505]]\n",
      "iterations  519 \n",
      " Loss_train:       [[ 0.72584191]] \n",
      " Loss_Validation:  [[ 0.68352028]]\n",
      "iterations  520 \n",
      " Loss_train:       [[ 0.72567045]] \n",
      " Loss_Validation:  [[ 0.6833455]]\n",
      "iterations  521 \n",
      " Loss_train:       [[ 0.72549899]] \n",
      " Loss_Validation:  [[ 0.68317072]]\n",
      "iterations  522 \n",
      " Loss_train:       [[ 0.72532753]] \n",
      " Loss_Validation:  [[ 0.68299594]]\n",
      "iterations  523 \n",
      " Loss_train:       [[ 0.72515607]] \n",
      " Loss_Validation:  [[ 0.68282117]]\n",
      "iterations  524 \n",
      " Loss_train:       [[ 0.72498461]] \n",
      " Loss_Validation:  [[ 0.68264639]]\n",
      "iterations  525 \n",
      " Loss_train:       [[ 0.72481315]] \n",
      " Loss_Validation:  [[ 0.68247162]]\n",
      "iterations  526 \n",
      " Loss_train:       [[ 0.72464169]] \n",
      " Loss_Validation:  [[ 0.68229684]]\n",
      "iterations  527 \n",
      " Loss_train:       [[ 0.72447023]] \n",
      " Loss_Validation:  [[ 0.68212207]]\n",
      "iterations  528 \n",
      " Loss_train:       [[ 0.72429877]] \n",
      " Loss_Validation:  [[ 0.68194729]]\n",
      "iterations  529 \n",
      " Loss_train:       [[ 0.72412732]] \n",
      " Loss_Validation:  [[ 0.68177252]]\n",
      "iterations  530 \n",
      " Loss_train:       [[ 0.72395586]] \n",
      " Loss_Validation:  [[ 0.68159775]]\n",
      "iterations  531 \n",
      " Loss_train:       [[ 0.7237844]] \n",
      " Loss_Validation:  [[ 0.68142297]]\n",
      "iterations  532 \n",
      " Loss_train:       [[ 0.72361294]] \n",
      " Loss_Validation:  [[ 0.6812482]]\n",
      "iterations  533 \n",
      " Loss_train:       [[ 0.72344149]] \n",
      " Loss_Validation:  [[ 0.68107343]]\n",
      "iterations  534 \n",
      " Loss_train:       [[ 0.72327003]] \n",
      " Loss_Validation:  [[ 0.68089866]]\n",
      "iterations  535 \n",
      " Loss_train:       [[ 0.72309858]] \n",
      " Loss_Validation:  [[ 0.68072389]]\n",
      "iterations  536 \n",
      " Loss_train:       [[ 0.72292712]] \n",
      " Loss_Validation:  [[ 0.68054912]]\n",
      "iterations  537 \n",
      " Loss_train:       [[ 0.72275566]] \n",
      " Loss_Validation:  [[ 0.68037434]]\n",
      "iterations  538 \n",
      " Loss_train:       [[ 0.72258421]] \n",
      " Loss_Validation:  [[ 0.68019958]]\n",
      "iterations  539 \n",
      " Loss_train:       [[ 0.72241333]] \n",
      " Loss_Validation:  [[ 0.68002481]]\n",
      "iterations  540 \n",
      " Loss_train:       [[ 0.72224842]] \n",
      " Loss_Validation:  [[ 0.67985334]]\n",
      "iterations  541 \n",
      " Loss_train:       [[ 0.72208352]] \n",
      " Loss_Validation:  [[ 0.67968188]]\n",
      "iterations  542 \n",
      " Loss_train:       [[ 0.72191862]] \n",
      " Loss_Validation:  [[ 0.67951041]]\n",
      "iterations  543 \n",
      " Loss_train:       [[ 0.72175372]] \n",
      " Loss_Validation:  [[ 0.67933895]]\n",
      "iterations  544 \n",
      " Loss_train:       [[ 0.72158882]] \n",
      " Loss_Validation:  [[ 0.67916748]]\n",
      "iterations  545 \n",
      " Loss_train:       [[ 0.72142392]] \n",
      " Loss_Validation:  [[ 0.67899602]]\n",
      "iterations  546 \n",
      " Loss_train:       [[ 0.72125902]] \n",
      " Loss_Validation:  [[ 0.67882456]]\n",
      "iterations  547 \n",
      " Loss_train:       [[ 0.72109412]] \n",
      " Loss_Validation:  [[ 0.6786531]]\n",
      "iterations  548 \n",
      " Loss_train:       [[ 0.72092922]] \n",
      " Loss_Validation:  [[ 0.67848163]]\n",
      "iterations  549 \n",
      " Loss_train:       [[ 0.72076446]] \n",
      " Loss_Validation:  [[ 0.67831054]]\n",
      "iterations  550 \n",
      " Loss_train:       [[ 0.72060093]] \n",
      " Loss_Validation:  [[ 0.67814126]]\n",
      "iterations  551 \n",
      " Loss_train:       [[ 0.7204374]] \n",
      " Loss_Validation:  [[ 0.67797198]]\n",
      "iterations  552 \n",
      " Loss_train:       [[ 0.72027388]] \n",
      " Loss_Validation:  [[ 0.6778027]]\n",
      "iterations  553 \n",
      " Loss_train:       [[ 0.72011035]] \n",
      " Loss_Validation:  [[ 0.67763342]]\n",
      "iterations  554 \n",
      " Loss_train:       [[ 0.71994682]] \n",
      " Loss_Validation:  [[ 0.67746414]]\n",
      "iterations  555 \n",
      " Loss_train:       [[ 0.7197833]] \n",
      " Loss_Validation:  [[ 0.67729692]]\n",
      "iterations  556 \n",
      " Loss_train:       [[ 0.71961977]] \n",
      " Loss_Validation:  [[ 0.6771313]]\n",
      "iterations  557 \n",
      " Loss_train:       [[ 0.71945625]] \n",
      " Loss_Validation:  [[ 0.67696567]]\n",
      "iterations  558 \n",
      " Loss_train:       [[ 0.71929272]] \n",
      " Loss_Validation:  [[ 0.67680005]]\n",
      "iterations  559 \n",
      " Loss_train:       [[ 0.7191292]] \n",
      " Loss_Validation:  [[ 0.67663442]]\n",
      "iterations  560 \n",
      " Loss_train:       [[ 0.71896568]] \n",
      " Loss_Validation:  [[ 0.6764688]]\n",
      "iterations  561 \n",
      " Loss_train:       [[ 0.71880215]] \n",
      " Loss_Validation:  [[ 0.67630317]]\n",
      "iterations  562 \n",
      " Loss_train:       [[ 0.71863863]] \n",
      " Loss_Validation:  [[ 0.67613755]]\n",
      "iterations  563 \n",
      " Loss_train:       [[ 0.71847511]] \n",
      " Loss_Validation:  [[ 0.67597193]]\n",
      "iterations  564 \n",
      " Loss_train:       [[ 0.7183127]] \n",
      " Loss_Validation:  [[ 0.6758063]]\n",
      "iterations  565 \n",
      " Loss_train:       [[ 0.71815216]] \n",
      " Loss_Validation:  [[ 0.67564215]]\n",
      "iterations  566 \n",
      " Loss_train:       [[ 0.71799162]] \n",
      " Loss_Validation:  [[ 0.67547801]]\n",
      "iterations  567 \n",
      " Loss_train:       [[ 0.71783108]] \n",
      " Loss_Validation:  [[ 0.67531386]]\n",
      "iterations  568 \n",
      " Loss_train:       [[ 0.71767054]] \n",
      " Loss_Validation:  [[ 0.67514971]]\n",
      "iterations  569 \n",
      " Loss_train:       [[ 0.71751]] \n",
      " Loss_Validation:  [[ 0.67498556]]\n",
      "iterations  570 \n",
      " Loss_train:       [[ 0.71734946]] \n",
      " Loss_Validation:  [[ 0.67482141]]\n",
      "iterations  571 \n",
      " Loss_train:       [[ 0.71718892]] \n",
      " Loss_Validation:  [[ 0.67465726]]\n",
      "iterations  572 \n",
      " Loss_train:       [[ 0.71702838]] \n",
      " Loss_Validation:  [[ 0.67449312]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iterations  573 \n",
      " Loss_train:       [[ 0.71686784]] \n",
      " Loss_Validation:  [[ 0.67432897]]\n",
      "iterations  574 \n",
      " Loss_train:       [[ 0.7167073]] \n",
      " Loss_Validation:  [[ 0.67416482]]\n",
      "iterations  575 \n",
      " Loss_train:       [[ 0.71654676]] \n",
      " Loss_Validation:  [[ 0.67400068]]\n",
      "iterations  576 \n",
      " Loss_train:       [[ 0.71638622]] \n",
      " Loss_Validation:  [[ 0.67383653]]\n",
      "iterations  577 \n",
      " Loss_train:       [[ 0.71622569]] \n",
      " Loss_Validation:  [[ 0.67367239]]\n",
      "iterations  578 \n",
      " Loss_train:       [[ 0.71606515]] \n",
      " Loss_Validation:  [[ 0.67350824]]\n",
      "iterations  579 \n",
      " Loss_train:       [[ 0.71590461]] \n",
      " Loss_Validation:  [[ 0.6733441]]\n",
      "iterations  580 \n",
      " Loss_train:       [[ 0.71574407]] \n",
      " Loss_Validation:  [[ 0.67317995]]\n",
      "iterations  581 \n",
      " Loss_train:       [[ 0.71558354]] \n",
      " Loss_Validation:  [[ 0.67301581]]\n",
      "iterations  582 \n",
      " Loss_train:       [[ 0.71542418]] \n",
      " Loss_Validation:  [[ 0.67285167]]\n",
      "iterations  583 \n",
      " Loss_train:       [[ 0.71526661]] \n",
      " Loss_Validation:  [[ 0.67268902]]\n",
      "iterations  584 \n",
      " Loss_train:       [[ 0.71510905]] \n",
      " Loss_Validation:  [[ 0.67252636]]\n",
      "iterations  585 \n",
      " Loss_train:       [[ 0.71495148]] \n",
      " Loss_Validation:  [[ 0.67236371]]\n",
      "iterations  586 \n",
      " Loss_train:       [[ 0.71479392]] \n",
      " Loss_Validation:  [[ 0.67220106]]\n",
      "iterations  587 \n",
      " Loss_train:       [[ 0.71463635]] \n",
      " Loss_Validation:  [[ 0.67203841]]\n",
      "iterations  588 \n",
      " Loss_train:       [[ 0.71447879]] \n",
      " Loss_Validation:  [[ 0.67187576]]\n",
      "iterations  589 \n",
      " Loss_train:       [[ 0.71432123]] \n",
      " Loss_Validation:  [[ 0.67171311]]\n",
      "iterations  590 \n",
      " Loss_train:       [[ 0.71416366]] \n",
      " Loss_Validation:  [[ 0.67155046]]\n",
      "iterations  591 \n",
      " Loss_train:       [[ 0.7140061]] \n",
      " Loss_Validation:  [[ 0.67138781]]\n",
      "iterations  592 \n",
      " Loss_train:       [[ 0.71384854]] \n",
      " Loss_Validation:  [[ 0.67122516]]\n",
      "iterations  593 \n",
      " Loss_train:       [[ 0.71369097]] \n",
      " Loss_Validation:  [[ 0.67106251]]\n",
      "iterations  594 \n",
      " Loss_train:       [[ 0.71353341]] \n",
      " Loss_Validation:  [[ 0.67089986]]\n",
      "iterations  595 \n",
      " Loss_train:       [[ 0.71337585]] \n",
      " Loss_Validation:  [[ 0.67073722]]\n",
      "iterations  596 \n",
      " Loss_train:       [[ 0.71321829]] \n",
      " Loss_Validation:  [[ 0.67057457]]\n",
      "iterations  597 \n",
      " Loss_train:       [[ 0.71306073]] \n",
      " Loss_Validation:  [[ 0.67041192]]\n",
      "iterations  598 \n",
      " Loss_train:       [[ 0.71290317]] \n",
      " Loss_Validation:  [[ 0.67024928]]\n",
      "iterations  599 \n",
      " Loss_train:       [[ 0.7127456]] \n",
      " Loss_Validation:  [[ 0.67008663]]\n",
      "iterations  600 \n",
      " Loss_train:       [[ 0.71258823]] \n",
      " Loss_Validation:  [[ 0.66992398]]\n",
      "iterations  601 \n",
      " Loss_train:       [[ 0.71243151]] \n",
      " Loss_Validation:  [[ 0.6697618]]\n",
      "iterations  602 \n",
      " Loss_train:       [[ 0.71227479]] \n",
      " Loss_Validation:  [[ 0.66959962]]\n",
      "iterations  603 \n",
      " Loss_train:       [[ 0.71211807]] \n",
      " Loss_Validation:  [[ 0.66943744]]\n",
      "iterations  604 \n",
      " Loss_train:       [[ 0.71196135]] \n",
      " Loss_Validation:  [[ 0.66927526]]\n",
      "iterations  605 \n",
      " Loss_train:       [[ 0.71180463]] \n",
      " Loss_Validation:  [[ 0.66911308]]\n",
      "iterations  606 \n",
      " Loss_train:       [[ 0.71164791]] \n",
      " Loss_Validation:  [[ 0.6689509]]\n",
      "iterations  607 \n",
      " Loss_train:       [[ 0.7114912]] \n",
      " Loss_Validation:  [[ 0.66878872]]\n",
      "iterations  608 \n",
      " Loss_train:       [[ 0.71133448]] \n",
      " Loss_Validation:  [[ 0.66862654]]\n",
      "iterations  609 \n",
      " Loss_train:       [[ 0.71117776]] \n",
      " Loss_Validation:  [[ 0.66846436]]\n",
      "iterations  610 \n",
      " Loss_train:       [[ 0.71102104]] \n",
      " Loss_Validation:  [[ 0.66830218]]\n",
      "iterations  611 \n",
      " Loss_train:       [[ 0.71086436]] \n",
      " Loss_Validation:  [[ 0.66814001]]\n",
      "iterations  612 \n",
      " Loss_train:       [[ 0.71071068]] \n",
      " Loss_Validation:  [[ 0.66797939]]\n",
      "iterations  613 \n",
      " Loss_train:       [[ 0.710557]] \n",
      " Loss_Validation:  [[ 0.66781877]]\n",
      "iterations  614 \n",
      " Loss_train:       [[ 0.71040331]] \n",
      " Loss_Validation:  [[ 0.66765815]]\n",
      "iterations  615 \n",
      " Loss_train:       [[ 0.71024963]] \n",
      " Loss_Validation:  [[ 0.66749753]]\n",
      "iterations  616 \n",
      " Loss_train:       [[ 0.71009595]] \n",
      " Loss_Validation:  [[ 0.66733691]]\n",
      "iterations  617 \n",
      " Loss_train:       [[ 0.70994227]] \n",
      " Loss_Validation:  [[ 0.66717629]]\n",
      "iterations  618 \n",
      " Loss_train:       [[ 0.70978859]] \n",
      " Loss_Validation:  [[ 0.66701568]]\n",
      "iterations  619 \n",
      " Loss_train:       [[ 0.70963491]] \n",
      " Loss_Validation:  [[ 0.66685506]]\n",
      "iterations  620 \n",
      " Loss_train:       [[ 0.70948123]] \n",
      " Loss_Validation:  [[ 0.66669444]]\n",
      "iterations  621 \n",
      " Loss_train:       [[ 0.70932755]] \n",
      " Loss_Validation:  [[ 0.66653383]]\n",
      "iterations  622 \n",
      " Loss_train:       [[ 0.70917387]] \n",
      " Loss_Validation:  [[ 0.66637321]]\n",
      "iterations  623 \n",
      " Loss_train:       [[ 0.70902019]] \n",
      " Loss_Validation:  [[ 0.6662126]]\n",
      "iterations  624 \n",
      " Loss_train:       [[ 0.70886651]] \n",
      " Loss_Validation:  [[ 0.66605198]]\n",
      "iterations  625 \n",
      " Loss_train:       [[ 0.70871284]] \n",
      " Loss_Validation:  [[ 0.66589137]]\n",
      "iterations  626 \n",
      " Loss_train:       [[ 0.70855916]] \n",
      " Loss_Validation:  [[ 0.66573075]]\n",
      "iterations  627 \n",
      " Loss_train:       [[ 0.70840548]] \n",
      " Loss_Validation:  [[ 0.66557014]]\n",
      "iterations  628 \n",
      " Loss_train:       [[ 0.7082518]] \n",
      " Loss_Validation:  [[ 0.66540952]]\n",
      "iterations  629 \n",
      " Loss_train:       [[ 0.70809813]] \n",
      " Loss_Validation:  [[ 0.66524891]]\n",
      "iterations  630 \n",
      " Loss_train:       [[ 0.70794445]] \n",
      " Loss_Validation:  [[ 0.6650883]]\n",
      "iterations  631 \n",
      " Loss_train:       [[ 0.70779077]] \n",
      " Loss_Validation:  [[ 0.66492769]]\n",
      "iterations  632 \n",
      " Loss_train:       [[ 0.7076371]] \n",
      " Loss_Validation:  [[ 0.66476707]]\n",
      "iterations  633 \n",
      " Loss_train:       [[ 0.70748342]] \n",
      " Loss_Validation:  [[ 0.66460646]]\n",
      "iterations  634 \n",
      " Loss_train:       [[ 0.70732975]] \n",
      " Loss_Validation:  [[ 0.66444585]]\n",
      "iterations  635 \n",
      " Loss_train:       [[ 0.70717607]] \n",
      " Loss_Validation:  [[ 0.66428524]]\n",
      "iterations  636 \n",
      " Loss_train:       [[ 0.7070224]] \n",
      " Loss_Validation:  [[ 0.66412463]]\n",
      "iterations  637 \n",
      " Loss_train:       [[ 0.70686872]] \n",
      " Loss_Validation:  [[ 0.66396402]]\n",
      "iterations  638 \n",
      " Loss_train:       [[ 0.70671505]] \n",
      " Loss_Validation:  [[ 0.66380341]]\n",
      "iterations  639 \n",
      " Loss_train:       [[ 0.70656137]] \n",
      " Loss_Validation:  [[ 0.6636428]]\n",
      "iterations  640 \n",
      " Loss_train:       [[ 0.7064077]] \n",
      " Loss_Validation:  [[ 0.66348219]]\n",
      "iterations  641 \n",
      " Loss_train:       [[ 0.70625403]] \n",
      " Loss_Validation:  [[ 0.66332158]]\n",
      "iterations  642 \n",
      " Loss_train:       [[ 0.70610035]] \n",
      " Loss_Validation:  [[ 0.66316098]]\n",
      "iterations  643 \n",
      " Loss_train:       [[ 0.70594668]] \n",
      " Loss_Validation:  [[ 0.66300037]]\n",
      "iterations  644 \n",
      " Loss_train:       [[ 0.70579301]] \n",
      " Loss_Validation:  [[ 0.66283976]]\n",
      "iterations  645 \n",
      " Loss_train:       [[ 0.70563934]] \n",
      " Loss_Validation:  [[ 0.66267916]]\n",
      "iterations  646 \n",
      " Loss_train:       [[ 0.70548566]] \n",
      " Loss_Validation:  [[ 0.66251855]]\n",
      "iterations  647 \n",
      " Loss_train:       [[ 0.70533199]] \n",
      " Loss_Validation:  [[ 0.66235794]]\n",
      "iterations  648 \n",
      " Loss_train:       [[ 0.70517832]] \n",
      " Loss_Validation:  [[ 0.66219734]]\n",
      "iterations  649 \n",
      " Loss_train:       [[ 0.70502509]] \n",
      " Loss_Validation:  [[ 0.66203673]]\n",
      "iterations  650 \n",
      " Loss_train:       [[ 0.70487313]] \n",
      " Loss_Validation:  [[ 0.66187718]]\n",
      "iterations  651 \n",
      " Loss_train:       [[ 0.70472118]] \n",
      " Loss_Validation:  [[ 0.66171763]]\n",
      "iterations  652 \n",
      " Loss_train:       [[ 0.70456922]] \n",
      " Loss_Validation:  [[ 0.66155808]]\n",
      "iterations  653 \n",
      " Loss_train:       [[ 0.70441726]] \n",
      " Loss_Validation:  [[ 0.66139853]]\n",
      "iterations  654 \n",
      " Loss_train:       [[ 0.70426531]] \n",
      " Loss_Validation:  [[ 0.66123898]]\n",
      "iterations  655 \n",
      " Loss_train:       [[ 0.70411335]] \n",
      " Loss_Validation:  [[ 0.66107943]]\n",
      "iterations  656 \n",
      " Loss_train:       [[ 0.70396245]] \n",
      " Loss_Validation:  [[ 0.66092049]]\n",
      "iterations  657 \n",
      " Loss_train:       [[ 0.70381377]] \n",
      " Loss_Validation:  [[ 0.66076472]]\n",
      "iterations  658 \n",
      " Loss_train:       [[ 0.70366509]] \n",
      " Loss_Validation:  [[ 0.66060895]]\n",
      "iterations  659 \n",
      " Loss_train:       [[ 0.70351641]] \n",
      " Loss_Validation:  [[ 0.66045319]]\n",
      "iterations  660 \n",
      " Loss_train:       [[ 0.70336772]] \n",
      " Loss_Validation:  [[ 0.66029792]]\n",
      "iterations  661 \n",
      " Loss_train:       [[ 0.70321904]] \n",
      " Loss_Validation:  [[ 0.66014335]]\n",
      "iterations  662 \n",
      " Loss_train:       [[ 0.70307045]] \n",
      " Loss_Validation:  [[ 0.65998879]]\n",
      "iterations  663 \n",
      " Loss_train:       [[ 0.70292477]] \n",
      " Loss_Validation:  [[ 0.65983576]]\n",
      "iterations  664 \n",
      " Loss_train:       [[ 0.70277909]] \n",
      " Loss_Validation:  [[ 0.65968273]]\n",
      "iterations  665 \n",
      " Loss_train:       [[ 0.70263342]] \n",
      " Loss_Validation:  [[ 0.6595297]]\n",
      "iterations  666 \n",
      " Loss_train:       [[ 0.70248774]] \n",
      " Loss_Validation:  [[ 0.65937667]]\n",
      "iterations  667 \n",
      " Loss_train:       [[ 0.70234206]] \n",
      " Loss_Validation:  [[ 0.65922364]]\n",
      "iterations  668 \n",
      " Loss_train:       [[ 0.70219639]] \n",
      " Loss_Validation:  [[ 0.65907062]]\n",
      "iterations  669 \n",
      " Loss_train:       [[ 0.70205071]] \n",
      " Loss_Validation:  [[ 0.65891759]]\n",
      "iterations  670 \n",
      " Loss_train:       [[ 0.70190503]] \n",
      " Loss_Validation:  [[ 0.65876456]]\n",
      "iterations  671 \n",
      " Loss_train:       [[ 0.70175936]] \n",
      " Loss_Validation:  [[ 0.65861153]]\n",
      "iterations  672 \n",
      " Loss_train:       [[ 0.70161368]] \n",
      " Loss_Validation:  [[ 0.65845851]]\n",
      "iterations  673 \n",
      " Loss_train:       [[ 0.70146812]] \n",
      " Loss_Validation:  [[ 0.65830548]]\n",
      "iterations  674 \n",
      " Loss_train:       [[ 0.70132324]] \n",
      " Loss_Validation:  [[ 0.65815281]]\n",
      "iterations  675 \n",
      " Loss_train:       [[ 0.70117836]] \n",
      " Loss_Validation:  [[ 0.65800014]]\n",
      "iterations  676 \n",
      " Loss_train:       [[ 0.70103349]] \n",
      " Loss_Validation:  [[ 0.65784747]]\n",
      "iterations  677 \n",
      " Loss_train:       [[ 0.70088861]] \n",
      " Loss_Validation:  [[ 0.65769481]]\n",
      "iterations  678 \n",
      " Loss_train:       [[ 0.70074374]] \n",
      " Loss_Validation:  [[ 0.65754214]]\n",
      "iterations  679 \n",
      " Loss_train:       [[ 0.70059886]] \n",
      " Loss_Validation:  [[ 0.65738947]]\n",
      "iterations  680 \n",
      " Loss_train:       [[ 0.70045398]] \n",
      " Loss_Validation:  [[ 0.6572368]]\n",
      "iterations  681 \n",
      " Loss_train:       [[ 0.70030911]] \n",
      " Loss_Validation:  [[ 0.65708413]]\n",
      "iterations  682 \n",
      " Loss_train:       [[ 0.70016423]] \n",
      " Loss_Validation:  [[ 0.65693147]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iterations  683 \n",
      " Loss_train:       [[ 0.70001936]] \n",
      " Loss_Validation:  [[ 0.6567788]]\n",
      "iterations  684 \n",
      " Loss_train:       [[ 0.69987449]] \n",
      " Loss_Validation:  [[ 0.65662614]]\n",
      "iterations  685 \n",
      " Loss_train:       [[ 0.69972961]] \n",
      " Loss_Validation:  [[ 0.65647347]]\n",
      "iterations  686 \n",
      " Loss_train:       [[ 0.69958474]] \n",
      " Loss_Validation:  [[ 0.6563208]]\n",
      "iterations  687 \n",
      " Loss_train:       [[ 0.69943986]] \n",
      " Loss_Validation:  [[ 0.65616814]]\n",
      "iterations  688 \n",
      " Loss_train:       [[ 0.69929499]] \n",
      " Loss_Validation:  [[ 0.65601547]]\n",
      "iterations  689 \n",
      " Loss_train:       [[ 0.69915012]] \n",
      " Loss_Validation:  [[ 0.65586281]]\n",
      "iterations  690 \n",
      " Loss_train:       [[ 0.69900525]] \n",
      " Loss_Validation:  [[ 0.65571088]]\n",
      "iterations  691 \n",
      " Loss_train:       [[ 0.69886037]] \n",
      " Loss_Validation:  [[ 0.65555949]]\n",
      "iterations  692 \n",
      " Loss_train:       [[ 0.6987155]] \n",
      " Loss_Validation:  [[ 0.65540811]]\n",
      "iterations  693 \n",
      " Loss_train:       [[ 0.69857063]] \n",
      " Loss_Validation:  [[ 0.65525673]]\n",
      "iterations  694 \n",
      " Loss_train:       [[ 0.69842576]] \n",
      " Loss_Validation:  [[ 0.65510535]]\n",
      "iterations  695 \n",
      " Loss_train:       [[ 0.69828089]] \n",
      " Loss_Validation:  [[ 0.65495397]]\n",
      "iterations  696 \n",
      " Loss_train:       [[ 0.69813602]] \n",
      " Loss_Validation:  [[ 0.65480259]]\n",
      "iterations  697 \n",
      " Loss_train:       [[ 0.69799115]] \n",
      " Loss_Validation:  [[ 0.65465121]]\n",
      "iterations  698 \n",
      " Loss_train:       [[ 0.69784628]] \n",
      " Loss_Validation:  [[ 0.65449983]]\n",
      "iterations  699 \n",
      " Loss_train:       [[ 0.69770141]] \n",
      " Loss_Validation:  [[ 0.65434845]]\n",
      "iterations  700 \n",
      " Loss_train:       [[ 0.69755654]] \n",
      " Loss_Validation:  [[ 0.65419707]]\n",
      "iterations  701 \n",
      " Loss_train:       [[ 0.69741177]] \n",
      " Loss_Validation:  [[ 0.65404569]]\n",
      "iterations  702 \n",
      " Loss_train:       [[ 0.69726999]] \n",
      " Loss_Validation:  [[ 0.6538959]]\n",
      "iterations  703 \n",
      " Loss_train:       [[ 0.69712821]] \n",
      " Loss_Validation:  [[ 0.65374612]]\n",
      "iterations  704 \n",
      " Loss_train:       [[ 0.69698643]] \n",
      " Loss_Validation:  [[ 0.65359633]]\n",
      "iterations  705 \n",
      " Loss_train:       [[ 0.69684465]] \n",
      " Loss_Validation:  [[ 0.65344654]]\n",
      "iterations  706 \n",
      " Loss_train:       [[ 0.69670286]] \n",
      " Loss_Validation:  [[ 0.65329675]]\n",
      "iterations  707 \n",
      " Loss_train:       [[ 0.69656108]] \n",
      " Loss_Validation:  [[ 0.65314696]]\n",
      "iterations  708 \n",
      " Loss_train:       [[ 0.6964193]] \n",
      " Loss_Validation:  [[ 0.65299718]]\n",
      "iterations  709 \n",
      " Loss_train:       [[ 0.69627752]] \n",
      " Loss_Validation:  [[ 0.65284739]]\n",
      "iterations  710 \n",
      " Loss_train:       [[ 0.69613574]] \n",
      " Loss_Validation:  [[ 0.6526976]]\n",
      "iterations  711 \n",
      " Loss_train:       [[ 0.69599396]] \n",
      " Loss_Validation:  [[ 0.65254782]]\n",
      "iterations  712 \n",
      " Loss_train:       [[ 0.69585218]] \n",
      " Loss_Validation:  [[ 0.65239803]]\n",
      "iterations  713 \n",
      " Loss_train:       [[ 0.6957104]] \n",
      " Loss_Validation:  [[ 0.65224825]]\n",
      "iterations  714 \n",
      " Loss_train:       [[ 0.69556863]] \n",
      " Loss_Validation:  [[ 0.65209846]]\n",
      "iterations  715 \n",
      " Loss_train:       [[ 0.69542685]] \n",
      " Loss_Validation:  [[ 0.65194868]]\n",
      "iterations  716 \n",
      " Loss_train:       [[ 0.69528507]] \n",
      " Loss_Validation:  [[ 0.65179889]]\n",
      "iterations  717 \n",
      " Loss_train:       [[ 0.69514329]] \n",
      " Loss_Validation:  [[ 0.65164911]]\n",
      "iterations  718 \n",
      " Loss_train:       [[ 0.69500151]] \n",
      " Loss_Validation:  [[ 0.65149933]]\n",
      "iterations  719 \n",
      " Loss_train:       [[ 0.69485974]] \n",
      " Loss_Validation:  [[ 0.65134954]]\n",
      "iterations  720 \n",
      " Loss_train:       [[ 0.69471796]] \n",
      " Loss_Validation:  [[ 0.65119976]]\n",
      "iterations  721 \n",
      " Loss_train:       [[ 0.69457618]] \n",
      " Loss_Validation:  [[ 0.65104998]]\n",
      "iterations  722 \n",
      " Loss_train:       [[ 0.69443521]] \n",
      " Loss_Validation:  [[ 0.6509002]]\n",
      "iterations  723 \n",
      " Loss_train:       [[ 0.69429516]] \n",
      " Loss_Validation:  [[ 0.65075147]]\n",
      "iterations  724 \n",
      " Loss_train:       [[ 0.6941551]] \n",
      " Loss_Validation:  [[ 0.65060275]]\n",
      "iterations  725 \n",
      " Loss_train:       [[ 0.69401504]] \n",
      " Loss_Validation:  [[ 0.65045402]]\n",
      "iterations  726 \n",
      " Loss_train:       [[ 0.69387498]] \n",
      " Loss_Validation:  [[ 0.6503053]]\n",
      "iterations  727 \n",
      " Loss_train:       [[ 0.69373493]] \n",
      " Loss_Validation:  [[ 0.65015658]]\n",
      "iterations  728 \n",
      " Loss_train:       [[ 0.69359487]] \n",
      " Loss_Validation:  [[ 0.65000785]]\n",
      "iterations  729 \n",
      " Loss_train:       [[ 0.69345482]] \n",
      " Loss_Validation:  [[ 0.64985913]]\n",
      "iterations  730 \n",
      " Loss_train:       [[ 0.69331476]] \n",
      " Loss_Validation:  [[ 0.64971041]]\n",
      "iterations  731 \n",
      " Loss_train:       [[ 0.6931747]] \n",
      " Loss_Validation:  [[ 0.64956168]]\n",
      "iterations  732 \n",
      " Loss_train:       [[ 0.69303465]] \n",
      " Loss_Validation:  [[ 0.64941296]]\n",
      "iterations  733 \n",
      " Loss_train:       [[ 0.69289459]] \n",
      " Loss_Validation:  [[ 0.64926424]]\n",
      "iterations  734 \n",
      " Loss_train:       [[ 0.69275454]] \n",
      " Loss_Validation:  [[ 0.64911552]]\n",
      "iterations  735 \n",
      " Loss_train:       [[ 0.69261449]] \n",
      " Loss_Validation:  [[ 0.6489668]]\n",
      "iterations  736 \n",
      " Loss_train:       [[ 0.69247443]] \n",
      " Loss_Validation:  [[ 0.64881808]]\n",
      "iterations  737 \n",
      " Loss_train:       [[ 0.69233438]] \n",
      " Loss_Validation:  [[ 0.64866936]]\n",
      "iterations  738 \n",
      " Loss_train:       [[ 0.69219432]] \n",
      " Loss_Validation:  [[ 0.64852064]]\n",
      "iterations  739 \n",
      " Loss_train:       [[ 0.69205427]] \n",
      " Loss_Validation:  [[ 0.64837192]]\n",
      "iterations  740 \n",
      " Loss_train:       [[ 0.69191422]] \n",
      " Loss_Validation:  [[ 0.6482232]]\n",
      "iterations  741 \n",
      " Loss_train:       [[ 0.69177417]] \n",
      " Loss_Validation:  [[ 0.64807448]]\n",
      "iterations  742 \n",
      " Loss_train:       [[ 0.69163411]] \n",
      " Loss_Validation:  [[ 0.64792577]]\n",
      "iterations  743 \n",
      " Loss_train:       [[ 0.69149406]] \n",
      " Loss_Validation:  [[ 0.64777705]]\n",
      "iterations  744 \n",
      " Loss_train:       [[ 0.69135401]] \n",
      " Loss_Validation:  [[ 0.64762833]]\n",
      "iterations  745 \n",
      " Loss_train:       [[ 0.69121396]] \n",
      " Loss_Validation:  [[ 0.64747961]]\n",
      "iterations  746 \n",
      " Loss_train:       [[ 0.69107391]] \n",
      " Loss_Validation:  [[ 0.6473309]]\n",
      "iterations  747 \n",
      " Loss_train:       [[ 0.69093386]] \n",
      " Loss_Validation:  [[ 0.64718218]]\n",
      "iterations  748 \n",
      " Loss_train:       [[ 0.69079381]] \n",
      " Loss_Validation:  [[ 0.64703346]]\n",
      "iterations  749 \n",
      " Loss_train:       [[ 0.69065375]] \n",
      " Loss_Validation:  [[ 0.64688475]]\n",
      "iterations  750 \n",
      " Loss_train:       [[ 0.6905137]] \n",
      " Loss_Validation:  [[ 0.64673603]]\n",
      "iterations  751 \n",
      " Loss_train:       [[ 0.69037365]] \n",
      " Loss_Validation:  [[ 0.64658732]]\n",
      "iterations  752 \n",
      " Loss_train:       [[ 0.69023361]] \n",
      " Loss_Validation:  [[ 0.6464386]]\n",
      "iterations  753 \n",
      " Loss_train:       [[ 0.69009356]] \n",
      " Loss_Validation:  [[ 0.64628989]]\n",
      "iterations  754 \n",
      " Loss_train:       [[ 0.68995351]] \n",
      " Loss_Validation:  [[ 0.64614118]]\n",
      "iterations  755 \n",
      " Loss_train:       [[ 0.68981346]] \n",
      " Loss_Validation:  [[ 0.64599246]]\n",
      "iterations  756 \n",
      " Loss_train:       [[ 0.68967341]] \n",
      " Loss_Validation:  [[ 0.64584375]]\n",
      "iterations  757 \n",
      " Loss_train:       [[ 0.68953336]] \n",
      " Loss_Validation:  [[ 0.64569504]]\n",
      "iterations  758 \n",
      " Loss_train:       [[ 0.68939331]] \n",
      " Loss_Validation:  [[ 0.64554632]]\n",
      "iterations  759 \n",
      " Loss_train:       [[ 0.68925379]] \n",
      " Loss_Validation:  [[ 0.64539761]]\n",
      "iterations  760 \n",
      " Loss_train:       [[ 0.68911687]] \n",
      " Loss_Validation:  [[ 0.64525074]]\n",
      "iterations  761 \n",
      " Loss_train:       [[ 0.68897994]] \n",
      " Loss_Validation:  [[ 0.64510386]]\n",
      "iterations  762 \n",
      " Loss_train:       [[ 0.68884302]] \n",
      " Loss_Validation:  [[ 0.64495698]]\n",
      "iterations  763 \n",
      " Loss_train:       [[ 0.68870609]] \n",
      " Loss_Validation:  [[ 0.64481011]]\n",
      "iterations  764 \n",
      " Loss_train:       [[ 0.68856917]] \n",
      " Loss_Validation:  [[ 0.64466323]]\n",
      "iterations  765 \n",
      " Loss_train:       [[ 0.68843225]] \n",
      " Loss_Validation:  [[ 0.64451636]]\n",
      "iterations  766 \n",
      " Loss_train:       [[ 0.68829532]] \n",
      " Loss_Validation:  [[ 0.64436948]]\n",
      "iterations  767 \n",
      " Loss_train:       [[ 0.6881584]] \n",
      " Loss_Validation:  [[ 0.64422261]]\n",
      "iterations  768 \n",
      " Loss_train:       [[ 0.68802148]] \n",
      " Loss_Validation:  [[ 0.64407573]]\n",
      "iterations  769 \n",
      " Loss_train:       [[ 0.68788456]] \n",
      " Loss_Validation:  [[ 0.64392886]]\n",
      "iterations  770 \n",
      " Loss_train:       [[ 0.68774763]] \n",
      " Loss_Validation:  [[ 0.64378198]]\n",
      "iterations  771 \n",
      " Loss_train:       [[ 0.68761071]] \n",
      " Loss_Validation:  [[ 0.64363511]]\n",
      "iterations  772 \n",
      " Loss_train:       [[ 0.68747379]] \n",
      " Loss_Validation:  [[ 0.64348824]]\n",
      "iterations  773 \n",
      " Loss_train:       [[ 0.68733687]] \n",
      " Loss_Validation:  [[ 0.64334137]]\n",
      "iterations  774 \n",
      " Loss_train:       [[ 0.68719995]] \n",
      " Loss_Validation:  [[ 0.64319449]]\n",
      "iterations  775 \n",
      " Loss_train:       [[ 0.68706303]] \n",
      " Loss_Validation:  [[ 0.64304762]]\n",
      "iterations  776 \n",
      " Loss_train:       [[ 0.68692611]] \n",
      " Loss_Validation:  [[ 0.64290075]]\n",
      "iterations  777 \n",
      " Loss_train:       [[ 0.68678919]] \n",
      " Loss_Validation:  [[ 0.64275388]]\n",
      "iterations  778 \n",
      " Loss_train:       [[ 0.68665227]] \n",
      " Loss_Validation:  [[ 0.64260701]]\n",
      "iterations  779 \n",
      " Loss_train:       [[ 0.68651535]] \n",
      " Loss_Validation:  [[ 0.64246014]]\n",
      "iterations  780 \n",
      " Loss_train:       [[ 0.68637843]] \n",
      " Loss_Validation:  [[ 0.64231327]]\n",
      "iterations  781 \n",
      " Loss_train:       [[ 0.68624151]] \n",
      " Loss_Validation:  [[ 0.6421664]]\n",
      "iterations  782 \n",
      " Loss_train:       [[ 0.68610459]] \n",
      " Loss_Validation:  [[ 0.64201953]]\n",
      "iterations  783 \n",
      " Loss_train:       [[ 0.68596768]] \n",
      " Loss_Validation:  [[ 0.64187266]]\n",
      "iterations  784 \n",
      " Loss_train:       [[ 0.68583076]] \n",
      " Loss_Validation:  [[ 0.64172579]]\n",
      "iterations  785 \n",
      " Loss_train:       [[ 0.68569384]] \n",
      " Loss_Validation:  [[ 0.64157892]]\n",
      "iterations  786 \n",
      " Loss_train:       [[ 0.68555692]] \n",
      " Loss_Validation:  [[ 0.64143206]]\n",
      "iterations  787 \n",
      " Loss_train:       [[ 0.68542001]] \n",
      " Loss_Validation:  [[ 0.64128519]]\n",
      "iterations  788 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Loss_train:       [[ 0.68528309]] \n",
      " Loss_Validation:  [[ 0.64113832]]\n",
      "iterations  789 \n",
      " Loss_train:       [[ 0.68514617]] \n",
      " Loss_Validation:  [[ 0.64099145]]\n",
      "iterations  790 \n",
      " Loss_train:       [[ 0.68500926]] \n",
      " Loss_Validation:  [[ 0.64084459]]\n",
      "iterations  791 \n",
      " Loss_train:       [[ 0.68487234]] \n",
      " Loss_Validation:  [[ 0.64069772]]\n",
      "iterations  792 \n",
      " Loss_train:       [[ 0.68473542]] \n",
      " Loss_Validation:  [[ 0.64055085]]\n",
      "iterations  793 \n",
      " Loss_train:       [[ 0.68459851]] \n",
      " Loss_Validation:  [[ 0.64040399]]\n",
      "iterations  794 \n",
      " Loss_train:       [[ 0.68446159]] \n",
      " Loss_Validation:  [[ 0.64025712]]\n",
      "iterations  795 \n",
      " Loss_train:       [[ 0.68432468]] \n",
      " Loss_Validation:  [[ 0.64011026]]\n",
      "iterations  796 \n",
      " Loss_train:       [[ 0.68418776]] \n",
      " Loss_Validation:  [[ 0.6399634]]\n",
      "iterations  797 \n",
      " Loss_train:       [[ 0.68405085]] \n",
      " Loss_Validation:  [[ 0.63981653]]\n",
      "iterations  798 \n",
      " Loss_train:       [[ 0.68391393]] \n",
      " Loss_Validation:  [[ 0.63966967]]\n",
      "iterations  799 \n",
      " Loss_train:       [[ 0.68377702]] \n",
      " Loss_Validation:  [[ 0.6395228]]\n",
      "iterations  800 \n",
      " Loss_train:       [[ 0.68364011]] \n",
      " Loss_Validation:  [[ 0.63937594]]\n",
      "iterations  801 \n",
      " Loss_train:       [[ 0.68350319]] \n",
      " Loss_Validation:  [[ 0.63922908]]\n",
      "iterations  802 \n",
      " Loss_train:       [[ 0.68336628]] \n",
      " Loss_Validation:  [[ 0.63908222]]\n",
      "iterations  803 \n",
      " Loss_train:       [[ 0.68322937]] \n",
      " Loss_Validation:  [[ 0.63893535]]\n",
      "iterations  804 \n",
      " Loss_train:       [[ 0.68309246]] \n",
      " Loss_Validation:  [[ 0.63878849]]\n",
      "iterations  805 \n",
      " Loss_train:       [[ 0.68295554]] \n",
      " Loss_Validation:  [[ 0.63864163]]\n",
      "iterations  806 \n",
      " Loss_train:       [[ 0.68281863]] \n",
      " Loss_Validation:  [[ 0.63849477]]\n",
      "iterations  807 \n",
      " Loss_train:       [[ 0.68268172]] \n",
      " Loss_Validation:  [[ 0.63834791]]\n",
      "iterations  808 \n",
      " Loss_train:       [[ 0.68254481]] \n",
      " Loss_Validation:  [[ 0.63820105]]\n",
      "iterations  809 \n",
      " Loss_train:       [[ 0.6824079]] \n",
      " Loss_Validation:  [[ 0.63805419]]\n",
      "iterations  810 \n",
      " Loss_train:       [[ 0.68227099]] \n",
      " Loss_Validation:  [[ 0.63790733]]\n",
      "iterations  811 \n",
      " Loss_train:       [[ 0.68213408]] \n",
      " Loss_Validation:  [[ 0.63776047]]\n",
      "iterations  812 \n",
      " Loss_train:       [[ 0.68199717]] \n",
      " Loss_Validation:  [[ 0.63761361]]\n",
      "iterations  813 \n",
      " Loss_train:       [[ 0.68186026]] \n",
      " Loss_Validation:  [[ 0.63746675]]\n",
      "iterations  814 \n",
      " Loss_train:       [[ 0.68172335]] \n",
      " Loss_Validation:  [[ 0.6373199]]\n",
      "iterations  815 \n",
      " Loss_train:       [[ 0.68158644]] \n",
      " Loss_Validation:  [[ 0.63717304]]\n",
      "iterations  816 \n",
      " Loss_train:       [[ 0.68144953]] \n",
      " Loss_Validation:  [[ 0.63702618]]\n",
      "iterations  817 \n",
      " Loss_train:       [[ 0.68131262]] \n",
      " Loss_Validation:  [[ 0.63687932]]\n",
      "iterations  818 \n",
      " Loss_train:       [[ 0.68117571]] \n",
      " Loss_Validation:  [[ 0.63673247]]\n",
      "iterations  819 \n",
      " Loss_train:       [[ 0.6810388]] \n",
      " Loss_Validation:  [[ 0.63658561]]\n",
      "iterations  820 \n",
      " Loss_train:       [[ 0.68090189]] \n",
      " Loss_Validation:  [[ 0.63643876]]\n",
      "iterations  821 \n",
      " Loss_train:       [[ 0.68076498]] \n",
      " Loss_Validation:  [[ 0.6362919]]\n",
      "iterations  822 \n",
      " Loss_train:       [[ 0.68062808]] \n",
      " Loss_Validation:  [[ 0.63614504]]\n",
      "iterations  823 \n",
      " Loss_train:       [[ 0.68049117]] \n",
      " Loss_Validation:  [[ 0.63599819]]\n",
      "iterations  824 \n",
      " Loss_train:       [[ 0.68035426]] \n",
      " Loss_Validation:  [[ 0.63585134]]\n",
      "iterations  825 \n",
      " Loss_train:       [[ 0.68021736]] \n",
      " Loss_Validation:  [[ 0.63570448]]\n",
      "iterations  826 \n",
      " Loss_train:       [[ 0.68008045]] \n",
      " Loss_Validation:  [[ 0.63555763]]\n",
      "iterations  827 \n",
      " Loss_train:       [[ 0.67994354]] \n",
      " Loss_Validation:  [[ 0.63541077]]\n",
      "iterations  828 \n",
      " Loss_train:       [[ 0.67980664]] \n",
      " Loss_Validation:  [[ 0.63526392]]\n",
      "iterations  829 \n",
      " Loss_train:       [[ 0.67966973]] \n",
      " Loss_Validation:  [[ 0.63511707]]\n",
      "iterations  830 \n",
      " Loss_train:       [[ 0.67953283]] \n",
      " Loss_Validation:  [[ 0.63497022]]\n",
      "iterations  831 \n",
      " Loss_train:       [[ 0.67939592]] \n",
      " Loss_Validation:  [[ 0.63482336]]\n",
      "iterations  832 \n",
      " Loss_train:       [[ 0.67925902]] \n",
      " Loss_Validation:  [[ 0.63467651]]\n",
      "iterations  833 \n",
      " Loss_train:       [[ 0.67912211]] \n",
      " Loss_Validation:  [[ 0.63452966]]\n",
      "iterations  834 \n",
      " Loss_train:       [[ 0.67898521]] \n",
      " Loss_Validation:  [[ 0.63438296]]\n",
      "iterations  835 \n",
      " Loss_train:       [[ 0.6788483]] \n",
      " Loss_Validation:  [[ 0.63423708]]\n",
      "iterations  836 \n",
      " Loss_train:       [[ 0.6787114]] \n",
      " Loss_Validation:  [[ 0.63409119]]\n",
      "iterations  837 \n",
      " Loss_train:       [[ 0.6785745]] \n",
      " Loss_Validation:  [[ 0.63394531]]\n",
      "iterations  838 \n",
      " Loss_train:       [[ 0.67843759]] \n",
      " Loss_Validation:  [[ 0.63379943]]\n",
      "iterations  839 \n",
      " Loss_train:       [[ 0.67830069]] \n",
      " Loss_Validation:  [[ 0.63365355]]\n",
      "iterations  840 \n",
      " Loss_train:       [[ 0.67816379]] \n",
      " Loss_Validation:  [[ 0.63350766]]\n",
      "iterations  841 \n",
      " Loss_train:       [[ 0.67802688]] \n",
      " Loss_Validation:  [[ 0.63336178]]\n",
      "iterations  842 \n",
      " Loss_train:       [[ 0.67788998]] \n",
      " Loss_Validation:  [[ 0.6332159]]\n",
      "iterations  843 \n",
      " Loss_train:       [[ 0.67775308]] \n",
      " Loss_Validation:  [[ 0.63307002]]\n",
      "iterations  844 \n",
      " Loss_train:       [[ 0.67761648]] \n",
      " Loss_Validation:  [[ 0.63292414]]\n",
      "iterations  845 \n",
      " Loss_train:       [[ 0.67748066]] \n",
      " Loss_Validation:  [[ 0.63277882]]\n",
      "iterations  846 \n",
      " Loss_train:       [[ 0.67734484]] \n",
      " Loss_Validation:  [[ 0.6326335]]\n",
      "iterations  847 \n",
      " Loss_train:       [[ 0.67720902]] \n",
      " Loss_Validation:  [[ 0.63248818]]\n",
      "iterations  848 \n",
      " Loss_train:       [[ 0.6770732]] \n",
      " Loss_Validation:  [[ 0.63234287]]\n",
      "iterations  849 \n",
      " Loss_train:       [[ 0.67693738]] \n",
      " Loss_Validation:  [[ 0.63219755]]\n",
      "iterations  850 \n",
      " Loss_train:       [[ 0.67680156]] \n",
      " Loss_Validation:  [[ 0.63205223]]\n",
      "iterations  851 \n",
      " Loss_train:       [[ 0.67666574]] \n",
      " Loss_Validation:  [[ 0.63190692]]\n",
      "iterations  852 \n",
      " Loss_train:       [[ 0.67652992]] \n",
      " Loss_Validation:  [[ 0.6317616]]\n",
      "iterations  853 \n",
      " Loss_train:       [[ 0.67639411]] \n",
      " Loss_Validation:  [[ 0.63161628]]\n",
      "iterations  854 \n",
      " Loss_train:       [[ 0.67625829]] \n",
      " Loss_Validation:  [[ 0.63147097]]\n",
      "iterations  855 \n",
      " Loss_train:       [[ 0.67612247]] \n",
      " Loss_Validation:  [[ 0.63132565]]\n",
      "iterations  856 \n",
      " Loss_train:       [[ 0.67598665]] \n",
      " Loss_Validation:  [[ 0.63118034]]\n",
      "iterations  857 \n",
      " Loss_train:       [[ 0.67585084]] \n",
      " Loss_Validation:  [[ 0.63103502]]\n",
      "iterations  858 \n",
      " Loss_train:       [[ 0.67571502]] \n",
      " Loss_Validation:  [[ 0.63088971]]\n",
      "iterations  859 \n",
      " Loss_train:       [[ 0.67557921]] \n",
      " Loss_Validation:  [[ 0.6307444]]\n",
      "iterations  860 \n",
      " Loss_train:       [[ 0.67544339]] \n",
      " Loss_Validation:  [[ 0.63059908]]\n",
      "iterations  861 \n",
      " Loss_train:       [[ 0.67530757]] \n",
      " Loss_Validation:  [[ 0.63045377]]\n",
      "iterations  862 \n",
      " Loss_train:       [[ 0.67517176]] \n",
      " Loss_Validation:  [[ 0.63030846]]\n",
      "iterations  863 \n",
      " Loss_train:       [[ 0.67503703]] \n",
      " Loss_Validation:  [[ 0.63016314]]\n",
      "iterations  864 \n",
      " Loss_train:       [[ 0.67490417]] \n",
      " Loss_Validation:  [[ 0.63001955]]\n",
      "iterations  865 \n",
      " Loss_train:       [[ 0.67477136]] \n",
      " Loss_Validation:  [[ 0.62987595]]\n",
      "iterations  866 \n",
      " Loss_train:       [[ 0.67464133]] \n",
      " Loss_Validation:  [[ 0.62973388]]\n",
      "iterations  867 \n",
      " Loss_train:       [[ 0.67451129]] \n",
      " Loss_Validation:  [[ 0.62959181]]\n",
      "iterations  868 \n",
      " Loss_train:       [[ 0.67438126]] \n",
      " Loss_Validation:  [[ 0.62944974]]\n",
      "iterations  869 \n",
      " Loss_train:       [[ 0.67425123]] \n",
      " Loss_Validation:  [[ 0.62930767]]\n",
      "iterations  870 \n",
      " Loss_train:       [[ 0.6741212]] \n",
      " Loss_Validation:  [[ 0.6291656]]\n",
      "iterations  871 \n",
      " Loss_train:       [[ 0.67399116]] \n",
      " Loss_Validation:  [[ 0.62902353]]\n",
      "iterations  872 \n",
      " Loss_train:       [[ 0.67386113]] \n",
      " Loss_Validation:  [[ 0.62888146]]\n",
      "iterations  873 \n",
      " Loss_train:       [[ 0.6737311]] \n",
      " Loss_Validation:  [[ 0.62873939]]\n",
      "iterations  874 \n",
      " Loss_train:       [[ 0.67360107]] \n",
      " Loss_Validation:  [[ 0.62859732]]\n",
      "iterations  875 \n",
      " Loss_train:       [[ 0.67347104]] \n",
      " Loss_Validation:  [[ 0.62845525]]\n",
      "iterations  876 \n",
      " Loss_train:       [[ 0.67334101]] \n",
      " Loss_Validation:  [[ 0.62831318]]\n",
      "iterations  877 \n",
      " Loss_train:       [[ 0.67321098]] \n",
      " Loss_Validation:  [[ 0.62817111]]\n",
      "iterations  878 \n",
      " Loss_train:       [[ 0.67308095]] \n",
      " Loss_Validation:  [[ 0.62802905]]\n",
      "iterations  879 \n",
      " Loss_train:       [[ 0.67295092]] \n",
      " Loss_Validation:  [[ 0.62788698]]\n",
      "iterations  880 \n",
      " Loss_train:       [[ 0.67282089]] \n",
      " Loss_Validation:  [[ 0.62774491]]\n",
      "iterations  881 \n",
      " Loss_train:       [[ 0.67269086]] \n",
      " Loss_Validation:  [[ 0.62760285]]\n",
      "iterations  882 \n",
      " Loss_train:       [[ 0.67256083]] \n",
      " Loss_Validation:  [[ 0.62746078]]\n",
      "iterations  883 \n",
      " Loss_train:       [[ 0.6724308]] \n",
      " Loss_Validation:  [[ 0.62731871]]\n",
      "iterations  884 \n",
      " Loss_train:       [[ 0.67230077]] \n",
      " Loss_Validation:  [[ 0.62717665]]\n",
      "iterations  885 \n",
      " Loss_train:       [[ 0.67217074]] \n",
      " Loss_Validation:  [[ 0.62703458]]\n",
      "iterations  886 \n",
      " Loss_train:       [[ 0.67204071]] \n",
      " Loss_Validation:  [[ 0.62689315]]\n",
      "iterations  887 \n",
      " Loss_train:       [[ 0.67191069]] \n",
      " Loss_Validation:  [[ 0.6267531]]\n",
      "iterations  888 \n",
      " Loss_train:       [[ 0.67178066]] \n",
      " Loss_Validation:  [[ 0.62661394]]\n",
      "iterations  889 \n",
      " Loss_train:       [[ 0.67165063]] \n",
      " Loss_Validation:  [[ 0.62647502]]\n",
      "iterations  890 \n",
      " Loss_train:       [[ 0.6715206]] \n",
      " Loss_Validation:  [[ 0.62633611]]\n",
      "iterations  891 \n",
      " Loss_train:       [[ 0.67139058]] \n",
      " Loss_Validation:  [[ 0.62619777]]\n",
      "iterations  892 \n",
      " Loss_train:       [[ 0.67126055]] \n",
      " Loss_Validation:  [[ 0.6260601]]\n",
      "iterations  893 \n",
      " Loss_train:       [[ 0.67113052]] \n",
      " Loss_Validation:  [[ 0.62592244]]\n",
      "iterations  894 \n",
      " Loss_train:       [[ 0.6710005]] \n",
      " Loss_Validation:  [[ 0.62578478]]\n",
      "iterations  895 \n",
      " Loss_train:       [[ 0.67087047]] \n",
      " Loss_Validation:  [[ 0.62564712]]\n",
      "iterations  896 \n",
      " Loss_train:       [[ 0.67074045]] \n",
      " Loss_Validation:  [[ 0.62550946]]\n",
      "iterations  897 \n",
      " Loss_train:       [[ 0.67061042]] \n",
      " Loss_Validation:  [[ 0.6253718]]\n",
      "iterations  898 \n",
      " Loss_train:       [[ 0.6704804]] \n",
      " Loss_Validation:  [[ 0.62523414]]\n",
      "iterations  899 \n",
      " Loss_train:       [[ 0.67035037]] \n",
      " Loss_Validation:  [[ 0.62509648]]\n",
      "iterations  900 \n",
      " Loss_train:       [[ 0.67022035]] \n",
      " Loss_Validation:  [[ 0.62495882]]\n",
      "iterations  901 \n",
      " Loss_train:       [[ 0.67009032]] \n",
      " Loss_Validation:  [[ 0.62482116]]\n",
      "iterations  902 \n",
      " Loss_train:       [[ 0.6699603]] \n",
      " Loss_Validation:  [[ 0.6246835]]\n",
      "iterations  903 \n",
      " Loss_train:       [[ 0.66983027]] \n",
      " Loss_Validation:  [[ 0.62454584]]\n",
      "iterations  904 \n",
      " Loss_train:       [[ 0.66970025]] \n",
      " Loss_Validation:  [[ 0.62440818]]\n",
      "iterations  905 \n",
      " Loss_train:       [[ 0.66957023]] \n",
      " Loss_Validation:  [[ 0.62427052]]\n",
      "iterations  906 \n",
      " Loss_train:       [[ 0.6694402]] \n",
      " Loss_Validation:  [[ 0.62413287]]\n",
      "iterations  907 \n",
      " Loss_train:       [[ 0.66931018]] \n",
      " Loss_Validation:  [[ 0.62399521]]\n",
      "iterations  908 \n",
      " Loss_train:       [[ 0.66918016]] \n",
      " Loss_Validation:  [[ 0.62385755]]\n",
      "iterations  909 \n",
      " Loss_train:       [[ 0.66905014]] \n",
      " Loss_Validation:  [[ 0.62371989]]\n",
      "iterations  910 \n",
      " Loss_train:       [[ 0.66892152]] \n",
      " Loss_Validation:  [[ 0.62358224]]\n",
      "iterations  911 \n",
      " Loss_train:       [[ 0.6687943]] \n",
      " Loss_Validation:  [[ 0.62344604]]\n",
      "iterations  912 \n",
      " Loss_train:       [[ 0.66866708]] \n",
      " Loss_Validation:  [[ 0.62330983]]\n",
      "iterations  913 \n",
      " Loss_train:       [[ 0.66853985]] \n",
      " Loss_Validation:  [[ 0.62317363]]\n",
      "iterations  914 \n",
      " Loss_train:       [[ 0.66841263]] \n",
      " Loss_Validation:  [[ 0.62303743]]\n",
      "iterations  915 \n",
      " Loss_train:       [[ 0.66828541]] \n",
      " Loss_Validation:  [[ 0.62290123]]\n",
      "iterations  916 \n",
      " Loss_train:       [[ 0.66815818]] \n",
      " Loss_Validation:  [[ 0.62276503]]\n",
      "iterations  917 \n",
      " Loss_train:       [[ 0.66803096]] \n",
      " Loss_Validation:  [[ 0.62262883]]\n",
      "iterations  918 \n",
      " Loss_train:       [[ 0.66790374]] \n",
      " Loss_Validation:  [[ 0.6224933]]\n",
      "iterations  919 \n",
      " Loss_train:       [[ 0.66777652]] \n",
      " Loss_Validation:  [[ 0.62235911]]\n",
      "iterations  920 \n",
      " Loss_train:       [[ 0.66764929]] \n",
      " Loss_Validation:  [[ 0.62222492]]\n",
      "iterations  921 \n",
      " Loss_train:       [[ 0.66752207]] \n",
      " Loss_Validation:  [[ 0.62209073]]\n",
      "iterations  922 \n",
      " Loss_train:       [[ 0.66739485]] \n",
      " Loss_Validation:  [[ 0.62195654]]\n",
      "iterations  923 \n",
      " Loss_train:       [[ 0.66726763]] \n",
      " Loss_Validation:  [[ 0.62182235]]\n",
      "iterations  924 \n",
      " Loss_train:       [[ 0.66714041]] \n",
      " Loss_Validation:  [[ 0.62168817]]\n",
      "iterations  925 \n",
      " Loss_train:       [[ 0.66701319]] \n",
      " Loss_Validation:  [[ 0.62155398]]\n",
      "iterations  926 \n",
      " Loss_train:       [[ 0.66688597]] \n",
      " Loss_Validation:  [[ 0.62141979]]\n",
      "iterations  927 \n",
      " Loss_train:       [[ 0.66675875]] \n",
      " Loss_Validation:  [[ 0.6212856]]\n",
      "iterations  928 \n",
      " Loss_train:       [[ 0.66663153]] \n",
      " Loss_Validation:  [[ 0.62115142]]\n",
      "iterations  929 \n",
      " Loss_train:       [[ 0.66650431]] \n",
      " Loss_Validation:  [[ 0.62101723]]\n",
      "iterations  930 \n",
      " Loss_train:       [[ 0.66637709]] \n",
      " Loss_Validation:  [[ 0.62088304]]\n",
      "iterations  931 \n",
      " Loss_train:       [[ 0.66624987]] \n",
      " Loss_Validation:  [[ 0.62074886]]\n",
      "iterations  932 \n",
      " Loss_train:       [[ 0.66612265]] \n",
      " Loss_Validation:  [[ 0.62061467]]\n",
      "iterations  933 \n",
      " Loss_train:       [[ 0.66599543]] \n",
      " Loss_Validation:  [[ 0.62048048]]\n",
      "iterations  934 \n",
      " Loss_train:       [[ 0.66586822]] \n",
      " Loss_Validation:  [[ 0.6203463]]\n",
      "iterations  935 \n",
      " Loss_train:       [[ 0.665741]] \n",
      " Loss_Validation:  [[ 0.62021211]]\n",
      "iterations  936 \n",
      " Loss_train:       [[ 0.66561378]] \n",
      " Loss_Validation:  [[ 0.62007793]]\n",
      "iterations  937 \n",
      " Loss_train:       [[ 0.66548656]] \n",
      " Loss_Validation:  [[ 0.61994375]]\n",
      "iterations  938 \n",
      " Loss_train:       [[ 0.66535934]] \n",
      " Loss_Validation:  [[ 0.61980956]]\n",
      "iterations  939 \n",
      " Loss_train:       [[ 0.66523213]] \n",
      " Loss_Validation:  [[ 0.61967538]]\n",
      "iterations  940 \n",
      " Loss_train:       [[ 0.66510491]] \n",
      " Loss_Validation:  [[ 0.61954119]]\n",
      "iterations  941 \n",
      " Loss_train:       [[ 0.66497769]] \n",
      " Loss_Validation:  [[ 0.61940701]]\n",
      "iterations  942 \n",
      " Loss_train:       [[ 0.66485048]] \n",
      " Loss_Validation:  [[ 0.61927283]]\n",
      "iterations  943 \n",
      " Loss_train:       [[ 0.66472326]] \n",
      " Loss_Validation:  [[ 0.61913865]]\n",
      "iterations  944 \n",
      " Loss_train:       [[ 0.66459605]] \n",
      " Loss_Validation:  [[ 0.61900446]]\n",
      "iterations  945 \n",
      " Loss_train:       [[ 0.66446883]] \n",
      " Loss_Validation:  [[ 0.61887028]]\n",
      "iterations  946 \n",
      " Loss_train:       [[ 0.66434162]] \n",
      " Loss_Validation:  [[ 0.6187361]]\n",
      "iterations  947 \n",
      " Loss_train:       [[ 0.6642144]] \n",
      " Loss_Validation:  [[ 0.61860192]]\n",
      "iterations  948 \n",
      " Loss_train:       [[ 0.66408719]] \n",
      " Loss_Validation:  [[ 0.61846774]]\n",
      "iterations  949 \n",
      " Loss_train:       [[ 0.66395997]] \n",
      " Loss_Validation:  [[ 0.61833356]]\n",
      "iterations  950 \n",
      " Loss_train:       [[ 0.66383276]] \n",
      " Loss_Validation:  [[ 0.61819938]]\n",
      "iterations  951 \n",
      " Loss_train:       [[ 0.66370554]] \n",
      " Loss_Validation:  [[ 0.6180652]]\n",
      "iterations  952 \n",
      " Loss_train:       [[ 0.66357833]] \n",
      " Loss_Validation:  [[ 0.61793102]]\n",
      "iterations  953 \n",
      " Loss_train:       [[ 0.66345112]] \n",
      " Loss_Validation:  [[ 0.61779684]]\n",
      "iterations  954 \n",
      " Loss_train:       [[ 0.6633239]] \n",
      " Loss_Validation:  [[ 0.61766266]]\n",
      "iterations  955 \n",
      " Loss_train:       [[ 0.66319669]] \n",
      " Loss_Validation:  [[ 0.61752848]]\n",
      "iterations  956 \n",
      " Loss_train:       [[ 0.66306948]] \n",
      " Loss_Validation:  [[ 0.61739431]]\n",
      "iterations  957 \n",
      " Loss_train:       [[ 0.66294227]] \n",
      " Loss_Validation:  [[ 0.61726013]]\n",
      "iterations  958 \n",
      " Loss_train:       [[ 0.66281505]] \n",
      " Loss_Validation:  [[ 0.61712595]]\n",
      "iterations  959 \n",
      " Loss_train:       [[ 0.66268784]] \n",
      " Loss_Validation:  [[ 0.61699177]]\n",
      "iterations  960 \n",
      " Loss_train:       [[ 0.66256063]] \n",
      " Loss_Validation:  [[ 0.6168576]]\n",
      "iterations  961 \n",
      " Loss_train:       [[ 0.66243342]] \n",
      " Loss_Validation:  [[ 0.61672342]]\n",
      "iterations  962 \n",
      " Loss_train:       [[ 0.66230621]] \n",
      " Loss_Validation:  [[ 0.61658924]]\n",
      "iterations  963 \n",
      " Loss_train:       [[ 0.662179]] \n",
      " Loss_Validation:  [[ 0.61645507]]\n",
      "iterations  964 \n",
      " Loss_train:       [[ 0.66205179]] \n",
      " Loss_Validation:  [[ 0.61632089]]\n",
      "iterations  965 \n",
      " Loss_train:       [[ 0.66192457]] \n",
      " Loss_Validation:  [[ 0.61618672]]\n",
      "iterations  966 \n",
      " Loss_train:       [[ 0.66179736]] \n",
      " Loss_Validation:  [[ 0.61605254]]\n",
      "iterations  967 \n",
      " Loss_train:       [[ 0.66167015]] \n",
      " Loss_Validation:  [[ 0.61591837]]\n",
      "iterations  968 \n",
      " Loss_train:       [[ 0.66154295]] \n",
      " Loss_Validation:  [[ 0.6157842]]\n",
      "iterations  969 \n",
      " Loss_train:       [[ 0.66141574]] \n",
      " Loss_Validation:  [[ 0.61565002]]\n",
      "iterations  970 \n",
      " Loss_train:       [[ 0.66128853]] \n",
      " Loss_Validation:  [[ 0.61551585]]\n",
      "iterations  971 \n",
      " Loss_train:       [[ 0.66116132]] \n",
      " Loss_Validation:  [[ 0.61538167]]\n",
      "iterations  972 \n",
      " Loss_train:       [[ 0.66103411]] \n",
      " Loss_Validation:  [[ 0.6152475]]\n",
      "iterations  973 \n",
      " Loss_train:       [[ 0.6609069]] \n",
      " Loss_Validation:  [[ 0.61511333]]\n",
      "iterations  974 \n",
      " Loss_train:       [[ 0.66077969]] \n",
      " Loss_Validation:  [[ 0.61497916]]\n",
      "iterations  975 \n",
      " Loss_train:       [[ 0.66065249]] \n",
      " Loss_Validation:  [[ 0.61484498]]\n",
      "iterations  976 \n",
      " Loss_train:       [[ 0.66052528]] \n",
      " Loss_Validation:  [[ 0.61471081]]\n",
      "iterations  977 \n",
      " Loss_train:       [[ 0.66039817]] \n",
      " Loss_Validation:  [[ 0.61457664]]\n",
      "iterations  978 \n",
      " Loss_train:       [[ 0.66027353]] \n",
      " Loss_Validation:  [[ 0.6144438]]\n",
      "iterations  979 \n",
      " Loss_train:       [[ 0.66014889]] \n",
      " Loss_Validation:  [[ 0.61431095]]\n",
      "iterations  980 \n",
      " Loss_train:       [[ 0.66002425]] \n",
      " Loss_Validation:  [[ 0.61417811]]\n",
      "iterations  981 \n",
      " Loss_train:       [[ 0.6598996]] \n",
      " Loss_Validation:  [[ 0.61404527]]\n",
      "iterations  982 \n",
      " Loss_train:       [[ 0.65977496]] \n",
      " Loss_Validation:  [[ 0.61391242]]\n",
      "iterations  983 \n",
      " Loss_train:       [[ 0.65965032]] \n",
      " Loss_Validation:  [[ 0.61377958]]\n",
      "iterations  984 \n",
      " Loss_train:       [[ 0.65952568]] \n",
      " Loss_Validation:  [[ 0.61364674]]\n",
      "iterations  985 \n",
      " Loss_train:       [[ 0.65940104]] \n",
      " Loss_Validation:  [[ 0.61351389]]\n",
      "iterations  986 \n",
      " Loss_train:       [[ 0.6592764]] \n",
      " Loss_Validation:  [[ 0.61338105]]\n",
      "iterations  987 \n",
      " Loss_train:       [[ 0.65915176]] \n",
      " Loss_Validation:  [[ 0.61324821]]\n",
      "iterations  988 \n",
      " Loss_train:       [[ 0.65902712]] \n",
      " Loss_Validation:  [[ 0.61311537]]\n",
      "iterations  989 \n",
      " Loss_train:       [[ 0.65890248]] \n",
      " Loss_Validation:  [[ 0.61298253]]\n",
      "iterations  990 \n",
      " Loss_train:       [[ 0.65877784]] \n",
      " Loss_Validation:  [[ 0.61284969]]\n",
      "iterations  991 \n",
      " Loss_train:       [[ 0.6586532]] \n",
      " Loss_Validation:  [[ 0.61271685]]\n",
      "iterations  992 \n",
      " Loss_train:       [[ 0.65852856]] \n",
      " Loss_Validation:  [[ 0.61258401]]\n",
      "iterations  993 \n",
      " Loss_train:       [[ 0.65840393]] \n",
      " Loss_Validation:  [[ 0.61245117]]\n",
      "iterations  994 \n",
      " Loss_train:       [[ 0.65827929]] \n",
      " Loss_Validation:  [[ 0.61231833]]\n",
      "iterations  995 \n",
      " Loss_train:       [[ 0.65815465]] \n",
      " Loss_Validation:  [[ 0.61218549]]\n",
      "iterations  996 \n",
      " Loss_train:       [[ 0.65803001]] \n",
      " Loss_Validation:  [[ 0.61205265]]\n",
      "iterations  997 \n",
      " Loss_train:       [[ 0.65790537]] \n",
      " Loss_Validation:  [[ 0.61191981]]\n",
      "iterations  998 \n",
      " Loss_train:       [[ 0.65778074]] \n",
      " Loss_Validation:  [[ 0.61178697]]\n",
      "iterations  999 \n",
      " Loss_train:       [[ 0.6576561]] \n",
      " Loss_Validation:  [[ 0.61165413]]\n",
      "iterations  1000 \n",
      " Loss_train:       [[ 0.65753146]] \n",
      " Loss_Validation:  [[ 0.6115213]]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYwAAAEWCAYAAAB1xKBvAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nyhVNGEoppXJFE4ZSSqlc+X/e9Yg6S3j6OgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x20ee913e9e8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "from sklearn.datasets import load_svmlight_file\n",
    "from sklearn.cross_validation import train_test_split\n",
    "\n",
    "# Draw\n",
    "def Draw(loops, train_loss, validation_loss):\n",
    "    #the loss\n",
    "    plt.plot(np.arange(0,loops-1,1), train_loss[0:loops-1], label='Loss Train ')\n",
    "    plt.plot(np.arange(0,loops-1,1), validation_loss[0:loops-1], label='Loss Validation ')\n",
    "    plt.xlabel('iteration')\n",
    "    plt.ylabel('loss')\n",
    "    plt.title('Loss')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "# read data\n",
    "def get_data():\n",
    "    data = load_svmlight_file(\"australian.txt\")\n",
    "    return data\n",
    "\n",
    "    \n",
    "    # 梯度\n",
    "def grad(X, Y, theta, b):\n",
    "    grad = 1*theta\n",
    "\n",
    "    for i in range(X.shape[0]):\n",
    "        if Y[i] * (theta.transpose().dot(X[i]) + b) < 1:\n",
    "            grad = grad - (1 * X[i] * Y[i]).reshape(X.shape[1], 1)\n",
    "        else:\n",
    "            grad = grad - (0 * X[i] * Y[i]).reshape(X.shape[1], 1)\n",
    "\n",
    "    return grad\n",
    "    \n",
    "# Loss函数\n",
    "def Loss(X, Y, theta,  b):\n",
    "    lossFunction = (1/2)  * theta.transpose().dot(theta)\n",
    "    for i in range(X.shape[0]):\n",
    "        Tensor = Y[i] * (theta.transpose().dot(X[i]) + b)\n",
    "        if Tensor < 1:\n",
    "            lossFunction = lossFunction + 1 - Tensor\n",
    "            \n",
    "    return lossFunction\n",
    "    \n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    X = get_data()[0]\n",
    "    y = get_data()[1]\n",
    "    X = X.toarray()\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4, random_state=1)\n",
    "    column = X_train.shape[1]\n",
    "    train_row = X_train.shape[0]\n",
    "    test_row = X_test.shape[0]\n",
    "    y_train = y_train.reshape(train_row, 1)\n",
    "    y_test = y_test.reshape(test_row, 1) \n",
    "    \n",
    "    \n",
    "    theta = np.random.random(size = (column, 1))\n",
    "    alpha =0.000001#学习率\n",
    "    b = 2 \n",
    "    iterations = 1000 # 循环总次数\n",
    "    epsilon = 0.0001 # 收敛精度\n",
    "    count = 0 # iteration的次数\n",
    "    error = np.zeros((column, 1)) # 上次theta的值，初始为0向量\n",
    "    finish = 0 # 完成标志位\n",
    "    \n",
    "    # 初始化\n",
    "    lossTrain = []\n",
    "    lossValidation = []\n",
    "\n",
    "\n",
    "    \n",
    "    while count < iterations:\n",
    "        count += 1\n",
    "        theta = theta - alpha * grad(X_train, y_train, theta,  b)\n",
    "\n",
    "        if(np.linalg.norm(theta - error) < epsilon):\n",
    "            finish = 1\n",
    "            break\n",
    "        else:\n",
    "            error = theta\n",
    "            Loss_Train = Loss(X_train, y_train, theta, b)\n",
    "            Loss_Validation = Loss(X_test, y_test, theta, b)\n",
    "            lossTrain.append(Loss_Train[0] / train_row)\n",
    "            lossValidation.append(Loss_Validation[0] / test_row)\n",
    "\n",
    "            \n",
    "            print('iterations '.format(count), count, '\\n','Loss_train:      ',Loss_Train / train_row, '\\n ''Loss_Validation: ',Loss_Validation / test_row)\n",
    "            \n",
    "    Draw(count, lossTrain, lossValidation)   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
