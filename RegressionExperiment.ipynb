{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iterations  1 \n",
      "   LossTrain:       [[ 78933.44612143]] \n",
      "   LossValidation:  [[ 58133.39789814]]\n",
      "iterations  2 \n",
      "   LossTrain:       [[ 64632.35157451]] \n",
      "   LossValidation:  [[ 47737.42674563]]\n",
      "iterations  3 \n",
      "   LossTrain:       [[ 53394.26750278]] \n",
      "   LossValidation:  [[ 39538.11650465]]\n",
      "iterations  4 \n",
      "   LossTrain:       [[ 44556.4030842]] \n",
      "   LossValidation:  [[ 33064.43148256]]\n",
      "iterations  5 \n",
      "   LossTrain:       [[ 37599.71604246]] \n",
      "   LossValidation:  [[ 27946.93501862]]\n",
      "iterations  6 \n",
      "   LossTrain:       [[ 32117.70774011]] \n",
      "   LossValidation:  [[ 23895.80612216]]\n",
      "iterations  7 \n",
      "   LossTrain:       [[ 27791.99491542]] \n",
      "   LossValidation:  [[ 20683.6209945]]\n",
      "iterations  8 \n",
      "   LossTrain:       [[ 24373.18614677]] \n",
      "   LossValidation:  [[ 18131.8656574]]\n",
      "iterations  9 \n",
      "   LossTrain:       [[ 21665.91085982]] \n",
      "   LossValidation:  [[ 16100.37031656]]\n",
      "iterations  10 \n",
      "   LossTrain:       [[ 19517.09896984]] \n",
      "   LossValidation:  [[ 14479.03176691]]\n",
      "iterations  11 \n",
      "   LossTrain:       [[ 17806.80516054]] \n",
      "   LossValidation:  [[ 13181.32768007]]\n",
      "iterations  12 \n",
      "   LossTrain:       [[ 16441.02515417]] \n",
      "   LossValidation:  [[ 12139.2342869]]\n",
      "iterations  13 \n",
      "   LossTrain:       [[ 15346.07136765]] \n",
      "   LossValidation:  [[ 11299.24326498]]\n",
      "iterations  14 \n",
      "   LossTrain:       [[ 14464.16931485]] \n",
      "   LossValidation:  [[ 10619.23963713]]\n",
      "iterations  15 \n",
      "   LossTrain:       [[ 13750.00966832]] \n",
      "   LossValidation:  [[ 10066.05415795]]\n",
      "iterations  16 \n",
      "   LossTrain:       [[ 13168.04847021]] \n",
      "   LossValidation:  [[ 9613.54412039]]\n",
      "iterations  17 \n",
      "   LossTrain:       [[ 12690.39305175]] \n",
      "   LossValidation:  [[ 9241.08818944]]\n",
      "iterations  18 \n",
      "   LossTrain:       [[ 12295.14650012]] \n",
      "   LossValidation:  [[ 8932.40567111]]\n",
      "iterations  19 \n",
      "   LossTrain:       [[ 11965.11112849]] \n",
      "   LossValidation:  [[ 8674.63004456]]\n",
      "iterations  20 \n",
      "   LossTrain:       [[ 11686.77302265]] \n",
      "   LossValidation:  [[ 8457.58179142]]\n",
      "iterations  21 \n",
      "   LossTrain:       [[ 11449.50666015]] \n",
      "   LossValidation:  [[ 8273.19746418]]\n",
      "iterations  22 \n",
      "   LossTrain:       [[ 11244.95184482]] \n",
      "   LossValidation:  [[ 8115.08126046]]\n",
      "iterations  23 \n",
      "   LossTrain:       [[ 11066.52556915]] \n",
      "   LossValidation:  [[ 7978.15267266]]\n",
      "iterations  24 \n",
      "   LossTrain:       [[ 10909.03953435]] \n",
      "   LossValidation:  [[ 7858.36950223]]\n",
      "iterations  25 \n",
      "   LossTrain:       [[ 10768.40041275]] \n",
      "   LossValidation:  [[ 7752.51000745]]\n",
      "iterations  26 \n",
      "   LossTrain:       [[ 10641.37491146]] \n",
      "   LossValidation:  [[ 7658.00146249]]\n",
      "iterations  27 \n",
      "   LossTrain:       [[ 10525.40559027]] \n",
      "   LossValidation:  [[ 7572.78515454]]\n",
      "iterations  28 \n",
      "   LossTrain:       [[ 10418.46643576]] \n",
      "   LossValidation:  [[ 7495.20999898]]\n",
      "iterations  29 \n",
      "   LossTrain:       [[ 10318.94957955]] \n",
      "   LossValidation:  [[ 7423.94864011]]\n",
      "iterations  30 \n",
      "   LossTrain:       [[ 10225.57641768]] \n",
      "   LossValidation:  [[ 7357.93122693]]\n",
      "iterations  31 \n",
      "   LossTrain:       [[ 10137.32785022]] \n",
      "   LossValidation:  [[ 7296.29308953]]\n",
      "iterations  32 \n",
      "   LossTrain:       [[ 10053.38950585]] \n",
      "   LossValidation:  [[ 7238.33335397]]\n",
      "iterations  33 \n",
      "   LossTrain:       [[ 9973.10871251]] \n",
      "   LossValidation:  [[ 7183.48217005]]\n",
      "iterations  34 \n",
      "   LossTrain:       [[ 9895.96067734]] \n",
      "   LossValidation:  [[ 7131.27472556]]\n",
      "iterations  35 \n",
      "   LossTrain:       [[ 9821.52188861]] \n",
      "   LossValidation:  [[ 7081.33061201]]\n",
      "iterations  36 \n",
      "   LossTrain:       [[ 9749.44918304]] \n",
      "   LossValidation:  [[ 7033.33741406]]\n",
      "iterations  37 \n",
      "   LossTrain:       [[ 9679.46325855]] \n",
      "   LossValidation:  [[ 6987.0376356]]\n",
      "iterations  38 \n",
      "   LossTrain:       [[ 9611.33567672]] \n",
      "   LossValidation:  [[ 6942.21826453]]\n",
      "iterations  39 \n",
      "   LossTrain:       [[ 9544.87860565]] \n",
      "   LossValidation:  [[ 6898.7024269]]\n",
      "iterations  40 \n",
      "   LossTrain:       [[ 9479.93671612]] \n",
      "   LossValidation:  [[ 6856.34269729]]\n",
      "iterations  41 \n",
      "   LossTrain:       [[ 9416.38077041]] \n",
      "   LossValidation:  [[ 6815.01572428]]\n",
      "iterations  42 \n",
      "   LossTrain:       [[ 9354.10254266]] \n",
      "   LossValidation:  [[ 6774.61790155]]\n",
      "iterations  43 \n",
      "   LossTrain:       [[ 9293.01078751]] \n",
      "   LossValidation:  [[ 6735.06187178]]\n",
      "iterations  44 \n",
      "   LossTrain:       [[ 9233.0280345]] \n",
      "   LossValidation:  [[ 6696.27369524]]\n",
      "iterations  45 \n",
      "   LossTrain:       [[ 9174.08803388]] \n",
      "   LossValidation:  [[ 6658.19054979]]\n",
      "iterations  46 \n",
      "   LossTrain:       [[ 9116.13371644]] \n",
      "   LossValidation:  [[ 6620.75885677]]\n",
      "iterations  47 \n",
      "   LossTrain:       [[ 9059.11555986]] \n",
      "   LossValidation:  [[ 6583.93274896]]\n",
      "iterations  48 \n",
      "   LossTrain:       [[ 9002.99027665]] \n",
      "   LossValidation:  [[ 6547.67281396]]\n",
      "iterations  49 \n",
      "   LossTrain:       [[ 8947.71975713]] \n",
      "   LossValidation:  [[ 6511.94506011]]\n",
      "iterations  50 \n",
      "   LossTrain:       [[ 8893.27021493]] \n",
      "   LossValidation:  [[ 6476.72006235]]\n",
      "iterations  51 \n",
      "   LossTrain:       [[ 8839.6114936]] \n",
      "   LossValidation:  [[ 6441.97225446]]\n",
      "iterations  52 \n",
      "   LossTrain:       [[ 8786.71650166]] \n",
      "   LossValidation:  [[ 6407.6793404]]\n",
      "iterations  53 \n",
      "   LossTrain:       [[ 8734.56075048]] \n",
      "   LossValidation:  [[ 6373.82180314]]\n",
      "iterations  54 \n",
      "   LossTrain:       [[ 8683.12197436]] \n",
      "   LossValidation:  [[ 6340.38249323]]\n",
      "iterations  55 \n",
      "   LossTrain:       [[ 8632.37981687]] \n",
      "   LossValidation:  [[ 6307.34628326]]\n",
      "iterations  56 \n",
      "   LossTrain:       [[ 8582.31557052]] \n",
      "   LossValidation:  [[ 6274.69977655]]\n",
      "iterations  57 \n",
      "   LossTrain:       [[ 8532.9119596]] \n",
      "   LossValidation:  [[ 6242.43106082]]\n",
      "iterations  58 \n",
      "   LossTrain:       [[ 8484.15295821]] \n",
      "   LossValidation:  [[ 6210.52949934]]\n",
      "iterations  59 \n",
      "   LossTrain:       [[ 8436.0236368]] \n",
      "   LossValidation:  [[ 6178.98555332]]\n",
      "iterations  60 \n",
      "   LossTrain:       [[ 8388.51003219]] \n",
      "   LossValidation:  [[ 6147.79063048]]\n",
      "iterations  61 \n",
      "   LossTrain:       [[ 8341.59903695]] \n",
      "   LossValidation:  [[ 6116.93695565]]\n",
      "iterations  62 \n",
      "   LossTrain:       [[ 8295.27830464]] \n",
      "   LossValidation:  [[ 6086.41745985]]\n",
      "iterations  63 \n",
      "   LossTrain:       [[ 8249.53616833]] \n",
      "   LossValidation:  [[ 6056.22568523]]\n",
      "iterations  64 \n",
      "   LossTrain:       [[ 8204.36157028]] \n",
      "   LossValidation:  [[ 6026.3557032]]\n",
      "iterations  65 \n",
      "   LossTrain:       [[ 8159.74400075]] \n",
      "   LossValidation:  [[ 5996.80204398]]\n",
      "iterations  66 \n",
      "   LossTrain:       [[ 8115.6734448]] \n",
      "   LossValidation:  [[ 5967.55963587]]\n",
      "iterations  67 \n",
      "   LossTrain:       [[ 8072.1403356]] \n",
      "   LossValidation:  [[ 5938.62375277]]\n",
      "iterations  68 \n",
      "   LossTrain:       [[ 8029.13551354]] \n",
      "   LossValidation:  [[ 5909.98996881]]\n",
      "iterations  69 \n",
      "   LossTrain:       [[ 7986.65019008]] \n",
      "   LossValidation:  [[ 5881.65411916]]\n",
      "iterations  70 \n",
      "   LossTrain:       [[ 7944.67591578]] \n",
      "   LossValidation:  [[ 5853.61226607]]\n",
      "iterations  71 \n",
      "   LossTrain:       [[ 7903.20455204]] \n",
      "   LossValidation:  [[ 5825.86066949]]\n",
      "iterations  72 \n",
      "   LossTrain:       [[ 7862.2282458]] \n",
      "   LossValidation:  [[ 5798.39576156]]\n",
      "iterations  73 \n",
      "   LossTrain:       [[ 7821.73940711]] \n",
      "   LossValidation:  [[ 5771.21412464]]\n",
      "iterations  74 \n",
      "   LossTrain:       [[ 7781.73068903]] \n",
      "   LossValidation:  [[ 5744.31247218]]\n",
      "iterations  75 \n",
      "   LossTrain:       [[ 7742.19496958]] \n",
      "   LossValidation:  [[ 5717.68763217]]\n",
      "iterations  76 \n",
      "   LossTrain:       [[ 7703.1253356]] \n",
      "   LossValidation:  [[ 5691.33653288]]\n",
      "iterations  77 \n",
      "   LossTrain:       [[ 7664.51506819]] \n",
      "   LossValidation:  [[ 5665.25619048]]\n",
      "iterations  78 \n",
      "   LossTrain:       [[ 7626.35762962]] \n",
      "   LossValidation:  [[ 5639.44369839]]\n",
      "iterations  79 \n",
      "   LossTrain:       [[ 7588.64665149]] \n",
      "   LossValidation:  [[ 5613.89621801]]\n",
      "iterations  80 \n",
      "   LossTrain:       [[ 7551.37592404]] \n",
      "   LossValidation:  [[ 5588.61097085]]\n",
      "iterations  81 \n",
      "   LossTrain:       [[ 7514.53938648]] \n",
      "   LossValidation:  [[ 5563.58523164]]\n",
      "iterations  82 \n",
      "   LossTrain:       [[ 7478.1311182]] \n",
      "   LossValidation:  [[ 5538.81632252]]\n",
      "iterations  83 \n",
      "   LossTrain:       [[ 7442.14533081]] \n",
      "   LossValidation:  [[ 5514.30160798]]\n",
      "iterations  84 \n",
      "   LossTrain:       [[ 7406.57636088]] \n",
      "   LossValidation:  [[ 5490.03849059]]\n",
      "iterations  85 \n",
      "   LossTrain:       [[ 7371.41866338]] \n",
      "   LossValidation:  [[ 5466.02440732]]\n",
      "iterations  86 \n",
      "   LossTrain:       [[ 7336.66680563]] \n",
      "   LossValidation:  [[ 5442.25682644]]\n",
      "iterations  87 \n",
      "   LossTrain:       [[ 7302.31546184]] \n",
      "   LossValidation:  [[ 5418.7332449]]\n",
      "iterations  88 \n",
      "   LossTrain:       [[ 7268.35940806]] \n",
      "   LossValidation:  [[ 5395.45118607]]\n",
      "iterations  89 \n",
      "   LossTrain:       [[ 7234.79351763]] \n",
      "   LossValidation:  [[ 5372.40819793]]\n",
      "iterations  90 \n",
      "   LossTrain:       [[ 7201.6127569]] \n",
      "   LossValidation:  [[ 5349.60185149]]\n",
      "iterations  91 \n",
      "   LossTrain:       [[ 7168.81218145]] \n",
      "   LossValidation:  [[ 5327.02973948]]\n",
      "iterations  92 \n",
      "   LossTrain:       [[ 7136.38693244]] \n",
      "   LossValidation:  [[ 5304.68947534]]\n",
      "iterations  93 \n",
      "   LossTrain:       [[ 7104.33223341]] \n",
      "   LossValidation:  [[ 5282.57869231]]\n",
      "iterations  94 \n",
      "   LossTrain:       [[ 7072.64338721]] \n",
      "   LossValidation:  [[ 5260.69504278]]\n",
      "iterations  95 \n",
      "   LossTrain:       [[ 7041.31577323]] \n",
      "   LossValidation:  [[ 5239.03619769]]\n",
      "iterations  96 \n",
      "   LossTrain:       [[ 7010.3448448]] \n",
      "   LossValidation:  [[ 5217.59984612]]\n",
      "iterations  97 \n",
      "   LossTrain:       [[ 6979.72612681]] \n",
      "   LossValidation:  [[ 5196.38369498]]\n",
      "iterations  98 \n",
      "   LossTrain:       [[ 6949.45521346]] \n",
      "   LossValidation:  [[ 5175.38546873]]\n",
      "iterations  99 \n",
      "   LossTrain:       [[ 6919.52776622]] \n",
      "   LossValidation:  [[ 5154.60290922]]\n",
      "iterations  100 \n",
      "   LossTrain:       [[ 6889.93951184]] \n",
      "   LossValidation:  [[ 5134.0337756]]\n",
      "iterations  101 \n",
      "   LossTrain:       [[ 6860.6862406]] \n",
      "   LossValidation:  [[ 5113.67584422]]\n",
      "iterations  102 \n",
      "   LossTrain:       [[ 6831.76380462]] \n",
      "   LossValidation:  [[ 5093.52690863]]\n",
      "iterations  103 \n",
      "   LossTrain:       [[ 6803.16811621]] \n",
      "   LossValidation:  [[ 5073.58477958]]\n",
      "iterations  104 \n",
      "   LossTrain:       [[ 6774.89514647]] \n",
      "   LossValidation:  [[ 5053.84728505]]\n",
      "iterations  105 \n",
      "   LossTrain:       [[ 6746.9409238]] \n",
      "   LossValidation:  [[ 5034.31227031]]\n",
      "iterations  106 \n",
      "   LossTrain:       [[ 6719.30153264]] \n",
      "   LossValidation:  [[ 5014.97759795]]\n",
      "iterations  107 \n",
      "   LossTrain:       [[ 6691.9731122]] \n",
      "   LossValidation:  [[ 4995.84114801]]\n",
      "iterations  108 \n",
      "   LossTrain:       [[ 6664.95185526]] \n",
      "   LossValidation:  [[ 4976.90081801]]\n",
      "iterations  109 \n",
      "   LossTrain:       [[ 6638.23400706]] \n",
      "   LossValidation:  [[ 4958.15452305]]\n",
      "iterations  110 \n",
      "   LossTrain:       [[ 6611.81586422]] \n",
      "   LossValidation:  [[ 4939.60019591]]\n",
      "iterations  111 \n",
      "   LossTrain:       [[ 6585.69377373]] \n",
      "   LossValidation:  [[ 4921.23578707]]\n",
      "iterations  112 \n",
      "   LossTrain:       [[ 6559.86413198]] \n",
      "   LossValidation:  [[ 4903.05926489]]\n",
      "iterations  113 \n",
      "   LossTrain:       [[ 6534.32338382]] \n",
      "   LossValidation:  [[ 4885.06861557]]\n",
      "iterations  114 \n",
      "   LossTrain:       [[ 6509.06802166]] \n",
      "   LossValidation:  [[ 4867.26184329]]\n",
      "iterations  115 \n",
      "   LossTrain:       [[ 6484.09458463]] \n",
      "   LossValidation:  [[ 4849.63697024]]\n",
      "iterations  116 \n",
      "   LossTrain:       [[ 6459.39965777]] \n",
      "   LossValidation:  [[ 4832.19203669]]\n",
      "iterations  117 \n",
      "   LossTrain:       [[ 6434.97987121]] \n",
      "   LossValidation:  [[ 4814.925101]]\n",
      "iterations  118 \n",
      "   LossTrain:       [[ 6410.83189941]] \n",
      "   LossValidation:  [[ 4797.83423968]]\n",
      "iterations  119 \n",
      "   LossTrain:       [[ 6386.95246046]] \n",
      "   LossValidation:  [[ 4780.9175474]]\n",
      "iterations  120 \n",
      "   LossTrain:       [[ 6363.3383153]] \n",
      "   LossValidation:  [[ 4764.17313702]]\n",
      "iterations  121 \n",
      "   LossTrain:       [[ 6339.98626711]] \n",
      "   LossValidation:  [[ 4747.59913959]]\n",
      "iterations  122 \n",
      "   LossTrain:       [[ 6316.89316057]] \n",
      "   LossValidation:  [[ 4731.19370436]]\n",
      "iterations  123 \n",
      "   LossTrain:       [[ 6294.05588126]] \n",
      "   LossValidation:  [[ 4714.95499872]]\n",
      "iterations  124 \n",
      "   LossTrain:       [[ 6271.471355]] \n",
      "   LossValidation:  [[ 4698.88120825]]\n",
      "iterations  125 \n",
      "   LossTrain:       [[ 6249.13654726]] \n",
      "   LossValidation:  [[ 4682.97053664]]\n",
      "iterations  126 \n",
      "   LossTrain:       [[ 6227.04846253]] \n",
      "   LossValidation:  [[ 4667.22120569]]\n",
      "iterations  127 \n",
      "   LossTrain:       [[ 6205.20414378]] \n",
      "   LossValidation:  [[ 4651.63145523]]\n",
      "iterations  128 \n",
      "   LossTrain:       [[ 6183.60067186]] \n",
      "   LossValidation:  [[ 4636.19954309]]\n",
      "iterations  129 \n",
      "   LossTrain:       [[ 6162.23516494]] \n",
      "   LossValidation:  [[ 4620.92374503]]\n",
      "iterations  130 \n",
      "   LossTrain:       [[ 6141.10477799]] \n",
      "   LossValidation:  [[ 4605.8023547]]\n",
      "iterations  131 \n",
      "   LossTrain:       [[ 6120.20670227]] \n",
      "   LossValidation:  [[ 4590.83368353]]\n",
      "iterations  132 \n",
      "   LossTrain:       [[ 6099.53816474]] \n",
      "   LossValidation:  [[ 4576.01606066]]\n",
      "iterations  133 \n",
      "   LossTrain:       [[ 6079.09642762]] \n",
      "   LossValidation:  [[ 4561.34783288]]\n",
      "iterations  134 \n",
      "   LossTrain:       [[ 6058.87878788]] \n",
      "   LossValidation:  [[ 4546.8273645]]\n",
      "iterations  135 \n",
      "   LossTrain:       [[ 6038.88257672]] \n",
      "   LossValidation:  [[ 4532.4530373]]\n",
      "iterations  136 \n",
      "   LossTrain:       [[ 6019.10515912]] \n",
      "   LossValidation:  [[ 4518.22325039]]\n",
      "iterations  137 \n",
      "   LossTrain:       [[ 5999.54393339]] \n",
      "   LossValidation:  [[ 4504.13642009]]\n",
      "iterations  138 \n",
      "   LossTrain:       [[ 5980.19633064]] \n",
      "   LossValidation:  [[ 4490.1909799]]\n",
      "iterations  139 \n",
      "   LossTrain:       [[ 5961.05981442]] \n",
      "   LossValidation:  [[ 4476.38538029]]\n",
      "iterations  140 \n",
      "   LossTrain:       [[ 5942.13188021]] \n",
      "   LossValidation:  [[ 4462.71808863]]\n",
      "iterations  141 \n",
      "   LossTrain:       [[ 5923.41005499]] \n",
      "   LossValidation:  [[ 4449.18758909]]\n",
      "iterations  142 \n",
      "   LossTrain:       [[ 5904.89189686]] \n",
      "   LossValidation:  [[ 4435.79238244]]\n",
      "iterations  143 \n",
      "   LossTrain:       [[ 5886.57499456]] \n",
      "   LossValidation:  [[ 4422.53098602]]\n",
      "iterations  144 \n",
      "   LossTrain:       [[ 5868.45696708]] \n",
      "   LossValidation:  [[ 4409.40193352]]\n",
      "iterations  145 \n",
      "   LossTrain:       [[ 5850.53546328]] \n",
      "   LossValidation:  [[ 4396.40377491]]\n",
      "iterations  146 \n",
      "   LossTrain:       [[ 5832.80816143]] \n",
      "   LossValidation:  [[ 4383.53507627]]\n",
      "iterations  147 \n",
      "   LossTrain:       [[ 5815.27276887]] \n",
      "   LossValidation:  [[ 4370.79441966]]\n",
      "iterations  148 \n",
      "   LossTrain:       [[ 5797.92702159]] \n",
      "   LossValidation:  [[ 4358.18040301]]\n",
      "iterations  149 \n",
      "   LossTrain:       [[ 5780.76868387]] \n",
      "   LossValidation:  [[ 4345.69163993]]\n",
      "iterations  150 \n",
      "   LossTrain:       [[ 5763.79554787]] \n",
      "   LossValidation:  [[ 4333.3267596]]\n",
      "iterations  151 \n",
      "   LossTrain:       [[ 5747.00543332]] \n",
      "   LossValidation:  [[ 4321.08440662]]\n",
      "iterations  152 \n",
      "   LossTrain:       [[ 5730.39618708]] \n",
      "   LossValidation:  [[ 4308.96324089]]\n",
      "iterations  153 \n",
      "   LossTrain:       [[ 5713.96568283]] \n",
      "   LossValidation:  [[ 4296.96193741]]\n",
      "iterations  154 \n",
      "   LossTrain:       [[ 5697.7118207]] \n",
      "   LossValidation:  [[ 4285.07918617]]\n",
      "iterations  155 \n",
      "   LossTrain:       [[ 5681.63252694]] \n",
      "   LossValidation:  [[ 4273.31369204]]\n",
      "iterations  156 \n",
      "   LossTrain:       [[ 5665.72575355]] \n",
      "   LossValidation:  [[ 4261.66417453]]\n",
      "iterations  157 \n",
      "   LossTrain:       [[ 5649.98947795]] \n",
      "   LossValidation:  [[ 4250.12936774]]\n",
      "iterations  158 \n",
      "   LossTrain:       [[ 5634.42170265]] \n",
      "   LossValidation:  [[ 4238.70802016]]\n",
      "iterations  159 \n",
      "   LossTrain:       [[ 5619.02045493]] \n",
      "   LossValidation:  [[ 4227.39889451]]\n",
      "iterations  160 \n",
      "   LossTrain:       [[ 5603.78378649]] \n",
      "   LossValidation:  [[ 4216.20076767]]\n",
      "iterations  161 \n",
      "   LossTrain:       [[ 5588.70977316]] \n",
      "   LossValidation:  [[ 4205.11243042]]\n",
      "iterations  162 \n",
      "   LossTrain:       [[ 5573.79651456]] \n",
      "   LossValidation:  [[ 4194.13268741]]\n",
      "iterations  163 \n",
      "   LossTrain:       [[ 5559.04213382]] \n",
      "   LossValidation:  [[ 4183.26035692]]\n",
      "iterations  164 \n",
      "   LossTrain:       [[ 5544.44477722]] \n",
      "   LossValidation:  [[ 4172.49427077]]\n",
      "iterations  165 \n",
      "   LossTrain:       [[ 5530.00261397]] \n",
      "   LossValidation:  [[ 4161.83327416]]\n",
      "iterations  166 \n",
      "   LossTrain:       [[ 5515.71383583]] \n",
      "   LossValidation:  [[ 4151.27622552]]\n",
      "iterations  167 \n",
      "   LossTrain:       [[ 5501.57665686]] \n",
      "   LossValidation:  [[ 4140.82199637]]\n",
      "iterations  168 \n",
      "   LossTrain:       [[ 5487.58931312]] \n",
      "   LossValidation:  [[ 4130.46947117]]\n",
      "iterations  169 \n",
      "   LossTrain:       [[ 5473.7500624]] \n",
      "   LossValidation:  [[ 4120.21754719]]\n",
      "iterations  170 \n",
      "   LossTrain:       [[ 5460.05718393]] \n",
      "   LossValidation:  [[ 4110.06513438]]\n",
      "iterations  171 \n",
      "   LossTrain:       [[ 5446.50897807]] \n",
      "   LossValidation:  [[ 4100.01115517]]\n",
      "iterations  172 \n",
      "   LossTrain:       [[ 5433.10376609]] \n",
      "   LossValidation:  [[ 4090.05454442]]\n",
      "iterations  173 \n",
      "   LossTrain:       [[ 5419.83988986]] \n",
      "   LossValidation:  [[ 4080.19424919]]\n",
      "iterations  174 \n",
      "   LossTrain:       [[ 5406.71571162]] \n",
      "   LossValidation:  [[ 4070.42922867]]\n",
      "iterations  175 \n",
      "   LossTrain:       [[ 5393.72961368]] \n",
      "   LossValidation:  [[ 4060.75845401]]\n",
      "iterations  176 \n",
      "   LossTrain:       [[ 5380.87999817]] \n",
      "   LossValidation:  [[ 4051.1809082]]\n",
      "iterations  177 \n",
      "   LossTrain:       [[ 5368.16528681]] \n",
      "   LossValidation:  [[ 4041.69558592]]\n",
      "iterations  178 \n",
      "   LossTrain:       [[ 5355.58392064]] \n",
      "   LossValidation:  [[ 4032.3014934]]\n",
      "iterations  179 \n",
      "   LossTrain:       [[ 5343.13435977]] \n",
      "   LossValidation:  [[ 4022.99764832]]\n",
      "iterations  180 \n",
      "   LossTrain:       [[ 5330.81508311]] \n",
      "   LossValidation:  [[ 4013.78307966]]\n",
      "iterations  181 \n",
      "   LossTrain:       [[ 5318.6245882]] \n",
      "   LossValidation:  [[ 4004.65682755]]\n",
      "iterations  182 \n",
      "   LossTrain:       [[ 5306.5613909]] \n",
      "   LossValidation:  [[ 3995.61794317]]\n",
      "iterations  183 \n",
      "   LossTrain:       [[ 5294.62402517]] \n",
      "   LossValidation:  [[ 3986.66548861]]\n",
      "iterations  184 \n",
      "   LossTrain:       [[ 5282.81104289]] \n",
      "   LossValidation:  [[ 3977.79853674]]\n",
      "iterations  185 \n",
      "   LossTrain:       [[ 5271.12101355]] \n",
      "   LossValidation:  [[ 3969.01617111]]\n",
      "iterations  186 \n",
      "   LossTrain:       [[ 5259.55252409]] \n",
      "   LossValidation:  [[ 3960.31748577]]\n",
      "iterations  187 \n",
      "   LossTrain:       [[ 5248.10417863]] \n",
      "   LossValidation:  [[ 3951.70158519]]\n",
      "iterations  188 \n",
      "   LossTrain:       [[ 5236.7745983]] \n",
      "   LossValidation:  [[ 3943.16758416]]\n",
      "iterations  189 \n",
      "   LossTrain:       [[ 5225.56242098]] \n",
      "   LossValidation:  [[ 3934.71460759]]\n",
      "iterations  190 \n",
      "   LossTrain:       [[ 5214.46630109]] \n",
      "   LossValidation:  [[ 3926.34179047]]\n",
      "iterations  191 \n",
      "   LossTrain:       [[ 5203.48490941]] \n",
      "   LossValidation:  [[ 3918.04827771]]\n",
      "iterations  192 \n",
      "   LossTrain:       [[ 5192.61693284]] \n",
      "   LossValidation:  [[ 3909.83322403]]\n",
      "iterations  193 \n",
      "   LossTrain:       [[ 5181.86107421]] \n",
      "   LossValidation:  [[ 3901.69579386]]\n",
      "iterations  194 \n",
      "   LossTrain:       [[ 5171.21605209]] \n",
      "   LossValidation:  [[ 3893.63516119]]\n",
      "iterations  195 \n",
      "   LossTrain:       [[ 5160.68060054]] \n",
      "   LossValidation:  [[ 3885.6505095]]\n",
      "iterations  196 \n",
      "   LossTrain:       [[ 5150.25346898]] \n",
      "   LossValidation:  [[ 3877.74103162]]\n",
      "iterations  197 \n",
      "   LossTrain:       [[ 5139.93342194]] \n",
      "   LossValidation:  [[ 3869.90592962]]\n",
      "iterations  198 \n",
      "   LossTrain:       [[ 5129.71923891]] \n",
      "   LossValidation:  [[ 3862.1444147]]\n",
      "iterations  199 \n",
      "   LossTrain:       [[ 5119.60971411]] \n",
      "   LossValidation:  [[ 3854.45570712]]\n",
      "iterations  200 \n",
      "   LossTrain:       [[ 5109.60365633]] \n",
      "   LossValidation:  [[ 3846.83903602]]\n",
      "iterations  201 \n",
      "   LossTrain:       [[ 5099.69988876]] \n",
      "   LossValidation:  [[ 3839.29363938]]\n",
      "iterations  202 \n",
      "   LossTrain:       [[ 5089.89724875]] \n",
      "   LossValidation:  [[ 3831.81876387]]\n",
      "iterations  203 \n",
      "   LossTrain:       [[ 5080.1945877]] \n",
      "   LossValidation:  [[ 3824.4136648]]\n",
      "iterations  204 \n",
      "   LossTrain:       [[ 5070.59077083]] \n",
      "   LossValidation:  [[ 3817.07760593]]\n",
      "iterations  205 \n",
      "   LossTrain:       [[ 5061.08467704]] \n",
      "   LossValidation:  [[ 3809.80985948]]\n",
      "iterations  206 \n",
      "   LossTrain:       [[ 5051.67519871]] \n",
      "   LossValidation:  [[ 3802.60970593]]\n",
      "iterations  207 \n",
      "   LossTrain:       [[ 5042.36124156]] \n",
      "   LossValidation:  [[ 3795.47643398]]\n",
      "iterations  208 \n",
      "   LossTrain:       [[ 5033.14172444]] \n",
      "   LossValidation:  [[ 3788.40934042]]\n",
      "iterations  209 \n",
      "   LossTrain:       [[ 5024.0155792]] \n",
      "   LossValidation:  [[ 3781.40773008]]\n",
      "iterations  210 \n",
      "   LossTrain:       [[ 5014.98175053]] \n",
      "   LossValidation:  [[ 3774.47091567]]\n",
      "iterations  211 \n",
      "   LossTrain:       [[ 5006.03919577]] \n",
      "   LossValidation:  [[ 3767.59821773]]\n",
      "iterations  212 \n",
      "   LossTrain:       [[ 4997.18688475]] \n",
      "   LossValidation:  [[ 3760.78896452]]\n",
      "iterations  213 \n",
      "   LossTrain:       [[ 4988.42379968]] \n",
      "   LossValidation:  [[ 3754.04249196]]\n",
      "iterations  214 \n",
      "   LossTrain:       [[ 4979.74893494]] \n",
      "   LossValidation:  [[ 3747.35814347]]\n",
      "iterations  215 \n",
      "   LossTrain:       [[ 4971.16129695]] \n",
      "   LossValidation:  [[ 3740.73526995]]\n",
      "iterations  216 \n",
      "   LossTrain:       [[ 4962.65990402]] \n",
      "   LossValidation:  [[ 3734.17322965]]\n",
      "iterations  217 \n",
      "   LossTrain:       [[ 4954.24378621]] \n",
      "   LossValidation:  [[ 3727.6713881]]\n",
      "iterations  218 \n",
      "   LossTrain:       [[ 4945.91198515]] \n",
      "   LossValidation:  [[ 3721.22911802]]\n",
      "iterations  219 \n",
      "   LossTrain:       [[ 4937.66355393]] \n",
      "   LossValidation:  [[ 3714.84579923]]\n",
      "iterations  220 \n",
      "   LossTrain:       [[ 4929.49755695]] \n",
      "   LossValidation:  [[ 3708.52081857]]\n",
      "iterations  221 \n",
      "   LossTrain:       [[ 4921.41306976]] \n",
      "   LossValidation:  [[ 3702.25356981]]\n",
      "iterations  222 \n",
      "   LossTrain:       [[ 4913.40917893]] \n",
      "   LossValidation:  [[ 3696.04345357]]\n",
      "iterations  223 \n",
      "   LossTrain:       [[ 4905.48498192]] \n",
      "   LossValidation:  [[ 3689.88987723]]\n",
      "iterations  224 \n",
      "   LossTrain:       [[ 4897.63958695]] \n",
      "   LossValidation:  [[ 3683.7922549]]\n",
      "iterations  225 \n",
      "   LossTrain:       [[ 4889.87211282]] \n",
      "   LossValidation:  [[ 3677.75000724]]\n",
      "iterations  226 \n",
      "   LossTrain:       [[ 4882.18168885]] \n",
      "   LossValidation:  [[ 3671.76256149]]\n",
      "iterations  227 \n",
      "   LossTrain:       [[ 4874.56745468]] \n",
      "   LossValidation:  [[ 3665.82935131]]\n",
      "iterations  228 \n",
      "   LossTrain:       [[ 4867.0285602]] \n",
      "   LossValidation:  [[ 3659.94981676]]\n",
      "iterations  229 \n",
      "   LossTrain:       [[ 4859.56416538]] \n",
      "   LossValidation:  [[ 3654.12340418]]\n",
      "iterations  230 \n",
      "   LossTrain:       [[ 4852.17344015]] \n",
      "   LossValidation:  [[ 3648.34956614]]\n",
      "iterations  231 \n",
      "   LossTrain:       [[ 4844.8555643]] \n",
      "   LossValidation:  [[ 3642.62776137]]\n",
      "iterations  232 \n",
      "   LossTrain:       [[ 4837.60972735]] \n",
      "   LossValidation:  [[ 3636.95745467]]\n",
      "iterations  233 \n",
      "   LossTrain:       [[ 4830.43512839]] \n",
      "   LossValidation:  [[ 3631.33811685]]\n",
      "iterations  234 \n",
      "   LossTrain:       [[ 4823.33097603]] \n",
      "   LossValidation:  [[ 3625.76922464]]\n",
      "iterations  235 \n",
      "   LossTrain:       [[ 4816.29648821]] \n",
      "   LossValidation:  [[ 3620.25026065]]\n",
      "iterations  236 \n",
      "   LossTrain:       [[ 4809.33089215]] \n",
      "   LossValidation:  [[ 3614.78071328]]\n",
      "iterations  237 \n",
      "   LossTrain:       [[ 4802.43342419]] \n",
      "   LossValidation:  [[ 3609.36007665]]\n",
      "iterations  238 \n",
      "   LossTrain:       [[ 4795.60332968]] \n",
      "   LossValidation:  [[ 3603.98785055]]\n",
      "iterations  239 \n",
      "   LossTrain:       [[ 4788.8398629]] \n",
      "   LossValidation:  [[ 3598.66354033]]\n",
      "iterations  240 \n",
      "   LossTrain:       [[ 4782.14228692]] \n",
      "   LossValidation:  [[ 3593.3866569]]\n",
      "iterations  241 \n",
      "   LossTrain:       [[ 4775.5098735]] \n",
      "   LossValidation:  [[ 3588.15671661]]\n",
      "iterations  242 \n",
      "   LossTrain:       [[ 4768.94190298]] \n",
      "   LossValidation:  [[ 3582.9732412]]\n",
      "iterations  243 \n",
      "   LossTrain:       [[ 4762.43766417]] \n",
      "   LossValidation:  [[ 3577.83575774]]\n",
      "iterations  244 \n",
      "   LossTrain:       [[ 4755.99645427]] \n",
      "   LossValidation:  [[ 3572.74379859]]\n",
      "iterations  245 \n",
      "   LossTrain:       [[ 4749.61757875]] \n",
      "   LossValidation:  [[ 3567.69690129]]\n",
      "iterations  246 \n",
      "   LossTrain:       [[ 4743.30035122]] \n",
      "   LossValidation:  [[ 3562.69460853]]\n",
      "iterations  247 \n",
      "   LossTrain:       [[ 4737.04409338]] \n",
      "   LossValidation:  [[ 3557.73646808]]\n",
      "iterations  248 \n",
      "   LossTrain:       [[ 4730.84813488]] \n",
      "   LossValidation:  [[ 3552.82203273]]\n",
      "iterations  249 \n",
      "   LossTrain:       [[ 4724.71181327]] \n",
      "   LossValidation:  [[ 3547.95086024]]\n",
      "iterations  250 \n",
      "   LossTrain:       [[ 4718.63447383]] \n",
      "   LossValidation:  [[ 3543.12251328]]\n",
      "iterations  251 \n",
      "   LossTrain:       [[ 4712.61546954]] \n",
      "   LossValidation:  [[ 3538.33655935]]\n",
      "iterations  252 \n",
      "   LossTrain:       [[ 4706.65416096]] \n",
      "   LossValidation:  [[ 3533.59257074]]\n",
      "iterations  253 \n",
      "   LossTrain:       [[ 4700.74991614]] \n",
      "   LossValidation:  [[ 3528.8901245]]\n",
      "iterations  254 \n",
      "   LossTrain:       [[ 4694.90211052]] \n",
      "   LossValidation:  [[ 3524.22880232]]\n",
      "iterations  255 \n",
      "   LossTrain:       [[ 4689.11012685]] \n",
      "   LossValidation:  [[ 3519.60819054]]\n",
      "iterations  256 \n",
      "   LossTrain:       [[ 4683.3733551]] \n",
      "   LossValidation:  [[ 3515.02788005]]\n",
      "iterations  257 \n",
      "   LossTrain:       [[ 4677.69119235]] \n",
      "   LossValidation:  [[ 3510.48746627]]\n",
      "iterations  258 \n",
      "   LossTrain:       [[ 4672.06304275]] \n",
      "   LossValidation:  [[ 3505.98654906]]\n",
      "iterations  259 \n",
      "   LossTrain:       [[ 4666.48831737]] \n",
      "   LossValidation:  [[ 3501.52473271]]\n",
      "iterations  260 \n",
      "   LossTrain:       [[ 4660.96643419]] \n",
      "   LossValidation:  [[ 3497.10162587]]\n",
      "iterations  261 \n",
      "   LossTrain:       [[ 4655.49681793]] \n",
      "   LossValidation:  [[ 3492.71684146]]\n",
      "iterations  262 \n",
      "   LossTrain:       [[ 4650.07890005]] \n",
      "   LossValidation:  [[ 3488.3699967]]\n",
      "iterations  263 \n",
      "   LossTrain:       [[ 4644.71211862]] \n",
      "   LossValidation:  [[ 3484.06071298]]\n",
      "iterations  264 \n",
      "   LossTrain:       [[ 4639.39591823]] \n",
      "   LossValidation:  [[ 3479.78861587]]\n",
      "iterations  265 \n",
      "   LossTrain:       [[ 4634.12974995]] \n",
      "   LossValidation:  [[ 3475.55333504]]\n",
      "iterations  266 \n",
      "   LossTrain:       [[ 4628.91307123]] \n",
      "   LossValidation:  [[ 3471.35450422]]\n",
      "iterations  267 \n",
      "   LossTrain:       [[ 4623.74534582]] \n",
      "   LossValidation:  [[ 3467.19176115]]\n",
      "iterations  268 \n",
      "   LossTrain:       [[ 4618.62604369]] \n",
      "   LossValidation:  [[ 3463.06474753]]\n",
      "iterations  269 \n",
      "   LossTrain:       [[ 4613.55464097]] \n",
      "   LossValidation:  [[ 3458.97310899]]\n",
      "iterations  270 \n",
      "   LossTrain:       [[ 4608.53061985]] \n",
      "   LossValidation:  [[ 3454.91649504]]\n",
      "iterations  271 \n",
      "   LossTrain:       [[ 4603.55346855]] \n",
      "   LossValidation:  [[ 3450.89455898]]\n",
      "iterations  272 \n",
      "   LossTrain:       [[ 4598.62268119]] \n",
      "   LossValidation:  [[ 3446.90695795]]\n",
      "iterations  273 \n",
      "   LossTrain:       [[ 4593.73775777]] \n",
      "   LossValidation:  [[ 3442.95335278]]\n",
      "iterations  274 \n",
      "   LossTrain:       [[ 4588.89820405]] \n",
      "   LossValidation:  [[ 3439.03340802]]\n",
      "iterations  275 \n",
      "   LossTrain:       [[ 4584.10353153]] \n",
      "   LossValidation:  [[ 3435.14679187]]\n",
      "iterations  276 \n",
      "   LossTrain:       [[ 4579.35325734]] \n",
      "   LossValidation:  [[ 3431.29317614]]\n",
      "iterations  277 \n",
      "   LossTrain:       [[ 4574.64690419]] \n",
      "   LossValidation:  [[ 3427.4722362]]\n",
      "iterations  278 \n",
      "   LossTrain:       [[ 4569.98400032]] \n",
      "   LossValidation:  [[ 3423.68365097]]\n",
      "iterations  279 \n",
      "   LossTrain:       [[ 4565.36407938]] \n",
      "   LossValidation:  [[ 3419.92710283]]\n",
      "iterations  280 \n",
      "   LossTrain:       [[ 4560.78668041]] \n",
      "   LossValidation:  [[ 3416.20227761]]\n",
      "iterations  281 \n",
      "   LossTrain:       [[ 4556.25134778]] \n",
      "   LossValidation:  [[ 3412.50886457]]\n",
      "iterations  282 \n",
      "   LossTrain:       [[ 4551.75763109]] \n",
      "   LossValidation:  [[ 3408.8465563]]\n",
      "iterations  283 \n",
      "   LossTrain:       [[ 4547.30508512]] \n",
      "   LossValidation:  [[ 3405.21504874]]\n",
      "iterations  284 \n",
      "   LossTrain:       [[ 4542.89326979]] \n",
      "   LossValidation:  [[ 3401.61404112]]\n",
      "iterations  285 \n",
      "   LossTrain:       [[ 4538.52175006]] \n",
      "   LossValidation:  [[ 3398.04323591]]\n",
      "iterations  286 \n",
      "   LossTrain:       [[ 4534.19009591]] \n",
      "   LossValidation:  [[ 3394.50233879]]\n",
      "iterations  287 \n",
      "   LossTrain:       [[ 4529.89788225]] \n",
      "   LossValidation:  [[ 3390.99105864]]\n",
      "iterations  288 \n",
      "   LossTrain:       [[ 4525.64468885]] \n",
      "   LossValidation:  [[ 3387.50910745]]\n",
      "iterations  289 \n",
      "   LossTrain:       [[ 4521.43010034]] \n",
      "   LossValidation:  [[ 3384.05620033]]\n",
      "iterations  290 \n",
      "   LossTrain:       [[ 4517.25370607]] \n",
      "   LossValidation:  [[ 3380.63205546]]\n",
      "iterations  291 \n",
      "   LossTrain:       [[ 4513.11510012]] \n",
      "   LossValidation:  [[ 3377.23639403]]\n",
      "iterations  292 \n",
      "   LossTrain:       [[ 4509.01388122]] \n",
      "   LossValidation:  [[ 3373.86894026]]\n",
      "iterations  293 \n",
      "   LossTrain:       [[ 4504.94965269]] \n",
      "   LossValidation:  [[ 3370.5294213]]\n",
      "iterations  294 \n",
      "   LossTrain:       [[ 4500.92202237]] \n",
      "   LossValidation:  [[ 3367.21756725]]\n",
      "iterations  295 \n",
      "   LossTrain:       [[ 4496.9306026]] \n",
      "   LossValidation:  [[ 3363.9331111]]\n",
      "iterations  296 \n",
      "   LossTrain:       [[ 4492.97501014]] \n",
      "   LossValidation:  [[ 3360.67578869]]\n",
      "iterations  297 \n",
      "   LossTrain:       [[ 4489.05486615]] \n",
      "   LossValidation:  [[ 3357.44533871]]\n",
      "iterations  298 \n",
      "   LossTrain:       [[ 4485.16979607]] \n",
      "   LossValidation:  [[ 3354.24150264]]\n",
      "iterations  299 \n",
      "   LossTrain:       [[ 4481.31942965]] \n",
      "   LossValidation:  [[ 3351.0640247]]\n",
      "iterations  300 \n",
      "   LossTrain:       [[ 4477.50340085]] \n",
      "   LossValidation:  [[ 3347.91265187]]\n",
      "iterations  301 \n",
      "   LossTrain:       [[ 4473.72134778]] \n",
      "   LossValidation:  [[ 3344.78713382]]\n",
      "iterations  302 \n",
      "   LossTrain:       [[ 4469.9729127]] \n",
      "   LossValidation:  [[ 3341.68722289]]\n",
      "iterations  303 \n",
      "   LossTrain:       [[ 4466.25774191]] \n",
      "   LossValidation:  [[ 3338.61267408]]\n",
      "iterations  304 \n",
      "   LossTrain:       [[ 4462.57548577]] \n",
      "   LossValidation:  [[ 3335.56324496]]\n",
      "iterations  305 \n",
      "   LossTrain:       [[ 4458.92579858]] \n",
      "   LossValidation:  [[ 3332.53869571]]\n",
      "iterations  306 \n",
      "   LossTrain:       [[ 4455.30833858]] \n",
      "   LossValidation:  [[ 3329.53878906]]\n",
      "iterations  307 \n",
      "   LossTrain:       [[ 4451.72276789]] \n",
      "   LossValidation:  [[ 3326.56329024]]\n",
      "iterations  308 \n",
      "   LossTrain:       [[ 4448.16875247]] \n",
      "   LossValidation:  [[ 3323.61196699]]\n",
      "iterations  309 \n",
      "   LossTrain:       [[ 4444.64596204]] \n",
      "   LossValidation:  [[ 3320.68458951]]\n",
      "iterations  310 \n",
      "   LossTrain:       [[ 4441.15407009]] \n",
      "   LossValidation:  [[ 3317.78093043]]\n",
      "iterations  311 \n",
      "   LossTrain:       [[ 4437.69275381]] \n",
      "   LossValidation:  [[ 3314.9007648]]\n",
      "iterations  312 \n",
      "   LossTrain:       [[ 4434.26169402]] \n",
      "   LossValidation:  [[ 3312.04387004]]\n",
      "iterations  313 \n",
      "   LossTrain:       [[ 4430.86057518]] \n",
      "   LossValidation:  [[ 3309.21002593]]\n",
      "iterations  314 \n",
      "   LossTrain:       [[ 4427.48908528]] \n",
      "   LossValidation:  [[ 3306.39901457]]\n",
      "iterations  315 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   LossTrain:       [[ 4424.14691587]] \n",
      "   LossValidation:  [[ 3303.61062037]]\n",
      "iterations  316 \n",
      "   LossTrain:       [[ 4420.83376197]] \n",
      "   LossValidation:  [[ 3300.84463001]]\n",
      "iterations  317 \n",
      "   LossTrain:       [[ 4417.54932204]] \n",
      "   LossValidation:  [[ 3298.10083242]]\n",
      "iterations  318 \n",
      "   LossTrain:       [[ 4414.29329794]] \n",
      "   LossValidation:  [[ 3295.37901874]]\n",
      "iterations  319 \n",
      "   LossTrain:       [[ 4411.06539489]] \n",
      "   LossValidation:  [[ 3292.67898233]]\n",
      "iterations  320 \n",
      "   LossTrain:       [[ 4407.86532144]] \n",
      "   LossValidation:  [[ 3290.0005187]]\n",
      "iterations  321 \n",
      "   LossTrain:       [[ 4404.69278941]] \n",
      "   LossValidation:  [[ 3287.34342554]]\n",
      "iterations  322 \n",
      "   LossTrain:       [[ 4401.54751387]] \n",
      "   LossValidation:  [[ 3284.70750263]]\n",
      "iterations  323 \n",
      "   LossTrain:       [[ 4398.42921309]] \n",
      "   LossValidation:  [[ 3282.09255186]]\n",
      "iterations  324 \n",
      "   LossTrain:       [[ 4395.33760851]] \n",
      "   LossValidation:  [[ 3279.49837722]]\n",
      "iterations  325 \n",
      "   LossTrain:       [[ 4392.2724247]] \n",
      "   LossValidation:  [[ 3276.92478472]]\n",
      "iterations  326 \n",
      "   LossTrain:       [[ 4389.2333893]] \n",
      "   LossValidation:  [[ 3274.37158242]]\n",
      "iterations  327 \n",
      "   LossTrain:       [[ 4386.22023305]] \n",
      "   LossValidation:  [[ 3271.83858039]]\n",
      "iterations  328 \n",
      "   LossTrain:       [[ 4383.23268966]] \n",
      "   LossValidation:  [[ 3269.32559065]]\n",
      "iterations  329 \n",
      "   LossTrain:       [[ 4380.27049586]] \n",
      "   LossValidation:  [[ 3266.83242723]]\n",
      "iterations  330 \n",
      "   LossTrain:       [[ 4377.33339131]] \n",
      "   LossValidation:  [[ 3264.35890608]]\n",
      "iterations  331 \n",
      "   LossTrain:       [[ 4374.42111859]] \n",
      "   LossValidation:  [[ 3261.90484504]]\n",
      "iterations  332 \n",
      "   LossTrain:       [[ 4371.53342315]] \n",
      "   LossValidation:  [[ 3259.4700639]]\n",
      "iterations  333 \n",
      "   LossTrain:       [[ 4368.6700533]] \n",
      "   LossValidation:  [[ 3257.05438428]]\n",
      "iterations  334 \n",
      "   LossTrain:       [[ 4365.83076015]] \n",
      "   LossValidation:  [[ 3254.65762969]]\n",
      "iterations  335 \n",
      "   LossTrain:       [[ 4363.01529759]] \n",
      "   LossValidation:  [[ 3252.27962543]]\n",
      "iterations  336 \n",
      "   LossTrain:       [[ 4360.22342227]] \n",
      "   LossValidation:  [[ 3249.92019866]]\n",
      "iterations  337 \n",
      "   LossTrain:       [[ 4357.45489353]] \n",
      "   LossValidation:  [[ 3247.5791783]]\n",
      "iterations  338 \n",
      "   LossTrain:       [[ 4354.70947342]] \n",
      "   LossValidation:  [[ 3245.25639506]]\n",
      "iterations  339 \n",
      "   LossTrain:       [[ 4351.98692661]] \n",
      "   LossValidation:  [[ 3242.95168139]]\n",
      "iterations  340 \n",
      "   LossTrain:       [[ 4349.28702041]] \n",
      "   LossValidation:  [[ 3240.66487148]]\n",
      "iterations  341 \n",
      "   LossTrain:       [[ 4346.60952472]] \n",
      "   LossValidation:  [[ 3238.39580124]]\n",
      "iterations  342 \n",
      "   LossTrain:       [[ 4343.95421198]] \n",
      "   LossValidation:  [[ 3236.14430825]]\n",
      "iterations  343 \n",
      "   LossTrain:       [[ 4341.32085718]] \n",
      "   LossValidation:  [[ 3233.9102318]]\n",
      "iterations  344 \n",
      "   LossTrain:       [[ 4338.70923778]] \n",
      "   LossValidation:  [[ 3231.69341282]]\n",
      "iterations  345 \n",
      "   LossTrain:       [[ 4336.11913374]] \n",
      "   LossValidation:  [[ 3229.49369387]]\n",
      "iterations  346 \n",
      "   LossTrain:       [[ 4333.55032745]] \n",
      "   LossValidation:  [[ 3227.31091914]]\n",
      "iterations  347 \n",
      "   LossTrain:       [[ 4331.00260371]] \n",
      "   LossValidation:  [[ 3225.14493442]]\n",
      "iterations  348 \n",
      "   LossTrain:       [[ 4328.4757497]] \n",
      "   LossValidation:  [[ 3222.99558709]]\n",
      "iterations  349 \n",
      "   LossTrain:       [[ 4325.96955495]] \n",
      "   LossValidation:  [[ 3220.8627261]]\n",
      "iterations  350 \n",
      "   LossTrain:       [[ 4323.48381134]] \n",
      "   LossValidation:  [[ 3218.74620192]]\n",
      "iterations  351 \n",
      "   LossTrain:       [[ 4321.01831303]] \n",
      "   LossValidation:  [[ 3216.6458666]]\n",
      "iterations  352 \n",
      "   LossTrain:       [[ 4318.57285647]] \n",
      "   LossValidation:  [[ 3214.56157366]]\n",
      "iterations  353 \n",
      "   LossTrain:       [[ 4316.14724034]] \n",
      "   LossValidation:  [[ 3212.49317814]]\n",
      "iterations  354 \n",
      "   LossTrain:       [[ 4313.74126557]] \n",
      "   LossValidation:  [[ 3210.44053657]]\n",
      "iterations  355 \n",
      "   LossTrain:       [[ 4311.35473525]] \n",
      "   LossValidation:  [[ 3208.40350692]]\n",
      "iterations  356 \n",
      "   LossTrain:       [[ 4308.98745467]] \n",
      "   LossValidation:  [[ 3206.38194864]]\n",
      "iterations  357 \n",
      "   LossTrain:       [[ 4306.63923125]] \n",
      "   LossValidation:  [[ 3204.37572258]]\n",
      "iterations  358 \n",
      "   LossTrain:       [[ 4304.30987453]] \n",
      "   LossValidation:  [[ 3202.38469104]]\n",
      "iterations  359 \n",
      "   LossTrain:       [[ 4301.99919615]] \n",
      "   LossValidation:  [[ 3200.40871769]]\n",
      "iterations  360 \n",
      "   LossTrain:       [[ 4299.70700982]] \n",
      "   LossValidation:  [[ 3198.44766762]]\n",
      "iterations  361 \n",
      "   LossTrain:       [[ 4297.4331313]] \n",
      "   LossValidation:  [[ 3196.50140727]]\n",
      "iterations  362 \n",
      "   LossTrain:       [[ 4295.17737836]] \n",
      "   LossValidation:  [[ 3194.56980442]]\n",
      "iterations  363 \n",
      "   LossTrain:       [[ 4292.93957078]] \n",
      "   LossValidation:  [[ 3192.65272823]]\n",
      "iterations  364 \n",
      "   LossTrain:       [[ 4290.71953033]] \n",
      "   LossValidation:  [[ 3190.75004916]]\n",
      "iterations  365 \n",
      "   LossTrain:       [[ 4288.5170807]] \n",
      "   LossValidation:  [[ 3188.86163898]]\n",
      "iterations  366 \n",
      "   LossTrain:       [[ 4286.33204754]] \n",
      "   LossValidation:  [[ 3186.98737078]]\n",
      "iterations  367 \n",
      "   LossTrain:       [[ 4284.1642584]] \n",
      "   LossValidation:  [[ 3185.1271189]]\n",
      "iterations  368 \n",
      "   LossTrain:       [[ 4282.01354271]] \n",
      "   LossValidation:  [[ 3183.28075898]]\n",
      "iterations  369 \n",
      "   LossTrain:       [[ 4279.87973177]] \n",
      "   LossValidation:  [[ 3181.44816789]]\n",
      "iterations  370 \n",
      "   LossTrain:       [[ 4277.76265873]] \n",
      "   LossValidation:  [[ 3179.62922375]]\n",
      "iterations  371 \n",
      "   LossTrain:       [[ 4275.66215855]] \n",
      "   LossValidation:  [[ 3177.82380593]]\n",
      "iterations  372 \n",
      "   LossTrain:       [[ 4273.57806799]] \n",
      "   LossValidation:  [[ 3176.03179497]]\n",
      "iterations  373 \n",
      "   LossTrain:       [[ 4271.5102256]] \n",
      "   LossValidation:  [[ 3174.25307264]]\n",
      "iterations  374 \n",
      "   LossTrain:       [[ 4269.45847169]] \n",
      "   LossValidation:  [[ 3172.4875219]]\n",
      "iterations  375 \n",
      "   LossTrain:       [[ 4267.4226483]] \n",
      "   LossValidation:  [[ 3170.73502687]]\n",
      "iterations  376 \n",
      "   LossTrain:       [[ 4265.4025992]] \n",
      "   LossValidation:  [[ 3168.99547283]]\n",
      "iterations  377 \n",
      "   LossTrain:       [[ 4263.39816984]] \n",
      "   LossValidation:  [[ 3167.26874623]]\n",
      "iterations  378 \n",
      "   LossTrain:       [[ 4261.40920737]] \n",
      "   LossValidation:  [[ 3165.55473464]]\n",
      "iterations  379 \n",
      "   LossTrain:       [[ 4259.43556058]] \n",
      "   LossValidation:  [[ 3163.85332676]]\n",
      "iterations  380 \n",
      "   LossTrain:       [[ 4257.47707994]] \n",
      "   LossValidation:  [[ 3162.16441239]]\n",
      "iterations  381 \n",
      "   LossTrain:       [[ 4255.5336175]] \n",
      "   LossValidation:  [[ 3160.48788245]]\n",
      "iterations  382 \n",
      "   LossTrain:       [[ 4253.60502693]] \n",
      "   LossValidation:  [[ 3158.82362893]]\n",
      "iterations  383 \n",
      "   LossTrain:       [[ 4251.69116348]] \n",
      "   LossValidation:  [[ 3157.1715449]]\n",
      "iterations  384 \n",
      "   LossTrain:       [[ 4249.79188399]] \n",
      "   LossValidation:  [[ 3155.53152451]]\n",
      "iterations  385 \n",
      "   LossTrain:       [[ 4247.90704683]] \n",
      "   LossValidation:  [[ 3153.90346295]]\n",
      "iterations  386 \n",
      "   LossTrain:       [[ 4246.03651189]] \n",
      "   LossValidation:  [[ 3152.28725644]]\n",
      "iterations  387 \n",
      "   LossTrain:       [[ 4244.18014059]] \n",
      "   LossValidation:  [[ 3150.68280225]]\n",
      "iterations  388 \n",
      "   LossTrain:       [[ 4242.33779586]] \n",
      "   LossValidation:  [[ 3149.08999865]]\n",
      "iterations  389 \n",
      "   LossTrain:       [[ 4240.50934208]] \n",
      "   LossValidation:  [[ 3147.50874495]]\n",
      "iterations  390 \n",
      "   LossTrain:       [[ 4238.6946451]] \n",
      "   LossValidation:  [[ 3145.93894142]]\n",
      "iterations  391 \n",
      "   LossTrain:       [[ 4236.89357224]] \n",
      "   LossValidation:  [[ 3144.38048933]]\n",
      "iterations  392 \n",
      "   LossTrain:       [[ 4235.10599222]] \n",
      "   LossValidation:  [[ 3142.83329093]]\n",
      "iterations  393 \n",
      "   LossTrain:       [[ 4233.33177518]] \n",
      "   LossValidation:  [[ 3141.29724944]]\n",
      "iterations  394 \n",
      "   LossTrain:       [[ 4231.57079267]] \n",
      "   LossValidation:  [[ 3139.77226902]]\n",
      "iterations  395 \n",
      "   LossTrain:       [[ 4229.82291761]] \n",
      "   LossValidation:  [[ 3138.25825478]]\n",
      "iterations  396 \n",
      "   LossTrain:       [[ 4228.08802428]] \n",
      "   LossValidation:  [[ 3136.75511277]]\n",
      "iterations  397 \n",
      "   LossTrain:       [[ 4226.36598832]] \n",
      "   LossValidation:  [[ 3135.26274995]]\n",
      "iterations  398 \n",
      "   LossTrain:       [[ 4224.65668671]] \n",
      "   LossValidation:  [[ 3133.78107422]]\n",
      "iterations  399 \n",
      "   LossTrain:       [[ 4222.95999772]] \n",
      "   LossValidation:  [[ 3132.30999435]]\n",
      "iterations  400 \n",
      "   LossTrain:       [[ 4221.27580095]] \n",
      "   LossValidation:  [[ 3130.84942004]]\n",
      "iterations  401 \n",
      "   LossTrain:       [[ 4219.60397729]] \n",
      "   LossValidation:  [[ 3129.39926184]]\n",
      "iterations  402 \n",
      "   LossTrain:       [[ 4217.94440889]] \n",
      "   LossValidation:  [[ 3127.9594312]]\n",
      "iterations  403 \n",
      "   LossTrain:       [[ 4216.29697916]] \n",
      "   LossValidation:  [[ 3126.52984043]]\n",
      "iterations  404 \n",
      "   LossTrain:       [[ 4214.66157278]] \n",
      "   LossValidation:  [[ 3125.11040269]]\n",
      "iterations  405 \n",
      "   LossTrain:       [[ 4213.03807562]] \n",
      "   LossValidation:  [[ 3123.701032]]\n",
      "iterations  406 \n",
      "   LossTrain:       [[ 4211.42637481]] \n",
      "   LossValidation:  [[ 3122.3016432]]\n",
      "iterations  407 \n",
      "   LossTrain:       [[ 4209.82635866]] \n",
      "   LossValidation:  [[ 3120.91215199]]\n",
      "iterations  408 \n",
      "   LossTrain:       [[ 4208.23791667]] \n",
      "   LossValidation:  [[ 3119.53247486]]\n",
      "iterations  409 \n",
      "   LossTrain:       [[ 4206.66093952]] \n",
      "   LossValidation:  [[ 3118.16252912]]\n",
      "iterations  410 \n",
      "   LossTrain:       [[ 4205.09531906]] \n",
      "   LossValidation:  [[ 3116.80223289]]\n",
      "iterations  411 \n",
      "   LossTrain:       [[ 4203.54094828]] \n",
      "   LossValidation:  [[ 3115.45150509]]\n",
      "iterations  412 \n",
      "   LossTrain:       [[ 4201.99772131]] \n",
      "   LossValidation:  [[ 3114.11026541]]\n",
      "iterations  413 \n",
      "   LossTrain:       [[ 4200.46553341]] \n",
      "   LossValidation:  [[ 3112.77843432]]\n",
      "iterations  414 \n",
      "   LossTrain:       [[ 4198.94428094]] \n",
      "   LossValidation:  [[ 3111.45593307]]\n",
      "iterations  415 \n",
      "   LossTrain:       [[ 4197.43386137]] \n",
      "   LossValidation:  [[ 3110.14268368]]\n",
      "iterations  416 \n",
      "   LossTrain:       [[ 4195.93417325]] \n",
      "   LossValidation:  [[ 3108.83860889]]\n",
      "iterations  417 \n",
      "   LossTrain:       [[ 4194.44511621]] \n",
      "   LossValidation:  [[ 3107.54363222]]\n",
      "iterations  418 \n",
      "   LossTrain:       [[ 4192.96659094]] \n",
      "   LossValidation:  [[ 3106.2576779]]\n",
      "iterations  419 \n",
      "   LossTrain:       [[ 4191.49849917]] \n",
      "   LossValidation:  [[ 3104.9806709]]\n",
      "iterations  420 \n",
      "   LossTrain:       [[ 4190.04074368]] \n",
      "   LossValidation:  [[ 3103.71253693]]\n",
      "iterations  421 \n",
      "   LossTrain:       [[ 4188.59322828]] \n",
      "   LossValidation:  [[ 3102.45320239]]\n",
      "iterations  422 \n",
      "   LossTrain:       [[ 4187.15585778]] \n",
      "   LossValidation:  [[ 3101.2025944]]\n",
      "iterations  423 \n",
      "   LossTrain:       [[ 4185.728538]] \n",
      "   LossValidation:  [[ 3099.96064076]]\n",
      "iterations  424 \n",
      "   LossTrain:       [[ 4184.31117576]] \n",
      "   LossValidation:  [[ 3098.72726999]]\n",
      "iterations  425 \n",
      "   LossTrain:       [[ 4182.90367886]] \n",
      "   LossValidation:  [[ 3097.50241127]]\n",
      "iterations  426 \n",
      "   LossTrain:       [[ 4181.50595605]] \n",
      "   LossValidation:  [[ 3096.28599447]]\n",
      "iterations  427 \n",
      "   LossTrain:       [[ 4180.11791707]] \n",
      "   LossValidation:  [[ 3095.07795013]]\n",
      "iterations  428 \n",
      "   LossTrain:       [[ 4178.73947258]] \n",
      "   LossValidation:  [[ 3093.87820945]]\n",
      "iterations  429 \n",
      "   LossTrain:       [[ 4177.3705342]] \n",
      "   LossValidation:  [[ 3092.68670428]]\n",
      "iterations  430 \n",
      "   LossTrain:       [[ 4176.01101447]] \n",
      "   LossValidation:  [[ 3091.50336714]]\n",
      "iterations  431 \n",
      "   LossTrain:       [[ 4174.66082684]] \n",
      "   LossValidation:  [[ 3090.32813116]]\n",
      "iterations  432 \n",
      "   LossTrain:       [[ 4173.31988567]] \n",
      "   LossValidation:  [[ 3089.16093013]]\n",
      "iterations  433 \n",
      "   LossTrain:       [[ 4171.98810622]] \n",
      "   LossValidation:  [[ 3088.00169847]]\n",
      "iterations  434 \n",
      "   LossTrain:       [[ 4170.66540464]] \n",
      "   LossValidation:  [[ 3086.85037121]]\n",
      "iterations  435 \n",
      "   LossTrain:       [[ 4169.35169794]] \n",
      "   LossValidation:  [[ 3085.70688399]]\n",
      "iterations  436 \n",
      "   LossTrain:       [[ 4168.04690402]] \n",
      "   LossValidation:  [[ 3084.57117309]]\n",
      "iterations  437 \n",
      "   LossTrain:       [[ 4166.75094162]] \n",
      "   LossValidation:  [[ 3083.44317535]]\n",
      "iterations  438 \n",
      "   LossTrain:       [[ 4165.46373034]] \n",
      "   LossValidation:  [[ 3082.32282825]]\n",
      "iterations  439 \n",
      "   LossTrain:       [[ 4164.18519059]] \n",
      "   LossValidation:  [[ 3081.21006982]]\n",
      "iterations  440 \n",
      "   LossTrain:       [[ 4162.91524365]] \n",
      "   LossValidation:  [[ 3080.10483871]]\n",
      "iterations  441 \n",
      "   LossTrain:       [[ 4161.65381159]] \n",
      "   LossValidation:  [[ 3079.00707412]]\n",
      "iterations  442 \n",
      "   LossTrain:       [[ 4160.40081731]] \n",
      "   LossValidation:  [[ 3077.91671584]]\n",
      "iterations  443 \n",
      "   LossTrain:       [[ 4159.15618448]] \n",
      "   LossValidation:  [[ 3076.83370422]]\n",
      "iterations  444 \n",
      "   LossTrain:       [[ 4157.9198376]] \n",
      "   LossValidation:  [[ 3075.75798017]]\n",
      "iterations  445 \n",
      "   LossTrain:       [[ 4156.69170193]] \n",
      "   LossValidation:  [[ 3074.68948515]]\n",
      "iterations  446 \n",
      "   LossTrain:       [[ 4155.47170351]] \n",
      "   LossValidation:  [[ 3073.62816118]]\n",
      "iterations  447 \n",
      "   LossTrain:       [[ 4154.25976915]] \n",
      "   LossValidation:  [[ 3072.57395081]]\n",
      "iterations  448 \n",
      "   LossTrain:       [[ 4153.05582642]] \n",
      "   LossValidation:  [[ 3071.52679715]]\n",
      "iterations  449 \n",
      "   LossTrain:       [[ 4151.85980362]] \n",
      "   LossValidation:  [[ 3070.4866438]]\n",
      "iterations  450 \n",
      "   LossTrain:       [[ 4150.67162982]] \n",
      "   LossValidation:  [[ 3069.45343493]]\n",
      "iterations  451 \n",
      "   LossTrain:       [[ 4149.4912348]] \n",
      "   LossValidation:  [[ 3068.4271152]]\n",
      "iterations  452 \n",
      "   LossTrain:       [[ 4148.31854908]] \n",
      "   LossValidation:  [[ 3067.4076298]]\n",
      "iterations  453 \n",
      "   LossTrain:       [[ 4147.15350389]] \n",
      "   LossValidation:  [[ 3066.39492444]]\n",
      "iterations  454 \n",
      "   LossTrain:       [[ 4145.99603116]] \n",
      "   LossValidation:  [[ 3065.38894531]]\n",
      "iterations  455 \n",
      "   LossTrain:       [[ 4144.84606354]] \n",
      "   LossValidation:  [[ 3064.38963912]]\n",
      "iterations  456 \n",
      "   LossTrain:       [[ 4143.70353435]] \n",
      "   LossValidation:  [[ 3063.39695305]]\n",
      "iterations  457 \n",
      "   LossTrain:       [[ 4142.56837762]] \n",
      "   LossValidation:  [[ 3062.4108348]]\n",
      "iterations  458 \n",
      "   LossTrain:       [[ 4141.44052804]] \n",
      "   LossValidation:  [[ 3061.43123254]]\n",
      "iterations  459 \n",
      "   LossTrain:       [[ 4140.31992097]] \n",
      "   LossValidation:  [[ 3060.45809492]]\n",
      "iterations  460 \n",
      "   LossTrain:       [[ 4139.20649245]] \n",
      "   LossValidation:  [[ 3059.49137105]]\n",
      "iterations  461 \n",
      "   LossTrain:       [[ 4138.10017916]] \n",
      "   LossValidation:  [[ 3058.53101054]]\n",
      "iterations  462 \n",
      "   LossTrain:       [[ 4137.00091844]] \n",
      "   LossValidation:  [[ 3057.57696343]]\n",
      "iterations  463 \n",
      "   LossTrain:       [[ 4135.90864825]] \n",
      "   LossValidation:  [[ 3056.62918025]]\n",
      "iterations  464 \n",
      "   LossTrain:       [[ 4134.82330721]] \n",
      "   LossValidation:  [[ 3055.68761196]]\n",
      "iterations  465 \n",
      "   LossTrain:       [[ 4133.74483456]] \n",
      "   LossValidation:  [[ 3054.75221]]\n",
      "iterations  466 \n",
      "   LossTrain:       [[ 4132.67317014]] \n",
      "   LossValidation:  [[ 3053.82292623]]\n",
      "iterations  467 \n",
      "   LossTrain:       [[ 4131.60825444]] \n",
      "   LossValidation:  [[ 3052.89971296]]\n",
      "iterations  468 \n",
      "   LossTrain:       [[ 4130.55002852]] \n",
      "   LossValidation:  [[ 3051.98252293]]\n",
      "iterations  469 \n",
      "   LossTrain:       [[ 4129.49843407]] \n",
      "   LossValidation:  [[ 3051.07130932]]\n",
      "iterations  470 \n",
      "   LossTrain:       [[ 4128.45341335]] \n",
      "   LossValidation:  [[ 3050.16602575]]\n",
      "iterations  471 \n",
      "   LossTrain:       [[ 4127.41490922]] \n",
      "   LossValidation:  [[ 3049.26662624]]\n",
      "iterations  472 \n",
      "   LossTrain:       [[ 4126.38286513]] \n",
      "   LossValidation:  [[ 3048.37306525]]\n",
      "iterations  473 \n",
      "   LossTrain:       [[ 4125.35722508]] \n",
      "   LossValidation:  [[ 3047.48529762]]\n",
      "iterations  474 \n",
      "   LossTrain:       [[ 4124.33793365]] \n",
      "   LossValidation:  [[ 3046.60327865]]\n",
      "iterations  475 \n",
      "   LossTrain:       [[ 4123.32493599]] \n",
      "   LossValidation:  [[ 3045.72696401]]\n",
      "iterations  476 \n",
      "   LossTrain:       [[ 4122.31817779]] \n",
      "   LossValidation:  [[ 3044.85630979]]\n",
      "iterations  477 \n",
      "   LossTrain:       [[ 4121.31760531]] \n",
      "   LossValidation:  [[ 3043.99127247]]\n",
      "iterations  478 \n",
      "   LossTrain:       [[ 4120.32316532]] \n",
      "   LossValidation:  [[ 3043.13180892]]\n",
      "iterations  479 \n",
      "   LossTrain:       [[ 4119.33480517]] \n",
      "   LossValidation:  [[ 3042.27787642]]\n",
      "iterations  480 \n",
      "   LossTrain:       [[ 4118.35247271]] \n",
      "   LossValidation:  [[ 3041.42943261]]\n",
      "iterations  481 \n",
      "   LossTrain:       [[ 4117.37611632]] \n",
      "   LossValidation:  [[ 3040.58643554]]\n",
      "iterations  482 \n",
      "   LossTrain:       [[ 4116.40568491]] \n",
      "   LossValidation:  [[ 3039.74884361]]\n",
      "iterations  483 \n",
      "   LossTrain:       [[ 4115.44112791]] \n",
      "   LossValidation:  [[ 3038.91661562]]\n",
      "iterations  484 \n",
      "   LossTrain:       [[ 4114.48239524]] \n",
      "   LossValidation:  [[ 3038.08971072]]\n",
      "iterations  485 \n",
      "   LossTrain:       [[ 4113.52943734]] \n",
      "   LossValidation:  [[ 3037.26808844]]\n",
      "iterations  486 \n",
      "   LossTrain:       [[ 4112.58220513]] \n",
      "   LossValidation:  [[ 3036.45170867]]\n",
      "iterations  487 \n",
      "   LossTrain:       [[ 4111.64065005]] \n",
      "   LossValidation:  [[ 3035.64053167]]\n",
      "iterations  488 \n",
      "   LossTrain:       [[ 4110.70472399]] \n",
      "   LossValidation:  [[ 3034.83451803]]\n",
      "iterations  489 \n",
      "   LossTrain:       [[ 4109.77437935]] \n",
      "   LossValidation:  [[ 3034.03362872]]\n",
      "iterations  490 \n",
      "   LossTrain:       [[ 4108.84956901]] \n",
      "   LossValidation:  [[ 3033.23782505]]\n",
      "iterations  491 \n",
      "   LossTrain:       [[ 4107.9302463]] \n",
      "   LossValidation:  [[ 3032.44706867]]\n",
      "iterations  492 \n",
      "   LossTrain:       [[ 4107.01636502]] \n",
      "   LossValidation:  [[ 3031.66132157]]\n",
      "iterations  493 \n",
      "   LossTrain:       [[ 4106.10787945]] \n",
      "   LossValidation:  [[ 3030.88054609]]\n",
      "iterations  494 \n",
      "   LossTrain:       [[ 4105.20474431]] \n",
      "   LossValidation:  [[ 3030.10470491]]\n",
      "iterations  495 \n",
      "   LossTrain:       [[ 4104.30691478]] \n",
      "   LossValidation:  [[ 3029.33376101]]\n",
      "iterations  496 \n",
      "   LossTrain:       [[ 4103.41434647]] \n",
      "   LossValidation:  [[ 3028.56767773]]\n",
      "iterations  497 \n",
      "   LossTrain:       [[ 4102.52699545]] \n",
      "   LossValidation:  [[ 3027.80641874]]\n",
      "iterations  498 \n",
      "   LossTrain:       [[ 4101.64481822]] \n",
      "   LossValidation:  [[ 3027.04994799]]\n",
      "iterations  499 \n",
      "   LossTrain:       [[ 4100.76777171]] \n",
      "   LossValidation:  [[ 3026.2982298]]\n",
      "iterations  500 \n",
      "   LossTrain:       [[ 4099.89581328]] \n",
      "   LossValidation:  [[ 3025.55122876]]\n",
      "iterations  501 \n",
      "   LossTrain:       [[ 4099.02890072]] \n",
      "   LossValidation:  [[ 3024.80890982]]\n",
      "iterations  502 \n",
      "   LossTrain:       [[ 4098.16699222]] \n",
      "   LossValidation:  [[ 3024.0712382]]\n",
      "iterations  503 \n",
      "   LossTrain:       [[ 4097.31004641]] \n",
      "   LossValidation:  [[ 3023.33817943]]\n",
      "iterations  504 \n",
      "   LossTrain:       [[ 4096.45802229]] \n",
      "   LossValidation:  [[ 3022.60969938]]\n",
      "iterations  505 \n",
      "   LossTrain:       [[ 4095.61087931]] \n",
      "   LossValidation:  [[ 3021.88576417]]\n",
      "iterations  506 \n",
      "   LossTrain:       [[ 4094.76857729]] \n",
      "   LossValidation:  [[ 3021.16634025]]\n",
      "iterations  507 \n",
      "   LossTrain:       [[ 4093.93107646]] \n",
      "   LossValidation:  [[ 3020.45139435]]\n",
      "iterations  508 \n",
      "   LossTrain:       [[ 4093.09833744]] \n",
      "   LossValidation:  [[ 3019.74089351]]\n",
      "iterations  509 \n",
      "   LossTrain:       [[ 4092.27032122]] \n",
      "   LossValidation:  [[ 3019.03480504]]\n",
      "iterations  510 \n",
      "   LossTrain:       [[ 4091.4469892]] \n",
      "   LossValidation:  [[ 3018.33309653]]\n",
      "iterations  511 \n",
      "   LossTrain:       [[ 4090.62830315]] \n",
      "   LossValidation:  [[ 3017.63573588]]\n",
      "iterations  512 \n",
      "   LossTrain:       [[ 4089.81422521]] \n",
      "   LossValidation:  [[ 3016.94269124]]\n",
      "iterations  513 \n",
      "   LossTrain:       [[ 4089.0047179]] \n",
      "   LossValidation:  [[ 3016.25393106]]\n",
      "iterations  514 \n",
      "   LossTrain:       [[ 4088.1997441]] \n",
      "   LossValidation:  [[ 3015.56942406]]\n",
      "iterations  515 \n",
      "   LossTrain:       [[ 4087.39926706]] \n",
      "   LossValidation:  [[ 3014.88913922]]\n",
      "iterations  516 \n",
      "   LossTrain:       [[ 4086.60325037]] \n",
      "   LossValidation:  [[ 3014.2130458]]\n",
      "iterations  517 \n",
      "   LossTrain:       [[ 4085.81165801]] \n",
      "   LossValidation:  [[ 3013.54111332]]\n",
      "iterations  518 \n",
      "   LossTrain:       [[ 4085.02445429]] \n",
      "   LossValidation:  [[ 3012.87331157]]\n",
      "iterations  519 \n",
      "   LossTrain:       [[ 4084.24160386]] \n",
      "   LossValidation:  [[ 3012.2096106]]\n",
      "iterations  520 \n",
      "   LossTrain:       [[ 4083.46307174]] \n",
      "   LossValidation:  [[ 3011.54998071]]\n",
      "iterations  521 \n",
      "   LossTrain:       [[ 4082.68882326]] \n",
      "   LossValidation:  [[ 3010.89439248]]\n",
      "iterations  522 \n",
      "   LossTrain:       [[ 4081.91882413]] \n",
      "   LossValidation:  [[ 3010.24281672]]\n",
      "iterations  523 \n",
      "   LossTrain:       [[ 4081.15304035]] \n",
      "   LossValidation:  [[ 3009.59522449]]\n",
      "iterations  524 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   LossTrain:       [[ 4080.39143826]] \n",
      "   LossValidation:  [[ 3008.95158712]]\n",
      "iterations  525 \n",
      "   LossTrain:       [[ 4079.63398456]] \n",
      "   LossValidation:  [[ 3008.31187617]]\n",
      "iterations  526 \n",
      "   LossTrain:       [[ 4078.88064622]] \n",
      "   LossValidation:  [[ 3007.67606344]]\n",
      "iterations  527 \n",
      "   LossTrain:       [[ 4078.13139058]] \n",
      "   LossValidation:  [[ 3007.044121]]\n",
      "iterations  528 \n",
      "   LossTrain:       [[ 4077.38618525]] \n",
      "   LossValidation:  [[ 3006.41602112]]\n",
      "iterations  529 \n",
      "   LossTrain:       [[ 4076.6449982]] \n",
      "   LossValidation:  [[ 3005.79173633]]\n",
      "iterations  530 \n",
      "   LossTrain:       [[ 4075.90779767]] \n",
      "   LossValidation:  [[ 3005.17123939]]\n",
      "iterations  531 \n",
      "   LossTrain:       [[ 4075.17455222]] \n",
      "   LossValidation:  [[ 3004.5545033]]\n",
      "iterations  532 \n",
      "   LossTrain:       [[ 4074.44523073]] \n",
      "   LossValidation:  [[ 3003.94150128]]\n",
      "iterations  533 \n",
      "   LossTrain:       [[ 4073.71980235]] \n",
      "   LossValidation:  [[ 3003.33220678]]\n",
      "iterations  534 \n",
      "   LossTrain:       [[ 4072.99823655]] \n",
      "   LossValidation:  [[ 3002.72659347]]\n",
      "iterations  535 \n",
      "   LossTrain:       [[ 4072.28050308]] \n",
      "   LossValidation:  [[ 3002.12463526]]\n",
      "iterations  536 \n",
      "   LossTrain:       [[ 4071.56657198]] \n",
      "   LossValidation:  [[ 3001.52630626]]\n",
      "iterations  537 \n",
      "   LossTrain:       [[ 4070.8564136]] \n",
      "   LossValidation:  [[ 3000.93158083]]\n",
      "iterations  538 \n",
      "   LossTrain:       [[ 4070.14999854]] \n",
      "   LossValidation:  [[ 3000.3404335]]\n",
      "iterations  539 \n",
      "   LossTrain:       [[ 4069.4472977]] \n",
      "   LossValidation:  [[ 2999.75283907]]\n",
      "iterations  540 \n",
      "   LossTrain:       [[ 4068.74828226]] \n",
      "   LossValidation:  [[ 2999.1687725]]\n",
      "iterations  541 \n",
      "   LossTrain:       [[ 4068.05292368]] \n",
      "   LossValidation:  [[ 2998.58820901]]\n",
      "iterations  542 \n",
      "   LossTrain:       [[ 4067.36119367]] \n",
      "   LossValidation:  [[ 2998.01112398]]\n",
      "iterations  543 \n",
      "   LossTrain:       [[ 4066.67306423]] \n",
      "   LossValidation:  [[ 2997.43749305]]\n",
      "iterations  544 \n",
      "   LossTrain:       [[ 4065.98850762]] \n",
      "   LossValidation:  [[ 2996.86729201]]\n",
      "iterations  545 \n",
      "   LossTrain:       [[ 4065.30749636]] \n",
      "   LossValidation:  [[ 2996.30049689]]\n",
      "iterations  546 \n",
      "   LossTrain:       [[ 4064.63000325]] \n",
      "   LossValidation:  [[ 2995.73708391]]\n",
      "iterations  547 \n",
      "   LossTrain:       [[ 4063.95600132]] \n",
      "   LossValidation:  [[ 2995.1770295]]\n",
      "iterations  548 \n",
      "   LossTrain:       [[ 4063.28546387]] \n",
      "   LossValidation:  [[ 2994.62031026]]\n",
      "iterations  549 \n",
      "   LossTrain:       [[ 4062.61836447]] \n",
      "   LossValidation:  [[ 2994.066903]]\n",
      "iterations  550 \n",
      "   LossTrain:       [[ 4061.9546769]] \n",
      "   LossValidation:  [[ 2993.51678474]]\n",
      "iterations  551 \n",
      "   LossTrain:       [[ 4061.29437523]] \n",
      "   LossValidation:  [[ 2992.96993267]]\n",
      "iterations  552 \n",
      "   LossTrain:       [[ 4060.63743374]] \n",
      "   LossValidation:  [[ 2992.42632418]]\n",
      "iterations  553 \n",
      "   LossTrain:       [[ 4059.98382699]] \n",
      "   LossValidation:  [[ 2991.88593683]]\n",
      "iterations  554 \n",
      "   LossTrain:       [[ 4059.33352974]] \n",
      "   LossValidation:  [[ 2991.34874839]]\n",
      "iterations  555 \n",
      "   LossTrain:       [[ 4058.68651701]] \n",
      "   LossValidation:  [[ 2990.8147368]]\n",
      "iterations  556 \n",
      "   LossTrain:       [[ 4058.04276406]] \n",
      "   LossValidation:  [[ 2990.28388019]]\n",
      "iterations  557 \n",
      "   LossTrain:       [[ 4057.40224638]] \n",
      "   LossValidation:  [[ 2989.75615687]]\n",
      "iterations  558 \n",
      "   LossTrain:       [[ 4056.76493966]] \n",
      "   LossValidation:  [[ 2989.23154533]]\n",
      "iterations  559 \n",
      "   LossTrain:       [[ 4056.13081987]] \n",
      "   LossValidation:  [[ 2988.71002422]]\n",
      "iterations  560 \n",
      "   LossTrain:       [[ 4055.49986316]] \n",
      "   LossValidation:  [[ 2988.19157239]]\n",
      "iterations  561 \n",
      "   LossTrain:       [[ 4054.87204592]] \n",
      "   LossValidation:  [[ 2987.67616885]]\n",
      "iterations  562 \n",
      "   LossTrain:       [[ 4054.24734478]] \n",
      "   LossValidation:  [[ 2987.16379278]]\n",
      "iterations  563 \n",
      "   LossTrain:       [[ 4053.62573655]] \n",
      "   LossValidation:  [[ 2986.65442356]]\n",
      "iterations  564 \n",
      "   LossTrain:       [[ 4053.00719829]] \n",
      "   LossValidation:  [[ 2986.14804069]]\n",
      "iterations  565 \n",
      "   LossTrain:       [[ 4052.39170726]] \n",
      "   LossValidation:  [[ 2985.64462388]]\n",
      "iterations  566 \n",
      "   LossTrain:       [[ 4051.77924093]] \n",
      "   LossValidation:  [[ 2985.14415298]]\n",
      "iterations  567 \n",
      "   LossTrain:       [[ 4051.16977697]] \n",
      "   LossValidation:  [[ 2984.64660802]]\n",
      "iterations  568 \n",
      "   LossTrain:       [[ 4050.56329329]] \n",
      "   LossValidation:  [[ 2984.15196919]]\n",
      "iterations  569 \n",
      "   LossTrain:       [[ 4049.95976797]] \n",
      "   LossValidation:  [[ 2983.66021683]]\n",
      "iterations  570 \n",
      "   LossTrain:       [[ 4049.35917931]] \n",
      "   LossValidation:  [[ 2983.17133146]]\n",
      "iterations  571 \n",
      "   LossTrain:       [[ 4048.76150581]] \n",
      "   LossValidation:  [[ 2982.68529374]]\n",
      "iterations  572 \n",
      "   LossTrain:       [[ 4048.16672617]] \n",
      "   LossValidation:  [[ 2982.20208449]]\n",
      "iterations  573 \n",
      "   LossTrain:       [[ 4047.57481928]] \n",
      "   LossValidation:  [[ 2981.7216847]]\n",
      "iterations  574 \n",
      "   LossTrain:       [[ 4046.98576423]] \n",
      "   LossValidation:  [[ 2981.24407549]]\n",
      "iterations  575 \n",
      "   LossTrain:       [[ 4046.39954031]] \n",
      "   LossValidation:  [[ 2980.76923815]]\n",
      "iterations  576 \n",
      "   LossTrain:       [[ 4045.81612698]] \n",
      "   LossValidation:  [[ 2980.29715412]]\n",
      "iterations  577 \n",
      "   LossTrain:       [[ 4045.23550391]] \n",
      "   LossValidation:  [[ 2979.82780499]]\n",
      "iterations  578 \n",
      "   LossTrain:       [[ 4044.65765095]] \n",
      "   LossValidation:  [[ 2979.36117249]]\n",
      "iterations  579 \n",
      "   LossTrain:       [[ 4044.08254813]] \n",
      "   LossValidation:  [[ 2978.8972385]]\n",
      "iterations  580 \n",
      "   LossTrain:       [[ 4043.51017567]] \n",
      "   LossValidation:  [[ 2978.43598505]]\n",
      "iterations  581 \n",
      "   LossTrain:       [[ 4042.94051396]] \n",
      "   LossValidation:  [[ 2977.97739431]]\n",
      "iterations  582 \n",
      "   LossTrain:       [[ 4042.37354359]] \n",
      "   LossValidation:  [[ 2977.5214486]]\n",
      "iterations  583 \n",
      "   LossTrain:       [[ 4041.8092453]] \n",
      "   LossValidation:  [[ 2977.06813036]]\n",
      "iterations  584 \n",
      "   LossTrain:       [[ 4041.24760003]] \n",
      "   LossValidation:  [[ 2976.61742221]]\n",
      "iterations  585 \n",
      "   LossTrain:       [[ 4040.68858889]] \n",
      "   LossValidation:  [[ 2976.16930687]]\n",
      "iterations  586 \n",
      "   LossTrain:       [[ 4040.13219315]] \n",
      "   LossValidation:  [[ 2975.72376721]]\n",
      "iterations  587 \n",
      "   LossTrain:       [[ 4039.57839425]] \n",
      "   LossValidation:  [[ 2975.28078625]]\n",
      "iterations  588 \n",
      "   LossTrain:       [[ 4039.02717382]] \n",
      "   LossValidation:  [[ 2974.84034713]]\n",
      "iterations  589 \n",
      "   LossTrain:       [[ 4038.47851364]] \n",
      "   LossValidation:  [[ 2974.40243312]]\n",
      "iterations  590 \n",
      "   LossTrain:       [[ 4037.93239564]] \n",
      "   LossValidation:  [[ 2973.96702765]]\n",
      "iterations  591 \n",
      "   LossTrain:       [[ 4037.38880195]] \n",
      "   LossValidation:  [[ 2973.53411425]]\n",
      "iterations  592 \n",
      "   LossTrain:       [[ 4036.84771484]] \n",
      "   LossValidation:  [[ 2973.10367659]]\n",
      "iterations  593 \n",
      "   LossTrain:       [[ 4036.30911675]] \n",
      "   LossValidation:  [[ 2972.67569847]]\n",
      "iterations  594 \n",
      "   LossTrain:       [[ 4035.77299025]] \n",
      "   LossValidation:  [[ 2972.25016383]]\n",
      "iterations  595 \n",
      "   LossTrain:       [[ 4035.23931811]] \n",
      "   LossValidation:  [[ 2971.82705673]]\n",
      "iterations  596 \n",
      "   LossTrain:       [[ 4034.70808322]] \n",
      "   LossValidation:  [[ 2971.40636133]]\n",
      "iterations  597 \n",
      "   LossTrain:       [[ 4034.17926864]] \n",
      "   LossValidation:  [[ 2970.98806196]]\n",
      "iterations  598 \n",
      "   LossTrain:       [[ 4033.65285759]] \n",
      "   LossValidation:  [[ 2970.57214303]]\n",
      "iterations  599 \n",
      "   LossTrain:       [[ 4033.12883343]] \n",
      "   LossValidation:  [[ 2970.15858911]]\n",
      "iterations  600 \n",
      "   LossTrain:       [[ 4032.60717966]] \n",
      "   LossValidation:  [[ 2969.74738486]]\n",
      "iterations  601 \n",
      "   LossTrain:       [[ 4032.08787994]] \n",
      "   LossValidation:  [[ 2969.33851507]]\n",
      "iterations  602 \n",
      "   LossTrain:       [[ 4031.57091808]] \n",
      "   LossValidation:  [[ 2968.93196467]]\n",
      "iterations  603 \n",
      "   LossTrain:       [[ 4031.05627802]] \n",
      "   LossValidation:  [[ 2968.52771867]]\n",
      "iterations  604 \n",
      "   LossTrain:       [[ 4030.54394386]] \n",
      "   LossValidation:  [[ 2968.12576222]]\n",
      "iterations  605 \n",
      "   LossTrain:       [[ 4030.03389983]] \n",
      "   LossValidation:  [[ 2967.72608059]]\n",
      "iterations  606 \n",
      "   LossTrain:       [[ 4029.52613029]] \n",
      "   LossValidation:  [[ 2967.32865914]]\n",
      "iterations  607 \n",
      "   LossTrain:       [[ 4029.02061978]] \n",
      "   LossValidation:  [[ 2966.93348338]]\n",
      "iterations  608 \n",
      "   LossTrain:       [[ 4028.51735293]] \n",
      "   LossValidation:  [[ 2966.5405389]]\n",
      "iterations  609 \n",
      "   LossTrain:       [[ 4028.01631452]] \n",
      "   LossValidation:  [[ 2966.14981141]]\n",
      "iterations  610 \n",
      "   LossTrain:       [[ 4027.51748949]] \n",
      "   LossValidation:  [[ 2965.76128675]]\n",
      "iterations  611 \n",
      "   LossTrain:       [[ 4027.02086289]] \n",
      "   LossValidation:  [[ 2965.37495084]]\n",
      "iterations  612 \n",
      "   LossTrain:       [[ 4026.52641989]] \n",
      "   LossValidation:  [[ 2964.99078972]]\n",
      "iterations  613 \n",
      "   LossTrain:       [[ 4026.03414583]] \n",
      "   LossValidation:  [[ 2964.60878955]]\n",
      "iterations  614 \n",
      "   LossTrain:       [[ 4025.54402614]] \n",
      "   LossValidation:  [[ 2964.22893658]]\n",
      "iterations  615 \n",
      "   LossTrain:       [[ 4025.05604639]] \n",
      "   LossValidation:  [[ 2963.85121717]]\n",
      "iterations  616 \n",
      "   LossTrain:       [[ 4024.5701923]] \n",
      "   LossValidation:  [[ 2963.47561779]]\n",
      "iterations  617 \n",
      "   LossTrain:       [[ 4024.08644968]] \n",
      "   LossValidation:  [[ 2963.10212502]]\n",
      "iterations  618 \n",
      "   LossTrain:       [[ 4023.60480449]] \n",
      "   LossValidation:  [[ 2962.73072551]]\n",
      "iterations  619 \n",
      "   LossTrain:       [[ 4023.1252428]] \n",
      "   LossValidation:  [[ 2962.36140606]]\n",
      "iterations  620 \n",
      "   LossTrain:       [[ 4022.6477508]] \n",
      "   LossValidation:  [[ 2961.99415353]]\n",
      "iterations  621 \n",
      "   LossTrain:       [[ 4022.17231482]] \n",
      "   LossValidation:  [[ 2961.6289549]]\n",
      "iterations  622 \n",
      "   LossTrain:       [[ 4021.69892129]] \n",
      "   LossValidation:  [[ 2961.26579724]]\n",
      "iterations  623 \n",
      "   LossTrain:       [[ 4021.22755676]] \n",
      "   LossValidation:  [[ 2960.90466774]]\n",
      "iterations  624 \n",
      "   LossTrain:       [[ 4020.7582079]] \n",
      "   LossValidation:  [[ 2960.54555365]]\n",
      "iterations  625 \n",
      "   LossTrain:       [[ 4020.2908615]] \n",
      "   LossValidation:  [[ 2960.18844236]]\n",
      "iterations  626 \n",
      "   LossTrain:       [[ 4019.82550446]] \n",
      "   LossValidation:  [[ 2959.83332131]]\n",
      "iterations  627 \n",
      "   LossTrain:       [[ 4019.3621238]] \n",
      "   LossValidation:  [[ 2959.48017808]]\n",
      "iterations  628 \n",
      "   LossTrain:       [[ 4018.90070664]] \n",
      "   LossValidation:  [[ 2959.1290003]]\n",
      "iterations  629 \n",
      "   LossTrain:       [[ 4018.44124022]] \n",
      "   LossValidation:  [[ 2958.77977573]]\n",
      "iterations  630 \n",
      "   LossTrain:       [[ 4017.98371189]] \n",
      "   LossValidation:  [[ 2958.43249221]]\n",
      "iterations  631 \n",
      "   LossTrain:       [[ 4017.52810912]] \n",
      "   LossValidation:  [[ 2958.08713766]]\n",
      "iterations  632 \n",
      "   LossTrain:       [[ 4017.07441945]] \n",
      "   LossValidation:  [[ 2957.74370011]]\n",
      "iterations  633 \n",
      "   LossTrain:       [[ 4016.62263057]] \n",
      "   LossValidation:  [[ 2957.40216765]]\n",
      "iterations  634 \n",
      "   LossTrain:       [[ 4016.17273026]] \n",
      "   LossValidation:  [[ 2957.06252851]]\n",
      "iterations  635 \n",
      "   LossTrain:       [[ 4015.7247064]] \n",
      "   LossValidation:  [[ 2956.72477095]]\n",
      "iterations  636 \n",
      "   LossTrain:       [[ 4015.27854697]] \n",
      "   LossValidation:  [[ 2956.38888336]]\n",
      "iterations  637 \n",
      "   LossTrain:       [[ 4014.83424008]] \n",
      "   LossValidation:  [[ 2956.05485419]]\n",
      "iterations  638 \n",
      "   LossTrain:       [[ 4014.3917739]] \n",
      "   LossValidation:  [[ 2955.722672]]\n",
      "iterations  639 \n",
      "   LossTrain:       [[ 4013.95113673]] \n",
      "   LossValidation:  [[ 2955.39232542]]\n",
      "iterations  640 \n",
      "   LossTrain:       [[ 4013.51231697]] \n",
      "   LossValidation:  [[ 2955.06380317]]\n",
      "iterations  641 \n",
      "   LossTrain:       [[ 4013.0753031]] \n",
      "   LossValidation:  [[ 2954.73709405]]\n",
      "iterations  642 \n",
      "   LossTrain:       [[ 4012.64008372]] \n",
      "   LossValidation:  [[ 2954.41218694]]\n",
      "iterations  643 \n",
      "   LossTrain:       [[ 4012.2066475]] \n",
      "   LossValidation:  [[ 2954.08907081]]\n",
      "iterations  644 \n",
      "   LossTrain:       [[ 4011.77498324]] \n",
      "   LossValidation:  [[ 2953.7677347]]\n",
      "iterations  645 \n",
      "   LossTrain:       [[ 4011.34507981]] \n",
      "   LossValidation:  [[ 2953.44816776]]\n",
      "iterations  646 \n",
      "   LossTrain:       [[ 4010.91692618]] \n",
      "   LossValidation:  [[ 2953.13035919]]\n",
      "iterations  647 \n",
      "   LossTrain:       [[ 4010.49051142]] \n",
      "   LossValidation:  [[ 2952.81429828]]\n",
      "iterations  648 \n",
      "   LossTrain:       [[ 4010.06582467]] \n",
      "   LossValidation:  [[ 2952.4999744]]\n",
      "iterations  649 \n",
      "   LossTrain:       [[ 4009.64285519]] \n",
      "   LossValidation:  [[ 2952.18737698]]\n",
      "iterations  650 \n",
      "   LossTrain:       [[ 4009.22159232]] \n",
      "   LossValidation:  [[ 2951.87649557]]\n",
      "iterations  651 \n",
      "   LossTrain:       [[ 4008.80202548]] \n",
      "   LossValidation:  [[ 2951.56731976]]\n",
      "iterations  652 \n",
      "   LossTrain:       [[ 4008.38414419]] \n",
      "   LossValidation:  [[ 2951.25983922]]\n",
      "iterations  653 \n",
      "   LossTrain:       [[ 4007.96793805]] \n",
      "   LossValidation:  [[ 2950.95404372]]\n",
      "iterations  654 \n",
      "   LossTrain:       [[ 4007.55339675]] \n",
      "   LossValidation:  [[ 2950.64992307]]\n",
      "iterations  655 \n",
      "   LossTrain:       [[ 4007.14051007]] \n",
      "   LossValidation:  [[ 2950.34746717]]\n",
      "iterations  656 \n",
      "   LossTrain:       [[ 4006.72926788]] \n",
      "   LossValidation:  [[ 2950.04666602]]\n",
      "iterations  657 \n",
      "   LossTrain:       [[ 4006.31966011]] \n",
      "   LossValidation:  [[ 2949.74750964]]\n",
      "iterations  658 \n",
      "   LossTrain:       [[ 4005.91167681]] \n",
      "   LossValidation:  [[ 2949.44998817]]\n",
      "iterations  659 \n",
      "   LossTrain:       [[ 4005.50530807]] \n",
      "   LossValidation:  [[ 2949.15409179]]\n",
      "iterations  660 \n",
      "   LossTrain:       [[ 4005.1005441]] \n",
      "   LossValidation:  [[ 2948.85981077]]\n",
      "iterations  661 \n",
      "   LossTrain:       [[ 4004.69737517]] \n",
      "   LossValidation:  [[ 2948.56713544]]\n",
      "iterations  662 \n",
      "   LossTrain:       [[ 4004.29579164]] \n",
      "   LossValidation:  [[ 2948.27605621]]\n",
      "iterations  663 \n",
      "   LossTrain:       [[ 4003.89578395]] \n",
      "   LossValidation:  [[ 2947.98656355]]\n",
      "iterations  664 \n",
      "   LossTrain:       [[ 4003.4973426]] \n",
      "   LossValidation:  [[ 2947.698648]]\n",
      "iterations  665 \n",
      "   LossTrain:       [[ 4003.10045819]] \n",
      "   LossValidation:  [[ 2947.41230017]]\n",
      "iterations  666 \n",
      "   LossTrain:       [[ 4002.7051214]] \n",
      "   LossValidation:  [[ 2947.12751074]]\n",
      "iterations  667 \n",
      "   LossTrain:       [[ 4002.31132297]] \n",
      "   LossValidation:  [[ 2946.84427045]]\n",
      "iterations  668 \n",
      "   LossTrain:       [[ 4001.91905371]] \n",
      "   LossValidation:  [[ 2946.56257012]]\n",
      "iterations  669 \n",
      "   LossTrain:       [[ 4001.52830454]] \n",
      "   LossValidation:  [[ 2946.28240061]]\n",
      "iterations  670 \n",
      "   LossTrain:       [[ 4001.13906642]] \n",
      "   LossValidation:  [[ 2946.00375288]]\n",
      "iterations  671 \n",
      "   LossTrain:       [[ 4000.75133039]] \n",
      "   LossValidation:  [[ 2945.72661792]]\n",
      "iterations  672 \n",
      "   LossTrain:       [[ 4000.36508758]] \n",
      "   LossValidation:  [[ 2945.45098681]]\n",
      "iterations  673 \n",
      "   LossTrain:       [[ 3999.98032918]] \n",
      "   LossValidation:  [[ 2945.17685069]]\n",
      "iterations  674 \n",
      "   LossTrain:       [[ 3999.59704645]] \n",
      "   LossValidation:  [[ 2944.90420075]]\n",
      "iterations  675 \n",
      "   LossTrain:       [[ 3999.21523072]] \n",
      "   LossValidation:  [[ 2944.63302825]]\n",
      "iterations  676 \n",
      "   LossTrain:       [[ 3998.83487339]] \n",
      "   LossValidation:  [[ 2944.36332451]]\n",
      "iterations  677 \n",
      "   LossTrain:       [[ 3998.45596595]] \n",
      "   LossValidation:  [[ 2944.09508092]]\n",
      "iterations  678 \n",
      "   LossTrain:       [[ 3998.07849993]] \n",
      "   LossValidation:  [[ 2943.82828892]]\n",
      "iterations  679 \n",
      "   LossTrain:       [[ 3997.70246694]] \n",
      "   LossValidation:  [[ 2943.56294001]]\n",
      "iterations  680 \n",
      "   LossTrain:       [[ 3997.32785867]] \n",
      "   LossValidation:  [[ 2943.29902577]]\n",
      "iterations  681 \n",
      "   LossTrain:       [[ 3996.95466685]] \n",
      "   LossValidation:  [[ 2943.03653781]]\n",
      "iterations  682 \n",
      "   LossTrain:       [[ 3996.58288329]] \n",
      "   LossValidation:  [[ 2942.77546782]]\n",
      "iterations  683 \n",
      "   LossTrain:       [[ 3996.21249988]] \n",
      "   LossValidation:  [[ 2942.51580755]]\n",
      "iterations  684 \n",
      "   LossTrain:       [[ 3995.84350855]] \n",
      "   LossValidation:  [[ 2942.25754878]]\n",
      "iterations  685 \n",
      "   LossTrain:       [[ 3995.47590132]] \n",
      "   LossValidation:  [[ 2942.00068338]]\n",
      "iterations  686 \n",
      "   LossTrain:       [[ 3995.10967025]] \n",
      "   LossValidation:  [[ 2941.74520327]]\n",
      "iterations  687 \n",
      "   LossTrain:       [[ 3994.74480747]] \n",
      "   LossValidation:  [[ 2941.49110041]]\n",
      "iterations  688 \n",
      "   LossTrain:       [[ 3994.38130518]] \n",
      "   LossValidation:  [[ 2941.23836684]]\n",
      "iterations  689 \n",
      "   LossTrain:       [[ 3994.01915564]] \n",
      "   LossValidation:  [[ 2940.98699462]]\n",
      "iterations  690 \n",
      "   LossTrain:       [[ 3993.65835117]] \n",
      "   LossValidation:  [[ 2940.73697592]]\n",
      "iterations  691 \n",
      "   LossTrain:       [[ 3993.29888414]] \n",
      "   LossValidation:  [[ 2940.4883029]]\n",
      "iterations  692 \n",
      "   LossTrain:       [[ 3992.94074701]] \n",
      "   LossValidation:  [[ 2940.24096783]]\n",
      "iterations  693 \n",
      "   LossTrain:       [[ 3992.58393226]] \n",
      "   LossValidation:  [[ 2939.99496301]]\n",
      "iterations  694 \n",
      "   LossTrain:       [[ 3992.22843246]] \n",
      "   LossValidation:  [[ 2939.75028078]]\n",
      "iterations  695 \n",
      "   LossTrain:       [[ 3991.87424022]] \n",
      "   LossValidation:  [[ 2939.50691355]]\n",
      "iterations  696 \n",
      "   LossTrain:       [[ 3991.52134823]] \n",
      "   LossValidation:  [[ 2939.26485379]]\n",
      "iterations  697 \n",
      "   LossTrain:       [[ 3991.16974921]] \n",
      "   LossValidation:  [[ 2939.02409401]]\n",
      "iterations  698 \n",
      "   LossTrain:       [[ 3990.81943595]] \n",
      "   LossValidation:  [[ 2938.78462677]]\n",
      "iterations  699 \n",
      "   LossTrain:       [[ 3990.4704013]] \n",
      "   LossValidation:  [[ 2938.54644468]]\n",
      "iterations  700 \n",
      "   LossTrain:       [[ 3990.12263817]] \n",
      "   LossValidation:  [[ 2938.30954042]]\n",
      "iterations  701 \n",
      "   LossTrain:       [[ 3989.77613951]] \n",
      "   LossValidation:  [[ 2938.07390669]]\n",
      "iterations  702 \n",
      "   LossTrain:       [[ 3989.43089833]] \n",
      "   LossValidation:  [[ 2937.83953626]]\n",
      "iterations  703 \n",
      "   LossTrain:       [[ 3989.0869077]] \n",
      "   LossValidation:  [[ 2937.60642194]]\n",
      "iterations  704 \n",
      "   LossTrain:       [[ 3988.74416075]] \n",
      "   LossValidation:  [[ 2937.37455661]]\n",
      "iterations  705 \n",
      "   LossTrain:       [[ 3988.40265064]] \n",
      "   LossValidation:  [[ 2937.14393318]]\n",
      "iterations  706 \n",
      "   LossTrain:       [[ 3988.0623706]] \n",
      "   LossValidation:  [[ 2936.9145446]]\n",
      "iterations  707 \n",
      "   LossTrain:       [[ 3987.72331391]] \n",
      "   LossValidation:  [[ 2936.68638389]]\n",
      "iterations  708 \n",
      "   LossTrain:       [[ 3987.38547391]] \n",
      "   LossValidation:  [[ 2936.4594441]]\n",
      "iterations  709 \n",
      "   LossTrain:       [[ 3987.04884397]] \n",
      "   LossValidation:  [[ 2936.23371834]]\n",
      "iterations  710 \n",
      "   LossTrain:       [[ 3986.71341753]] \n",
      "   LossValidation:  [[ 2936.00919976]]\n",
      "iterations  711 \n",
      "   LossTrain:       [[ 3986.37918807]] \n",
      "   LossValidation:  [[ 2935.78588155]]\n",
      "iterations  712 \n",
      "   LossTrain:       [[ 3986.04614912]] \n",
      "   LossValidation:  [[ 2935.56375697]]\n",
      "iterations  713 \n",
      "   LossTrain:       [[ 3985.71429428]] \n",
      "   LossValidation:  [[ 2935.3428193]]\n",
      "iterations  714 \n",
      "   LossTrain:       [[ 3985.38361716]] \n",
      "   LossValidation:  [[ 2935.12306187]]\n",
      "iterations  715 \n",
      "   LossTrain:       [[ 3985.05411146]] \n",
      "   LossValidation:  [[ 2934.90447806]]\n",
      "iterations  716 \n",
      "   LossTrain:       [[ 3984.7257709]] \n",
      "   LossValidation:  [[ 2934.68706131]]\n",
      "iterations  717 \n",
      "   LossTrain:       [[ 3984.39858925]] \n",
      "   LossValidation:  [[ 2934.47080507]]\n",
      "iterations  718 \n",
      "   LossTrain:       [[ 3984.07256034]] \n",
      "   LossValidation:  [[ 2934.25570287]]\n",
      "iterations  719 \n",
      "   LossTrain:       [[ 3983.74767805]] \n",
      "   LossValidation:  [[ 2934.04174825]]\n",
      "iterations  720 \n",
      "   LossTrain:       [[ 3983.42393628]] \n",
      "   LossValidation:  [[ 2933.82893481]]\n",
      "iterations  721 \n",
      "   LossTrain:       [[ 3983.101329]] \n",
      "   LossValidation:  [[ 2933.61725621]]\n",
      "iterations  722 \n",
      "   LossTrain:       [[ 3982.77985022]] \n",
      "   LossValidation:  [[ 2933.40670611]]\n",
      "iterations  723 \n",
      "   LossTrain:       [[ 3982.459494]] \n",
      "   LossValidation:  [[ 2933.19727825]]\n",
      "iterations  724 \n",
      "   LossTrain:       [[ 3982.14025442]] \n",
      "   LossValidation:  [[ 2932.98896641]]\n",
      "iterations  725 \n",
      "   LossTrain:       [[ 3981.82212563]] \n",
      "   LossValidation:  [[ 2932.78176437]]\n",
      "iterations  726 \n",
      "   LossTrain:       [[ 3981.50510183]] \n",
      "   LossValidation:  [[ 2932.57566601]]\n",
      "iterations  727 \n",
      "   LossTrain:       [[ 3981.18917723]] \n",
      "   LossValidation:  [[ 2932.37066521]]\n",
      "iterations  728 \n",
      "   LossTrain:       [[ 3980.87434611]] \n",
      "   LossValidation:  [[ 2932.1667559]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iterations  729 \n",
      "   LossTrain:       [[ 3980.56060279]] \n",
      "   LossValidation:  [[ 2931.96393206]]\n",
      "iterations  730 \n",
      "   LossTrain:       [[ 3980.24794162]] \n",
      "   LossValidation:  [[ 2931.76218769]]\n",
      "iterations  731 \n",
      "   LossTrain:       [[ 3979.93635701]] \n",
      "   LossValidation:  [[ 2931.56151686]]\n",
      "iterations  732 \n",
      "   LossTrain:       [[ 3979.62584339]] \n",
      "   LossValidation:  [[ 2931.36191365]]\n",
      "iterations  733 \n",
      "   LossTrain:       [[ 3979.31639524]] \n",
      "   LossValidation:  [[ 2931.16337219]]\n",
      "iterations  734 \n",
      "   LossTrain:       [[ 3979.0080071]] \n",
      "   LossValidation:  [[ 2930.96588665]]\n",
      "iterations  735 \n",
      "   LossTrain:       [[ 3978.70067352]] \n",
      "   LossValidation:  [[ 2930.76945124]]\n",
      "iterations  736 \n",
      "   LossTrain:       [[ 3978.39438911]] \n",
      "   LossValidation:  [[ 2930.57406021]]\n",
      "iterations  737 \n",
      "   LossTrain:       [[ 3978.08914851]] \n",
      "   LossValidation:  [[ 2930.37970783]]\n",
      "iterations  738 \n",
      "   LossTrain:       [[ 3977.7849464]] \n",
      "   LossValidation:  [[ 2930.18638843]]\n",
      "iterations  739 \n",
      "   LossTrain:       [[ 3977.4817775]] \n",
      "   LossValidation:  [[ 2929.99409637]]\n",
      "iterations  740 \n",
      "   LossTrain:       [[ 3977.17963658]] \n",
      "   LossValidation:  [[ 2929.80282604]]\n",
      "iterations  741 \n",
      "   LossTrain:       [[ 3976.87851843]] \n",
      "   LossValidation:  [[ 2929.61257187]]\n",
      "iterations  742 \n",
      "   LossTrain:       [[ 3976.57841789]] \n",
      "   LossValidation:  [[ 2929.42332833]]\n",
      "iterations  743 \n",
      "   LossTrain:       [[ 3976.27932983]] \n",
      "   LossValidation:  [[ 2929.23508992]]\n",
      "iterations  744 \n",
      "   LossTrain:       [[ 3975.98124916]] \n",
      "   LossValidation:  [[ 2929.04785118]]\n",
      "iterations  745 \n",
      "   LossTrain:       [[ 3975.68417084]] \n",
      "   LossValidation:  [[ 2928.86160668]]\n",
      "iterations  746 \n",
      "   LossTrain:       [[ 3975.38808984]] \n",
      "   LossValidation:  [[ 2928.67635105]]\n",
      "iterations  747 \n",
      "   LossTrain:       [[ 3975.09300119]] \n",
      "   LossValidation:  [[ 2928.49207891]]\n",
      "iterations  748 \n",
      "   LossTrain:       [[ 3974.79889994]] \n",
      "   LossValidation:  [[ 2928.30878495]]\n",
      "iterations  749 \n",
      "   LossTrain:       [[ 3974.50578119]] \n",
      "   LossValidation:  [[ 2928.12646388]]\n",
      "iterations  750 \n",
      "   LossTrain:       [[ 3974.21364005]] \n",
      "   LossValidation:  [[ 2927.94511046]]\n",
      "iterations  751 \n",
      "   LossTrain:       [[ 3973.92247171]] \n",
      "   LossValidation:  [[ 2927.76471946]]\n",
      "iterations  752 \n",
      "   LossTrain:       [[ 3973.63227134]] \n",
      "   LossValidation:  [[ 2927.58528569]]\n",
      "iterations  753 \n",
      "   LossTrain:       [[ 3973.34303419]] \n",
      "   LossValidation:  [[ 2927.40680401]]\n",
      "iterations  754 \n",
      "   LossTrain:       [[ 3973.05475552]] \n",
      "   LossValidation:  [[ 2927.2292693]]\n",
      "iterations  755 \n",
      "   LossTrain:       [[ 3972.76743063]] \n",
      "   LossValidation:  [[ 2927.05267647]]\n",
      "iterations  756 \n",
      "   LossTrain:       [[ 3972.48105485]] \n",
      "   LossValidation:  [[ 2926.87702046]]\n",
      "iterations  757 \n",
      "   LossTrain:       [[ 3972.19562355]] \n",
      "   LossValidation:  [[ 2926.70229627]]\n",
      "iterations  758 \n",
      "   LossTrain:       [[ 3971.91113212]] \n",
      "   LossValidation:  [[ 2926.52849889]]\n",
      "iterations  759 \n",
      "   LossTrain:       [[ 3971.627576]] \n",
      "   LossValidation:  [[ 2926.35562337]]\n",
      "iterations  760 \n",
      "   LossTrain:       [[ 3971.34495065]] \n",
      "   LossValidation:  [[ 2926.18366479]]\n",
      "iterations  761 \n",
      "   LossTrain:       [[ 3971.06325156]] \n",
      "   LossValidation:  [[ 2926.01261825]]\n",
      "iterations  762 \n",
      "   LossTrain:       [[ 3970.78247427]] \n",
      "   LossValidation:  [[ 2925.84247889]]\n",
      "iterations  763 \n",
      "   LossTrain:       [[ 3970.50261434]] \n",
      "   LossValidation:  [[ 2925.67324188]]\n",
      "iterations  764 \n",
      "   LossTrain:       [[ 3970.22366734]] \n",
      "   LossValidation:  [[ 2925.50490241]]\n",
      "iterations  765 \n",
      "   LossTrain:       [[ 3969.9456289]] \n",
      "   LossValidation:  [[ 2925.33745571]]\n",
      "iterations  766 \n",
      "   LossTrain:       [[ 3969.66849468]] \n",
      "   LossValidation:  [[ 2925.17089704]]\n",
      "iterations  767 \n",
      "   LossTrain:       [[ 3969.39226035]] \n",
      "   LossValidation:  [[ 2925.0052217]]\n",
      "iterations  768 \n",
      "   LossTrain:       [[ 3969.11692163]] \n",
      "   LossValidation:  [[ 2924.840425]]\n",
      "iterations  769 \n",
      "   LossTrain:       [[ 3968.84247426]] \n",
      "   LossValidation:  [[ 2924.67650228]]\n",
      "iterations  770 \n",
      "   LossTrain:       [[ 3968.56891401]] \n",
      "   LossValidation:  [[ 2924.51344893]]\n",
      "iterations  771 \n",
      "   LossTrain:       [[ 3968.29623668]] \n",
      "   LossValidation:  [[ 2924.35126035]]\n",
      "iterations  772 \n",
      "   LossTrain:       [[ 3968.02443809]] \n",
      "   LossValidation:  [[ 2924.18993197]]\n",
      "iterations  773 \n",
      "   LossTrain:       [[ 3967.75351411]] \n",
      "   LossValidation:  [[ 2924.02945927]]\n",
      "iterations  774 \n",
      "   LossTrain:       [[ 3967.48346063]] \n",
      "   LossValidation:  [[ 2923.86983773]]\n",
      "iterations  775 \n",
      "   LossTrain:       [[ 3967.21427355]] \n",
      "   LossValidation:  [[ 2923.71106287]]\n",
      "iterations  776 \n",
      "   LossTrain:       [[ 3966.94594882]] \n",
      "   LossValidation:  [[ 2923.55313024]]\n",
      "iterations  777 \n",
      "   LossTrain:       [[ 3966.67848242]] \n",
      "   LossValidation:  [[ 2923.39603542]]\n",
      "iterations  778 \n",
      "   LossTrain:       [[ 3966.41187033]] \n",
      "   LossValidation:  [[ 2923.23977401]]\n",
      "iterations  779 \n",
      "   LossTrain:       [[ 3966.14610859]] \n",
      "   LossValidation:  [[ 2923.08434164]]\n",
      "iterations  780 \n",
      "   LossTrain:       [[ 3965.88119325]] \n",
      "   LossValidation:  [[ 2922.92973396]]\n",
      "iterations  781 \n",
      "   LossTrain:       [[ 3965.61712039]] \n",
      "   LossValidation:  [[ 2922.77594668]]\n",
      "iterations  782 \n",
      "   LossTrain:       [[ 3965.3538861]] \n",
      "   LossValidation:  [[ 2922.62297548]]\n",
      "iterations  783 \n",
      "   LossTrain:       [[ 3965.09148654]] \n",
      "   LossValidation:  [[ 2922.47081612]]\n",
      "iterations  784 \n",
      "   LossTrain:       [[ 3964.82991785]] \n",
      "   LossValidation:  [[ 2922.31946436]]\n",
      "iterations  785 \n",
      "   LossTrain:       [[ 3964.56917621]] \n",
      "   LossValidation:  [[ 2922.16891599]]\n",
      "iterations  786 \n",
      "   LossTrain:       [[ 3964.30925785]] \n",
      "   LossValidation:  [[ 2922.01916682]]\n",
      "iterations  787 \n",
      "   LossTrain:       [[ 3964.05015899]] \n",
      "   LossValidation:  [[ 2921.8702127]]\n",
      "iterations  788 \n",
      "   LossTrain:       [[ 3963.7918759]] \n",
      "   LossValidation:  [[ 2921.7220495]]\n",
      "iterations  789 \n",
      "   LossTrain:       [[ 3963.53440486]] \n",
      "   LossValidation:  [[ 2921.57467311]]\n",
      "iterations  790 \n",
      "   LossTrain:       [[ 3963.27774218]] \n",
      "   LossValidation:  [[ 2921.42807944]]\n",
      "iterations  791 \n",
      "   LossTrain:       [[ 3963.0218842]] \n",
      "   LossValidation:  [[ 2921.28226444]]\n",
      "iterations  792 \n",
      "   LossTrain:       [[ 3962.76682727]] \n",
      "   LossValidation:  [[ 2921.13722409]]\n",
      "iterations  793 \n",
      "   LossTrain:       [[ 3962.51256779]] \n",
      "   LossValidation:  [[ 2920.99295436]]\n",
      "iterations  794 \n",
      "   LossTrain:       [[ 3962.25910216]] \n",
      "   LossValidation:  [[ 2920.84945129]]\n",
      "iterations  795 \n",
      "   LossTrain:       [[ 3962.00642681]] \n",
      "   LossValidation:  [[ 2920.7067109]]\n",
      "iterations  796 \n",
      "   LossTrain:       [[ 3961.7545382]] \n",
      "   LossValidation:  [[ 2920.56472928]]\n",
      "iterations  797 \n",
      "   LossTrain:       [[ 3961.5034328]] \n",
      "   LossValidation:  [[ 2920.4235025]]\n",
      "iterations  798 \n",
      "   LossTrain:       [[ 3961.25310712]] \n",
      "   LossValidation:  [[ 2920.28302668]]\n",
      "iterations  799 \n",
      "   LossTrain:       [[ 3961.00355769]] \n",
      "   LossValidation:  [[ 2920.14329796]]\n",
      "iterations  800 \n",
      "   LossTrain:       [[ 3960.75478104]] \n",
      "   LossValidation:  [[ 2920.00431249]]\n",
      "iterations  801 \n",
      "   LossTrain:       [[ 3960.50677376]] \n",
      "   LossValidation:  [[ 2919.86606647]]\n",
      "iterations  802 \n",
      "   LossTrain:       [[ 3960.25953243]] \n",
      "   LossValidation:  [[ 2919.7285561]]\n",
      "iterations  803 \n",
      "   LossTrain:       [[ 3960.01305367]] \n",
      "   LossValidation:  [[ 2919.5917776]]\n",
      "iterations  804 \n",
      "   LossTrain:       [[ 3959.76733412]] \n",
      "   LossValidation:  [[ 2919.45572724]]\n",
      "iterations  805 \n",
      "   LossTrain:       [[ 3959.52237043]] \n",
      "   LossValidation:  [[ 2919.32040128]]\n",
      "iterations  806 \n",
      "   LossTrain:       [[ 3959.27815929]] \n",
      "   LossValidation:  [[ 2919.18579602]]\n",
      "iterations  807 \n",
      "   LossTrain:       [[ 3959.03469739]] \n",
      "   LossValidation:  [[ 2919.05190779]]\n",
      "iterations  808 \n",
      "   LossTrain:       [[ 3958.79198147]] \n",
      "   LossValidation:  [[ 2918.91873292]]\n",
      "iterations  809 \n",
      "   LossTrain:       [[ 3958.55000827]] \n",
      "   LossValidation:  [[ 2918.78626778]]\n",
      "iterations  810 \n",
      "   LossTrain:       [[ 3958.30877454]] \n",
      "   LossValidation:  [[ 2918.65450876]]\n",
      "iterations  811 \n",
      "   LossTrain:       [[ 3958.06827709]] \n",
      "   LossValidation:  [[ 2918.52345225]]\n",
      "iterations  812 \n",
      "   LossTrain:       [[ 3957.82851271]] \n",
      "   LossValidation:  [[ 2918.39309469]]\n",
      "iterations  813 \n",
      "   LossTrain:       [[ 3957.58947823]] \n",
      "   LossValidation:  [[ 2918.26343254]]\n",
      "iterations  814 \n",
      "   LossTrain:       [[ 3957.3511705]] \n",
      "   LossValidation:  [[ 2918.13446225]]\n",
      "iterations  815 \n",
      "   LossTrain:       [[ 3957.1135864]] \n",
      "   LossValidation:  [[ 2918.00618032]]\n",
      "iterations  816 \n",
      "   LossTrain:       [[ 3956.87672279]] \n",
      "   LossValidation:  [[ 2917.87858327]]\n",
      "iterations  817 \n",
      "   LossTrain:       [[ 3956.6405766]] \n",
      "   LossValidation:  [[ 2917.75166763]]\n",
      "iterations  818 \n",
      "   LossTrain:       [[ 3956.40514475]] \n",
      "   LossValidation:  [[ 2917.62542995]]\n",
      "iterations  819 \n",
      "   LossTrain:       [[ 3956.17042419]] \n",
      "   LossValidation:  [[ 2917.49986681]]\n",
      "iterations  820 \n",
      "   LossTrain:       [[ 3955.93641188]] \n",
      "   LossValidation:  [[ 2917.3749748]]\n",
      "iterations  821 \n",
      "   LossTrain:       [[ 3955.70310481]] \n",
      "   LossValidation:  [[ 2917.25075054]]\n",
      "iterations  822 \n",
      "   LossTrain:       [[ 3955.47049998]] \n",
      "   LossValidation:  [[ 2917.12719066]]\n",
      "iterations  823 \n",
      "   LossTrain:       [[ 3955.23859441]] \n",
      "   LossValidation:  [[ 2917.00429181]]\n",
      "iterations  824 \n",
      "   LossTrain:       [[ 3955.00738515]] \n",
      "   LossValidation:  [[ 2916.88205068]]\n",
      "iterations  825 \n",
      "   LossTrain:       [[ 3954.77686926]] \n",
      "   LossValidation:  [[ 2916.76046395]]\n",
      "iterations  826 \n",
      "   LossTrain:       [[ 3954.54704381]] \n",
      "   LossValidation:  [[ 2916.63952834]]\n",
      "iterations  827 \n",
      "   LossTrain:       [[ 3954.3179059]] \n",
      "   LossValidation:  [[ 2916.51924058]]\n",
      "iterations  828 \n",
      "   LossTrain:       [[ 3954.08945265]] \n",
      "   LossValidation:  [[ 2916.39959742]]\n",
      "iterations  829 \n",
      "   LossTrain:       [[ 3953.86168118]] \n",
      "   LossValidation:  [[ 2916.28059564]]\n",
      "iterations  830 \n",
      "   LossTrain:       [[ 3953.63458865]] \n",
      "   LossValidation:  [[ 2916.16223202]]\n",
      "iterations  831 \n",
      "   LossTrain:       [[ 3953.40817222]] \n",
      "   LossValidation:  [[ 2916.04450337]]\n",
      "iterations  832 \n",
      "   LossTrain:       [[ 3953.18242909]] \n",
      "   LossValidation:  [[ 2915.92740653]]\n",
      "iterations  833 \n",
      "   LossTrain:       [[ 3952.95735645]] \n",
      "   LossValidation:  [[ 2915.81093833]]\n",
      "iterations  834 \n",
      "   LossTrain:       [[ 3952.73295152]] \n",
      "   LossValidation:  [[ 2915.69509563]]\n",
      "iterations  835 \n",
      "   LossTrain:       [[ 3952.50921154]] \n",
      "   LossValidation:  [[ 2915.57987533]]\n",
      "iterations  836 \n",
      "   LossTrain:       [[ 3952.28613377]] \n",
      "   LossValidation:  [[ 2915.46527432]]\n",
      "iterations  837 \n",
      "   LossTrain:       [[ 3952.06371547]] \n",
      "   LossValidation:  [[ 2915.35128952]]\n",
      "iterations  838 \n",
      "   LossTrain:       [[ 3951.84195393]] \n",
      "   LossValidation:  [[ 2915.23791786]]\n",
      "iterations  839 \n",
      "   LossTrain:       [[ 3951.62084646]] \n",
      "   LossValidation:  [[ 2915.12515631]]\n",
      "iterations  840 \n",
      "   LossTrain:       [[ 3951.40039037]] \n",
      "   LossValidation:  [[ 2915.01300183]]\n",
      "iterations  841 \n",
      "   LossTrain:       [[ 3951.180583]] \n",
      "   LossValidation:  [[ 2914.9014514]]\n",
      "iterations  842 \n",
      "   LossTrain:       [[ 3950.96142171]] \n",
      "   LossValidation:  [[ 2914.79050205]]\n",
      "iterations  843 \n",
      "   LossTrain:       [[ 3950.74290386]] \n",
      "   LossValidation:  [[ 2914.68015078]]\n",
      "iterations  844 \n",
      "   LossTrain:       [[ 3950.52502684]] \n",
      "   LossValidation:  [[ 2914.57039465]]\n",
      "iterations  845 \n",
      "   LossTrain:       [[ 3950.30778804]] \n",
      "   LossValidation:  [[ 2914.4612307]]\n",
      "iterations  846 \n",
      "   LossTrain:       [[ 3950.09118489]] \n",
      "   LossValidation:  [[ 2914.35265602]]\n",
      "iterations  847 \n",
      "   LossTrain:       [[ 3949.87521481]] \n",
      "   LossValidation:  [[ 2914.24466769]]\n",
      "iterations  848 \n",
      "   LossTrain:       [[ 3949.65987525]] \n",
      "   LossValidation:  [[ 2914.13726283]]\n",
      "iterations  849 \n",
      "   LossTrain:       [[ 3949.44516367]] \n",
      "   LossValidation:  [[ 2914.03043855]]\n",
      "iterations  850 \n",
      "   LossTrain:       [[ 3949.23107755]] \n",
      "   LossValidation:  [[ 2913.92419201]]\n",
      "iterations  851 \n",
      "   LossTrain:       [[ 3949.01761438]] \n",
      "   LossValidation:  [[ 2913.81852034]]\n",
      "iterations  852 \n",
      "   LossTrain:       [[ 3948.80477167]] \n",
      "   LossValidation:  [[ 2913.71342074]]\n",
      "iterations  853 \n",
      "   LossTrain:       [[ 3948.59254693]] \n",
      "   LossValidation:  [[ 2913.60889039]]\n",
      "iterations  854 \n",
      "   LossTrain:       [[ 3948.38093771]] \n",
      "   LossValidation:  [[ 2913.50492649]]\n",
      "iterations  855 \n",
      "   LossTrain:       [[ 3948.16994156]] \n",
      "   LossValidation:  [[ 2913.40152627]]\n",
      "iterations  856 \n",
      "   LossTrain:       [[ 3947.95955604]] \n",
      "   LossValidation:  [[ 2913.29868696]]\n",
      "iterations  857 \n",
      "   LossTrain:       [[ 3947.74977872]] \n",
      "   LossValidation:  [[ 2913.19640582]]\n",
      "iterations  858 \n",
      "   LossTrain:       [[ 3947.54060722]] \n",
      "   LossValidation:  [[ 2913.09468012]]\n",
      "iterations  859 \n",
      "   LossTrain:       [[ 3947.33203912]] \n",
      "   LossValidation:  [[ 2912.99350713]]\n",
      "iterations  860 \n",
      "   LossTrain:       [[ 3947.12407205]] \n",
      "   LossValidation:  [[ 2912.89288417]]\n",
      "iterations  861 \n",
      "   LossTrain:       [[ 3946.91670365]] \n",
      "   LossValidation:  [[ 2912.79280853]]\n",
      "iterations  862 \n",
      "   LossTrain:       [[ 3946.70993157]] \n",
      "   LossValidation:  [[ 2912.69327756]]\n",
      "iterations  863 \n",
      "   LossTrain:       [[ 3946.50375347]] \n",
      "   LossValidation:  [[ 2912.5942886]]\n",
      "iterations  864 \n",
      "   LossTrain:       [[ 3946.29816703]] \n",
      "   LossValidation:  [[ 2912.49583901]]\n",
      "iterations  865 \n",
      "   LossTrain:       [[ 3946.09316994]] \n",
      "   LossValidation:  [[ 2912.39792616]]\n",
      "iterations  866 \n",
      "   LossTrain:       [[ 3945.88875989]] \n",
      "   LossValidation:  [[ 2912.30054744]]\n",
      "iterations  867 \n",
      "   LossTrain:       [[ 3945.68493461]] \n",
      "   LossValidation:  [[ 2912.20370025]]\n",
      "iterations  868 \n",
      "   LossTrain:       [[ 3945.48169182]] \n",
      "   LossValidation:  [[ 2912.10738202]]\n",
      "iterations  869 \n",
      "   LossTrain:       [[ 3945.27902928]] \n",
      "   LossValidation:  [[ 2912.01159018]]\n",
      "iterations  870 \n",
      "   LossTrain:       [[ 3945.07694472]] \n",
      "   LossValidation:  [[ 2911.91632217]]\n",
      "iterations  871 \n",
      "   LossTrain:       [[ 3944.87543593]] \n",
      "   LossValidation:  [[ 2911.82157546]]\n",
      "iterations  872 \n",
      "   LossTrain:       [[ 3944.67450068]] \n",
      "   LossValidation:  [[ 2911.72734751]]\n",
      "iterations  873 \n",
      "   LossTrain:       [[ 3944.47413677]] \n",
      "   LossValidation:  [[ 2911.63363583]]\n",
      "iterations  874 \n",
      "   LossTrain:       [[ 3944.274342]] \n",
      "   LossValidation:  [[ 2911.54043792]]\n",
      "iterations  875 \n",
      "   LossTrain:       [[ 3944.0751142]] \n",
      "   LossValidation:  [[ 2911.44775129]]\n",
      "iterations  876 \n",
      "   LossTrain:       [[ 3943.87645118]] \n",
      "   LossValidation:  [[ 2911.35557347]]\n",
      "iterations  877 \n",
      "   LossTrain:       [[ 3943.6783508]] \n",
      "   LossValidation:  [[ 2911.26390201]]\n",
      "iterations  878 \n",
      "   LossTrain:       [[ 3943.48081091]] \n",
      "   LossValidation:  [[ 2911.17273447]]\n",
      "iterations  879 \n",
      "   LossTrain:       [[ 3943.28382938]] \n",
      "   LossValidation:  [[ 2911.08206842]]\n",
      "iterations  880 \n",
      "   LossTrain:       [[ 3943.08740409]] \n",
      "   LossValidation:  [[ 2910.99190144]]\n",
      "iterations  881 \n",
      "   LossTrain:       [[ 3942.89153292]] \n",
      "   LossValidation:  [[ 2910.90223114]]\n",
      "iterations  882 \n",
      "   LossTrain:       [[ 3942.69621379]] \n",
      "   LossValidation:  [[ 2910.81305513]]\n",
      "iterations  883 \n",
      "   LossTrain:       [[ 3942.5014446]] \n",
      "   LossValidation:  [[ 2910.72437102]]\n",
      "iterations  884 \n",
      "   LossTrain:       [[ 3942.30722329]] \n",
      "   LossValidation:  [[ 2910.63617647]]\n",
      "iterations  885 \n",
      "   LossTrain:       [[ 3942.11354779]] \n",
      "   LossValidation:  [[ 2910.54846913]]\n",
      "iterations  886 \n",
      "   LossTrain:       [[ 3941.92041605]] \n",
      "   LossValidation:  [[ 2910.46124665]]\n",
      "iterations  887 \n",
      "   LossTrain:       [[ 3941.72782602]] \n",
      "   LossValidation:  [[ 2910.37450671]]\n",
      "iterations  888 \n",
      "   LossTrain:       [[ 3941.53577569]] \n",
      "   LossValidation:  [[ 2910.28824701]]\n",
      "iterations  889 \n",
      "   LossTrain:       [[ 3941.34426304]] \n",
      "   LossValidation:  [[ 2910.20246524]]\n",
      "iterations  890 \n",
      "   LossTrain:       [[ 3941.15328606]] \n",
      "   LossValidation:  [[ 2910.11715912]]\n",
      "iterations  891 \n",
      "   LossTrain:       [[ 3940.96284275]] \n",
      "   LossValidation:  [[ 2910.03232639]]\n",
      "iterations  892 \n",
      "   LossTrain:       [[ 3940.77293113]] \n",
      "   LossValidation:  [[ 2909.94796477]]\n",
      "iterations  893 \n",
      "   LossTrain:       [[ 3940.58354923]] \n",
      "   LossValidation:  [[ 2909.86407202]]\n",
      "iterations  894 \n",
      "   LossTrain:       [[ 3940.39469508]] \n",
      "   LossValidation:  [[ 2909.78064591]]\n",
      "iterations  895 \n",
      "   LossTrain:       [[ 3940.20636674]] \n",
      "   LossValidation:  [[ 2909.69768422]]\n",
      "iterations  896 \n",
      "   LossTrain:       [[ 3940.01856226]] \n",
      "   LossValidation:  [[ 2909.61518472]]\n",
      "iterations  897 \n",
      "   LossTrain:       [[ 3939.83127972]] \n",
      "   LossValidation:  [[ 2909.53314523]]\n",
      "iterations  898 \n",
      "   LossTrain:       [[ 3939.64451718]] \n",
      "   LossValidation:  [[ 2909.45156356]]\n",
      "iterations  899 \n",
      "   LossTrain:       [[ 3939.45827275]] \n",
      "   LossValidation:  [[ 2909.37043752]]\n",
      "iterations  900 \n",
      "   LossTrain:       [[ 3939.27254453]] \n",
      "   LossValidation:  [[ 2909.28976496]]\n",
      "iterations  901 \n",
      "   LossTrain:       [[ 3939.08733061]] \n",
      "   LossValidation:  [[ 2909.20954372]]\n",
      "iterations  902 \n",
      "   LossTrain:       [[ 3938.90262914]] \n",
      "   LossValidation:  [[ 2909.12977167]]\n",
      "iterations  903 \n",
      "   LossTrain:       [[ 3938.71843823]] \n",
      "   LossValidation:  [[ 2909.05044668]]\n",
      "iterations  904 \n",
      "   LossTrain:       [[ 3938.53475602]] \n",
      "   LossValidation:  [[ 2908.97156662]]\n",
      "iterations  905 \n",
      "   LossTrain:       [[ 3938.35158068]] \n",
      "   LossValidation:  [[ 2908.8931294]]\n",
      "iterations  906 \n",
      "   LossTrain:       [[ 3938.16891035]] \n",
      "   LossValidation:  [[ 2908.81513292]]\n",
      "iterations  907 \n",
      "   LossTrain:       [[ 3937.98674322]] \n",
      "   LossValidation:  [[ 2908.73757509]]\n",
      "iterations  908 \n",
      "   LossTrain:       [[ 3937.80507745]] \n",
      "   LossValidation:  [[ 2908.66045384]]\n",
      "iterations  909 \n",
      "   LossTrain:       [[ 3937.62391125]] \n",
      "   LossValidation:  [[ 2908.58376712]]\n",
      "iterations  910 \n",
      "   LossTrain:       [[ 3937.44324281]] \n",
      "   LossValidation:  [[ 2908.50751287]]\n",
      "iterations  911 \n",
      "   LossTrain:       [[ 3937.26307033]] \n",
      "   LossValidation:  [[ 2908.43168905]]\n",
      "iterations  912 \n",
      "   LossTrain:       [[ 3937.08339204]] \n",
      "   LossValidation:  [[ 2908.35629364]]\n",
      "iterations  913 \n",
      "   LossTrain:       [[ 3936.90420617]] \n",
      "   LossValidation:  [[ 2908.28132461]]\n",
      "iterations  914 \n",
      "   LossTrain:       [[ 3936.72551095]] \n",
      "   LossValidation:  [[ 2908.20677997]]\n",
      "iterations  915 \n",
      "   LossTrain:       [[ 3936.54730463]] \n",
      "   LossValidation:  [[ 2908.13265772]]\n",
      "iterations  916 \n",
      "   LossTrain:       [[ 3936.36958546]] \n",
      "   LossValidation:  [[ 2908.05895586]]\n",
      "iterations  917 \n",
      "   LossTrain:       [[ 3936.19235171]] \n",
      "   LossValidation:  [[ 2907.98567244]]\n",
      "iterations  918 \n",
      "   LossTrain:       [[ 3936.01560166]] \n",
      "   LossValidation:  [[ 2907.91280547]]\n",
      "iterations  919 \n",
      "   LossTrain:       [[ 3935.83933358]] \n",
      "   LossValidation:  [[ 2907.84035302]]\n",
      "iterations  920 \n",
      "   LossTrain:       [[ 3935.66354576]] \n",
      "   LossValidation:  [[ 2907.76831313]]\n",
      "iterations  921 \n",
      "   LossTrain:       [[ 3935.48823652]] \n",
      "   LossValidation:  [[ 2907.69668387]]\n",
      "iterations  922 \n",
      "   LossTrain:       [[ 3935.31340415]] \n",
      "   LossValidation:  [[ 2907.62546333]]\n",
      "iterations  923 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   LossTrain:       [[ 3935.13904697]] \n",
      "   LossValidation:  [[ 2907.55464958]]\n",
      "iterations  924 \n",
      "   LossTrain:       [[ 3934.96516331]] \n",
      "   LossValidation:  [[ 2907.48424073]]\n",
      "iterations  925 \n",
      "   LossTrain:       [[ 3934.79175151]] \n",
      "   LossValidation:  [[ 2907.41423489]]\n",
      "iterations  926 \n",
      "   LossTrain:       [[ 3934.61880991]] \n",
      "   LossValidation:  [[ 2907.34463016]]\n",
      "iterations  927 \n",
      "   LossTrain:       [[ 3934.44633685]] \n",
      "   LossValidation:  [[ 2907.27542469]]\n",
      "iterations  928 \n",
      "   LossTrain:       [[ 3934.27433071]] \n",
      "   LossValidation:  [[ 2907.2066166]]\n",
      "iterations  929 \n",
      "   LossTrain:       [[ 3934.10278985]] \n",
      "   LossValidation:  [[ 2907.13820404]]\n",
      "iterations  930 \n",
      "   LossTrain:       [[ 3933.93171265]] \n",
      "   LossValidation:  [[ 2907.07018518]]\n",
      "iterations  931 \n",
      "   LossTrain:       [[ 3933.76109749]] \n",
      "   LossValidation:  [[ 2907.00255817]]\n",
      "iterations  932 \n",
      "   LossTrain:       [[ 3933.59094277]] \n",
      "   LossValidation:  [[ 2906.93532119]]\n",
      "iterations  933 \n",
      "   LossTrain:       [[ 3933.42124688]] \n",
      "   LossValidation:  [[ 2906.86847244]]\n",
      "iterations  934 \n",
      "   LossTrain:       [[ 3933.25200824]] \n",
      "   LossValidation:  [[ 2906.8020101]]\n",
      "iterations  935 \n",
      "   LossTrain:       [[ 3933.08322526]] \n",
      "   LossValidation:  [[ 2906.73593238]]\n",
      "iterations  936 \n",
      "   LossTrain:       [[ 3932.91489638]] \n",
      "   LossValidation:  [[ 2906.67023749]]\n",
      "iterations  937 \n",
      "   LossTrain:       [[ 3932.74702002]] \n",
      "   LossValidation:  [[ 2906.60492366]]\n",
      "iterations  938 \n",
      "   LossTrain:       [[ 3932.57959462]] \n",
      "   LossValidation:  [[ 2906.53998911]]\n",
      "iterations  939 \n",
      "   LossTrain:       [[ 3932.41261864]] \n",
      "   LossValidation:  [[ 2906.4754321]]\n",
      "iterations  940 \n",
      "   LossTrain:       [[ 3932.24609053]] \n",
      "   LossValidation:  [[ 2906.41125088]]\n",
      "iterations  941 \n",
      "   LossTrain:       [[ 3932.08000876]] \n",
      "   LossValidation:  [[ 2906.34744369]]\n",
      "iterations  942 \n",
      "   LossTrain:       [[ 3931.91437179]] \n",
      "   LossValidation:  [[ 2906.28400882]]\n",
      "iterations  943 \n",
      "   LossTrain:       [[ 3931.74917811]] \n",
      "   LossValidation:  [[ 2906.22094454]]\n",
      "iterations  944 \n",
      "   LossTrain:       [[ 3931.58442621]] \n",
      "   LossValidation:  [[ 2906.15824913]]\n",
      "iterations  945 \n",
      "   LossTrain:       [[ 3931.42011457]] \n",
      "   LossValidation:  [[ 2906.0959209]]\n",
      "iterations  946 \n",
      "   LossTrain:       [[ 3931.25624171]] \n",
      "   LossValidation:  [[ 2906.03395815]]\n",
      "iterations  947 \n",
      "   LossTrain:       [[ 3931.09280613]] \n",
      "   LossValidation:  [[ 2905.97235918]]\n",
      "iterations  948 \n",
      "   LossTrain:       [[ 3930.92980634]] \n",
      "   LossValidation:  [[ 2905.91112234]]\n",
      "iterations  949 \n",
      "   LossTrain:       [[ 3930.76724088]] \n",
      "   LossValidation:  [[ 2905.85024593]]\n",
      "iterations  950 \n",
      "   LossTrain:       [[ 3930.60510827]] \n",
      "   LossValidation:  [[ 2905.78972831]]\n",
      "iterations  951 \n",
      "   LossTrain:       [[ 3930.44340705]] \n",
      "   LossValidation:  [[ 2905.72956783]]\n",
      "iterations  952 \n",
      "   LossTrain:       [[ 3930.28213576]] \n",
      "   LossValidation:  [[ 2905.66976283]]\n",
      "iterations  953 \n",
      "   LossTrain:       [[ 3930.12129297]] \n",
      "   LossValidation:  [[ 2905.61031168]]\n",
      "iterations  954 \n",
      "   LossTrain:       [[ 3929.96087722]] \n",
      "   LossValidation:  [[ 2905.55121277]]\n",
      "iterations  955 \n",
      "   LossTrain:       [[ 3929.80088709]] \n",
      "   LossValidation:  [[ 2905.49246446]]\n",
      "iterations  956 \n",
      "   LossTrain:       [[ 3929.64132115]] \n",
      "   LossValidation:  [[ 2905.43406515]]\n",
      "iterations  957 \n",
      "   LossTrain:       [[ 3929.48217798]] \n",
      "   LossValidation:  [[ 2905.37601324]]\n",
      "iterations  958 \n",
      "   LossTrain:       [[ 3929.32345616]] \n",
      "   LossValidation:  [[ 2905.31830714]]\n",
      "iterations  959 \n",
      "   LossTrain:       [[ 3929.16515429]] \n",
      "   LossValidation:  [[ 2905.26094525]]\n",
      "iterations  960 \n",
      "   LossTrain:       [[ 3929.00727098]] \n",
      "   LossValidation:  [[ 2905.20392601]]\n",
      "iterations  961 \n",
      "   LossTrain:       [[ 3928.84980482]] \n",
      "   LossValidation:  [[ 2905.14724783]]\n",
      "iterations  962 \n",
      "   LossTrain:       [[ 3928.69275443]] \n",
      "   LossValidation:  [[ 2905.09090917]]\n",
      "iterations  963 \n",
      "   LossTrain:       [[ 3928.53611844]] \n",
      "   LossValidation:  [[ 2905.03490847]]\n",
      "iterations  964 \n",
      "   LossTrain:       [[ 3928.37989547]] \n",
      "   LossValidation:  [[ 2904.97924418]]\n",
      "iterations  965 \n",
      "   LossTrain:       [[ 3928.22408415]] \n",
      "   LossValidation:  [[ 2904.92391476]]\n",
      "iterations  966 \n",
      "   LossTrain:       [[ 3928.06868312]] \n",
      "   LossValidation:  [[ 2904.86891869]]\n",
      "iterations  967 \n",
      "   LossTrain:       [[ 3927.91369104]] \n",
      "   LossValidation:  [[ 2904.81425445]]\n",
      "iterations  968 \n",
      "   LossTrain:       [[ 3927.75910654]] \n",
      "   LossValidation:  [[ 2904.75992051]]\n",
      "iterations  969 \n",
      "   LossTrain:       [[ 3927.60492831]] \n",
      "   LossValidation:  [[ 2904.70591538]]\n",
      "iterations  970 \n",
      "   LossTrain:       [[ 3927.45115499]] \n",
      "   LossValidation:  [[ 2904.65223755]]\n",
      "iterations  971 \n",
      "   LossTrain:       [[ 3927.29778525]] \n",
      "   LossValidation:  [[ 2904.59888553]]\n",
      "iterations  972 \n",
      "   LossTrain:       [[ 3927.14481779]] \n",
      "   LossValidation:  [[ 2904.54585784]]\n",
      "iterations  973 \n",
      "   LossTrain:       [[ 3926.99225128]] \n",
      "   LossValidation:  [[ 2904.49315301]]\n",
      "iterations  974 \n",
      "   LossTrain:       [[ 3926.84008442]] \n",
      "   LossValidation:  [[ 2904.44076955]]\n",
      "iterations  975 \n",
      "   LossTrain:       [[ 3926.68831589]] \n",
      "   LossValidation:  [[ 2904.38870603]]\n",
      "iterations  976 \n",
      "   LossTrain:       [[ 3926.53694441]] \n",
      "   LossValidation:  [[ 2904.33696097]]\n",
      "iterations  977 \n",
      "   LossTrain:       [[ 3926.38596867]] \n",
      "   LossValidation:  [[ 2904.28553293]]\n",
      "iterations  978 \n",
      "   LossTrain:       [[ 3926.2353874]] \n",
      "   LossValidation:  [[ 2904.23442047]]\n",
      "iterations  979 \n",
      "   LossTrain:       [[ 3926.08519932]] \n",
      "   LossValidation:  [[ 2904.18362216]]\n",
      "iterations  980 \n",
      "   LossTrain:       [[ 3925.93540315]] \n",
      "   LossValidation:  [[ 2904.13313658]]\n",
      "iterations  981 \n",
      "   LossTrain:       [[ 3925.78599763]] \n",
      "   LossValidation:  [[ 2904.08296231]]\n",
      "iterations  982 \n",
      "   LossTrain:       [[ 3925.63698148]] \n",
      "   LossValidation:  [[ 2904.03309793]]\n",
      "iterations  983 \n",
      "   LossTrain:       [[ 3925.48835347]] \n",
      "   LossValidation:  [[ 2903.98354205]]\n",
      "iterations  984 \n",
      "   LossTrain:       [[ 3925.34011233]] \n",
      "   LossValidation:  [[ 2903.93429326]]\n",
      "iterations  985 \n",
      "   LossTrain:       [[ 3925.19225682]] \n",
      "   LossValidation:  [[ 2903.88535017]]\n",
      "iterations  986 \n",
      "   LossTrain:       [[ 3925.04478571]] \n",
      "   LossValidation:  [[ 2903.83671141]]\n",
      "iterations  987 \n",
      "   LossTrain:       [[ 3924.89769775]] \n",
      "   LossValidation:  [[ 2903.7883756]]\n",
      "iterations  988 \n",
      "   LossTrain:       [[ 3924.75099173]] \n",
      "   LossValidation:  [[ 2903.74034136]]\n",
      "iterations  989 \n",
      "   LossTrain:       [[ 3924.60466641]] \n",
      "   LossValidation:  [[ 2903.69260734]]\n",
      "iterations  990 \n",
      "   LossTrain:       [[ 3924.45872059]] \n",
      "   LossValidation:  [[ 2903.64517219]]\n",
      "iterations  991 \n",
      "   LossTrain:       [[ 3924.31315305]] \n",
      "   LossValidation:  [[ 2903.59803454]]\n",
      "iterations  992 \n",
      "   LossTrain:       [[ 3924.16796259]] \n",
      "   LossValidation:  [[ 2903.55119306]]\n",
      "iterations  993 \n",
      "   LossTrain:       [[ 3924.023148]] \n",
      "   LossValidation:  [[ 2903.50464642]]\n",
      "iterations  994 \n",
      "   LossTrain:       [[ 3923.87870809]] \n",
      "   LossValidation:  [[ 2903.45839328]]\n",
      "iterations  995 \n",
      "   LossTrain:       [[ 3923.73464168]] \n",
      "   LossValidation:  [[ 2903.41243233]]\n",
      "iterations  996 \n",
      "   LossTrain:       [[ 3923.59094757]] \n",
      "   LossValidation:  [[ 2903.36676224]]\n",
      "iterations  997 \n",
      "   LossTrain:       [[ 3923.4476246]] \n",
      "   LossValidation:  [[ 2903.32138172]]\n",
      "iterations  998 \n",
      "   LossTrain:       [[ 3923.30467157]] \n",
      "   LossValidation:  [[ 2903.27628945]]\n",
      "iterations  999 \n",
      "   LossTrain:       [[ 3923.16208734]] \n",
      "   LossValidation:  [[ 2903.23148413]]\n",
      "iterations  1000 \n",
      "   LossTrain:       [[ 3923.01987073]] \n",
      "   LossValidation:  [[ 2903.18696449]]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYgAAAEWCAYAAAB8LwAVAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAkJERPJSQIiISF4KCBERyUsBISIief3/tpL1W3ZYIooAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x28ce3d699e8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "from sklearn.datasets import load_svmlight_file\n",
    "from sklearn.cross_validation import train_test_split\n",
    "\n",
    "# Draw\n",
    "def Draw(iterations, Loss_train, Loss_Validation):\n",
    "    plt.plot(np.arange(0,100,1), Loss_train[0:100], label='Loss Train ')\n",
    "    plt.plot(np.arange(0,100,1), Loss_Validation[0:100], label='Loss Validation')\n",
    "    plt.xlabel('iterations')\n",
    "    plt.ylabel('loss')\n",
    "    plt.title('Loss')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "\n",
    "# read data\n",
    "def get_data():\n",
    "    data = load_svmlight_file(\"housing.txt\")\n",
    "    return data\n",
    "    \n",
    "def grad(theta, X_train , y_train):\n",
    "    grad = np.dot(X_train.transpose(), np.dot(X_train, theta)) - np.dot( X_train.transpose(),y_train)\n",
    "    return grad\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    \n",
    "    X = get_data()[0]\n",
    "    y = get_data()[1]\n",
    "\n",
    "    X = X.toarray()#\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4, random_state=1)\n",
    "\n",
    "    train_row = X_train.shape[0]\n",
    "    test_row = X_test.shape[0]\n",
    "    col = X_train.shape[1]\n",
    "    y_train = y_train.reshape(train_row, 1)\n",
    "    y_test = y_test.reshape(test_row, 1) \n",
    "    \n",
    "    theta = np.random.random(size = (col, 1))#theta\n",
    "    alpha = 0.0001#\n",
    "    iterations = 1000 # \n",
    "    epsilon = 0.000001 # \n",
    "    count = 0 # \n",
    "    theta1 = np.zeros((col, 1)) # theta0\n",
    "    finish = 0 # \n",
    "    lossTrain = []\n",
    "    lossValidation = []\n",
    "\n",
    "    \n",
    "    while count < iterations:\n",
    "        count += 1\n",
    "        # theta\n",
    "        theta = theta - alpha * (grad(theta, X_train , y_train) ) #theta\n",
    "        if (np.linalg.norm(theta - theta1) < epsilon):\n",
    "            finish = 1\n",
    "            break\n",
    "        else:\n",
    "            theta1 = theta \n",
    "            LossTrain = (1/2)  * theta.transpose().dot(theta) + (1/2) * (y_train - X_train.dot(theta)).transpose().dot((y_train - X_train.dot(theta)))\n",
    "            #loss\n",
    "            LossValidation = (1/2)  * theta.transpose().dot(theta) + (1/2) * (y_test-X_test.dot(theta)).transpose().dot((y_test - X_test.dot(theta)))\n",
    "            #loss\n",
    "            lossTrain.append(LossTrain[0] / train_row)\n",
    "            lossValidation.append(LossValidation[0] / test_row)\n",
    "            print('iterations '.format(count), count, '\\n',  '  LossTrain:      ',LossTrain,'\\n' '   LossValidation: ',LossValidation)\n",
    "\n",
    "    Draw(count, lossTrain, lossValidation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
